

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lingvo.core.batch_major_attention &mdash; Lingvo  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.batch_major_attention</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.batch_major_attention</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python2, python3</span>
<span class="c1"># Copyright 2020 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Multi-headed attention layers for Transformer machine translation.</span>

<span class="sd">[1] Attention is all you need.</span>
<span class="sd">    https://arxiv.org/pdf/1706.03762.pdf Section 3.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">from</span> <span class="nn">lingvo</span> <span class="kn">import</span> <span class="n">compat</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">base_layer</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">builder</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">conv_layers_builder</span> <span class="k">as</span> <span class="n">conv_layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">gpipe</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">hyperparams</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">layers_with_attention</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">py_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">relative_atten_util</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">symbolic</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">tshape</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">inplace_ops</span>  <span class="c1"># pylint: disable=g-direct-tensorflow-import</span>


<div class="viewcode-block" id="CausalPadding"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.CausalPadding">[docs]</a><span class="k">def</span> <span class="nf">CausalPadding</span><span class="p">(</span><span class="n">slen</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">band_part</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">slen</span><span class="p">,</span> <span class="n">slen</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="SegmentMask"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.SegmentMask">[docs]</a><span class="k">def</span> <span class="nf">SegmentMask</span><span class="p">(</span><span class="n">segment_id</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calculates a segment mask for attention.</span>

<span class="sd">  Args:</span>
<span class="sd">    segment_id: [B, T]</span>
<span class="sd">    source_segment_id: [B, S]</span>
<span class="sd">    dtype: data type of generated mask.</span>

<span class="sd">  Returns:</span>
<span class="sd">    segment_mask: [B, 1, T, S]: A mask that is ready to</span>
<span class="sd">    be added to [B, N, T, S] attention logits.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">segment_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="c1"># Compute [B, T, S] = [B, T, 1] != [B, 1, S]</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">segment_id</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_segment_id</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">ret</span> <span class="o">*=</span> <span class="n">ret</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">*</span> <span class="o">-</span><span class="mf">0.7</span>
  <span class="c1"># [B, T, S] -&gt; [B, 1, T, S]</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="PerDimScaleLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PerDimScaleLayer">[docs]</a><span class="k">class</span> <span class="nc">PerDimScaleLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A layer to scale individual dims of the input.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="PerDimScaleLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PerDimScaleLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for `PerDimScaleLayer`.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">PerDimScaleLayer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of individual dims .&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a PerDimScaleLayer object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PerDimScaleLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;per_dim_scale&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>

<div class="viewcode-block" id="PerDimScaleLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PerDimScaleLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return theta.scale * inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: weights defined in this layer.</span>
<span class="sd">      inputs: 4D tensor with shape [..., p.dim]</span>

<span class="sd">    Returns:</span>
<span class="sd">      outpus: 4D tensor with shape [..., p.dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">scale</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">inputs</span> <span class="o">*</span> <span class="n">scale</span></div>

<div class="viewcode-block" id="PerDimScaleLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PerDimScaleLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">flops</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">num_elements</span><span class="p">()</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,))</span></div></div>


<div class="viewcode-block" id="MultiHeadedProjectionLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadedProjectionLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Layer that computes multi heads projection.</span>

<span class="sd">    This layer is expected to be used within MultiHeadedAttention below.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiHeadedProjectionLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for MultiHeadedProjectionLayer.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedProjectionLayer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Input dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of heads.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dim_per_head&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Size of each head.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;is_output_projection&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether it is out projection or not. If False, we use &#39;</span>
        <span class="s1">&#39;&quot;BTD,DNH-&gt;BTNH&quot; for query,key,value projection. Otherwise we use &#39;</span>
        <span class="s1">&#39;&quot;BTNH,DNH-&gt;BTD&quot; for output projection.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_bias&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;If to add bias in projection.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedProjectionLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dim_per_head</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_output_projection</span><span class="p">:</span>
        <span class="n">pc_bias</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">pc_bias</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dim_per_head</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">pc_bias</span><span class="p">)</span>

<div class="viewcode-block" id="MultiHeadedProjectionLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the multi headed projection for inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: A tensor of shape [batch_size, time_steps, num_heads,</span>
<span class="sd">        dim_per_head] or [batch_size, time_steps, hidden_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      The projected tensor with shape [[batch_size, time_steps, hidden_size] or</span>
<span class="sd">      [batch_size, time_steps, num_heads, dim_per_head].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_output_projection</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
          <span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
                   <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">dim_per_head</span><span class="p">)])</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BTNH,DNH-&gt;BTD&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
          <span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)])</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BTD,DNH-&gt;BTNH&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
      <span class="n">ret</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">b</span>
    <span class="k">return</span> <span class="n">ret</span></div></div>


<div class="viewcode-block" id="MultiHeadedAttention"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Dot-product attention with multiple attention heads.</span>

<span class="sd">  This implementation heavily uses einsum (wrapped in py_utils.Einsum) to be</span>
<span class="sd">  efficient on TPUs.  We use the following capital letters to denote certain</span>
<span class="sd">  tensor parameters.</span>

<span class="sd">    B = batch size</span>
<span class="sd">    S = length of the key/value (source)</span>
<span class="sd">    T = length of the query (target)</span>
<span class="sd">    D = model dimension</span>
<span class="sd">    N = number of attention heads</span>
<span class="sd">    H = dimensions of each attention head.</span>

<span class="sd">  The algorithm is sketched as follows. Each intermediate tensor or weight</span>
<span class="sd">  tensor is annotated with its shape. E.g., Wq, the weight tensor for query&#39;s</span>
<span class="sd">  projection, its shape is [D, N, H].</span>

<span class="sd">  Trainable weights:</span>
<span class="sd">    Wq, Wk, Wv: [D, N, H]</span>
<span class="sd">    Wout: [D, N, H]</span>

<span class="sd">  Input q:[B, T, D]; k:[B, S, D]; v:[B, S, D]</span>
<span class="sd">  q_proj:[B, T, N, H] = einsum(&#39;BTD,DNH-&gt;BTNH&#39;, x, Wq)</span>
<span class="sd">  k_proj:[B, S, N, H] = einsum(&#39;BSD,DNH-&gt;BSNH&#39;, x, Wk)</span>
<span class="sd">  v_proj:[B, S, N, H] = einsum(&#39;BSD,DNH-&gt;BSNH&#39;, x, Wv)</span>
<span class="sd">  logits:[B, N, T, S] = einsum(&#39;BTNH,BSNH-&gt;BNTS&#39;, q_proj, k_proj) / sqrt(H)</span>
<span class="sd">  probs:[B, N, T, S] = softmax(logits)</span>
<span class="sd">  context:[B, T, N, H] = einsum(&#39;BNTS,BSNH-&gt;BTNH&#39;, probs, v_proj)</span>
<span class="sd">  Output y:[B, T, D] = einsum(&#39;BTNH,DNH&gt;BTD&#39;, context, Wout)</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiHeadedAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for _MultiHeadedAttention.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of key nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Num of attention heads.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dropout_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Params for dropout layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;enable_value_proj&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether value v is pre-projected &#39;</span>
        <span class="s1">&#39; before self attention or not.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;enable_per_dim_scale&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;Whether using per_dim_scale or scaling by a constant factor.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
             <span class="s1">&#39;Probability at which we apply dropout to the attention weights.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;proj_tpl&#39;</span><span class="p">,</span> <span class="n">MultiHeadedProjectionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Params for &#39;</span>
             <span class="s1">&#39;projection layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether there is packed input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_bias&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether to use bias for projection layers.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a _MultiHeadedAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="s1">&#39;input_dim is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="s1">&#39;hidden_dim is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">IsExpr</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="p">)</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;hidden_dim: </span><span class="si">%s</span><span class="s1">, num_heads: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
    <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">ProjectInput</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">dim_per_head</span><span class="o">=</span><span class="n">dim_per_head</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">)</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;key&#39;</span><span class="p">,</span> <span class="n">ProjectInput</span><span class="p">())</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;query&#39;</span><span class="p">,</span> <span class="n">ProjectInput</span><span class="p">())</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_value_proj</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">ProjectInput</span><span class="p">())</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;per_dim_scale&#39;</span><span class="p">,</span>
                         <span class="n">PerDimScaleLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim_per_head</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten_dropout&#39;</span><span class="p">,</span>
                       <span class="n">p</span><span class="o">.</span><span class="n">dropout_tpl</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">))</span>
      <span class="c1"># Setting is_output_projection=True to set the projection direction</span>
      <span class="c1"># from hidden dim to input dim.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span>
          <span class="s1">&#39;post&#39;</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
              <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
              <span class="n">num_heads</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
              <span class="n">dim_per_head</span><span class="o">=</span><span class="n">dim_per_head</span><span class="p">,</span>
              <span class="n">is_output_projection</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">use_bias</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">))</span>

<div class="viewcode-block" id="MultiHeadedAttention._AttenLogits"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">per_step_padding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes attention logits.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query: A Tensor of shape [B, T, N, H]</span>
<span class="sd">      key: A Tensor of shape [B, T, N, H]</span>
<span class="sd">      per_step_padding: A Tensor of shape [B, N, T, S] or None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [B, N, T, S]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BTNH,BSNH-&gt;BNTS&#39;</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttention._AttenLogitsOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenLogitsOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogitsOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Attention logits for one single target (query) step.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, N, H].</span>
<span class="sd">      key:      [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      time_step: Current time step.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [S, B, N]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">n</span>

    <span class="c1"># [s, b, n]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,SBNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span></div>

<div class="viewcode-block" id="MultiHeadedAttention.AttenProbs"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.AttenProbs">[docs]</a>  <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query</span><span class="p">,</span>
                 <span class="n">key</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute attention probability.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, T, N, H].</span>
<span class="sd">      key:      [B, S, N, H].</span>
<span class="sd">      paddings: [B, S].</span>
<span class="sd">      segment_mask: [B, 1, T, S]: A mask that is applied to prevent</span>
<span class="sd">        attention between different segments. This is already been</span>
<span class="sd">        converted into large negative logits. Only applied if</span>
<span class="sd">        packed_input = True.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, T, S] if</span>
<span class="sd">        not None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits: [B, N, T, S].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasRank</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">segment_mask</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">segment_mask</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">):</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttenLogits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">per_step_padding</span><span class="p">)</span>

    <span class="c1"># Apply segment mask.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">and</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Paddings have been included in segment_mask.</span>
      <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">+</span> <span class="n">segment_mask</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Exclude padding frames.</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="p">]),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">per_step_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">per_step_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">paddings</span> <span class="o">+=</span> <span class="n">per_step_padding</span>

      <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">*</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">*</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
      <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">paddings</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">padded_logits</span><span class="p">)</span>

    <span class="n">probs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">probs</span></div>

<div class="viewcode-block" id="MultiHeadedAttention._AttenContext"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenContext">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNTS,BSNH-&gt;BTNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttention._AttenContextOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenContextOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenContextOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">time_step</span><span class="p">):</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">n</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;SBN,SBNH-&gt;BNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span></div>

<div class="viewcode-block" id="MultiHeadedAttention._DotAtten"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._DotAtten">[docs]</a>  <span class="k">def</span> <span class="nf">_DotAtten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">theta</span><span class="p">,</span>
                <span class="n">query</span><span class="p">,</span>
                <span class="n">key</span><span class="p">,</span>
                <span class="n">value</span><span class="p">,</span>
                <span class="n">paddings</span><span class="p">,</span>
                <span class="n">segment_mask</span><span class="p">,</span>
                <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Main attention function.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, T, N, H].</span>
<span class="sd">      key:      [B, S, N, H].</span>
<span class="sd">      value:    [B, S, N, H].</span>
<span class="sd">      paddings: [B, S].</span>
<span class="sd">      segment_mask: [B, 1, T, S]: A mask that is applied to prevent</span>
<span class="sd">        attention between different segments. This is already been</span>
<span class="sd">        converted into large negative logits. Only applied if</span>
<span class="sd">        packed_input = True.</span>

<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, T, S] if</span>
<span class="sd">        not None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, T, N, H].</span>
<span class="sd">      atten_probs: [B, N, T, S].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Scale the query projection.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">:</span>
      <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">query</span> <span class="o">*=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>

    <span class="c1"># Compute prob with shape [batch, heads, target_time, source_time].</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;probs&#39;</span><span class="p">):</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">AttenProbs</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">segment_mask</span><span class="p">,</span>
                              <span class="n">per_step_padding</span><span class="p">)</span>
      <span class="c1"># Apply dropout to probs.</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">atten_dropout</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>

    <span class="c1"># Compute the attention context vector.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;ctx&#39;</span><span class="p">):</span>
      <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttenContext</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">probs</span></div>

<div class="viewcode-block" id="MultiHeadedAttention._DotAttenOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._DotAttenOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_DotAttenOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">theta</span><span class="p">,</span>
                       <span class="n">query</span><span class="p">,</span>
                       <span class="n">key</span><span class="p">,</span>
                       <span class="n">value</span><span class="p">,</span>
                       <span class="n">paddings</span><span class="p">,</span>
                       <span class="n">segment_mask</span><span class="p">,</span>
                       <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">time_step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Dot attention function for queries with 1 time step.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, 1, N, H].</span>
<span class="sd">      key:      [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      value:    [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      paddings: [B, S].</span>
<span class="sd">      segment_mask: [B, 1, T, S]: A mask that is applied to prevent</span>
<span class="sd">        attention between different segments. This is already been</span>
<span class="sd">        converted into large negative logits. Only applied if</span>
<span class="sd">        packed_input = True.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, 1, S] if</span>
<span class="sd">        not None.</span>
<span class="sd">      time_step: Current time step.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, 1, N, H].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Scale the query projection.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">:</span>
      <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">query</span> <span class="o">*=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>

    <span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasRank</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">t</span> <span class="o">==</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">per_step_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">paddings</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">per_step_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">query</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">]),</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">*</span> <span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">*</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_LongSeq</span><span class="p">():</span>
      <span class="sd">&quot;&quot;&quot;For long sequence, directly apply to the entire tensor with padding.&quot;&quot;&quot;</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttenLogitsOneStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">)</span>

      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">pad</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">padded_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">])</span>

      <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttenContextOneStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">time_step</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_ShortSeq</span><span class="p">():</span>
      <span class="sd">&quot;&quot;&quot;For short sequence, using while loop for early exit.&quot;&quot;&quot;</span>

      <span class="k">def</span> <span class="nf">_AttenStep</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">ts</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes logits for attention prob for one step.</span>

<span class="sd">        Args:</span>
<span class="sd">          o: the output logits of shape [S, B*N]</span>
<span class="sd">          k: cached key of shape [S, B, N*H/128, 8]</span>
<span class="sd">          q: query of shape [B, N, H]</span>
<span class="sd">          ts: a scala tensor to represent time_step</span>

<span class="sd">        Returns:</span>
<span class="sd">          Updated logits and time steps.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">ts</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span> <span class="o">*</span> <span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
            <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">ts</span><span class="p">,</span> <span class="n">ot</span><span class="p">),</span> <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">ts</span> <span class="o">+</span> <span class="mi">1</span>

      <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">_o</span><span class="p">,</span> <span class="n">_k</span><span class="p">,</span> <span class="n">_q</span><span class="p">,</span> <span class="n">ts</span><span class="p">:</span> <span class="n">ts</span> <span class="o">&lt;=</span> <span class="n">time_step</span><span class="p">,</span>
          <span class="n">_AttenStep</span><span class="p">,</span>
          <span class="n">loop_vars</span><span class="o">=</span><span class="p">(</span><span class="n">inplace_ops</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span> <span class="o">*</span> <span class="n">n</span><span class="p">],</span> <span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                       <span class="n">init</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span>
                     <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)))</span>

      <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">pad</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">padded_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">_DotStep</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">ts</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes encoded activation.</span>

<span class="sd">        Args:</span>
<span class="sd">          o: the output activation of shape [B, N, H]</span>
<span class="sd">          p: probabiliy of shape [S, B*N]</span>
<span class="sd">          v: cached value of shape [S, B, N*H/128, 8]</span>
<span class="sd">          ts: a scala tensor to represent time_step</span>

<span class="sd">        Returns:</span>
<span class="sd">          Updated output and time steps.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">o</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">ts</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">ts</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]),</span> <span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">ts</span> <span class="o">+</span> <span class="mi">1</span>

      <span class="n">encoded</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">o</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">ts</span><span class="p">:</span> <span class="n">ts</span> <span class="o">&lt;=</span> <span class="n">time_step</span><span class="p">,</span>
          <span class="n">_DotStep</span><span class="p">,</span>
          <span class="n">loop_vars</span><span class="o">=</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">],</span>
                              <span class="n">probs</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span>
                                                                   <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)))</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_ShortSeq</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_short_seq_opt</span> <span class="k">else</span> <span class="n">_LongSeq</span><span class="p">()</span></div>

<div class="viewcode-block" id="MultiHeadedAttention.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span>
            <span class="n">key_vec</span><span class="p">,</span>
            <span class="n">value_vec</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec: [B, T, D].</span>
<span class="sd">      key_vec:   [B, S, D].</span>
<span class="sd">      value_vec: [B, S, D].</span>
<span class="sd">      paddings:  [B, S].</span>
<span class="sd">      segment_mask: [B, 1, T, S]. A mask only applied if packed_input=True.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, T, T] if</span>
<span class="sd">        not None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, T, D].</span>
<span class="sd">      atten_probs: [B, N, T, S].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If value projection is disabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># Project inputs to key, value and query, respectively has shape</span>
    <span class="c1"># [B, S, N, H], [B, S, N, H], and [B, T, N, H].</span>
    <span class="n">query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="n">key_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">key_vec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_value_proj</span><span class="p">:</span>
      <span class="n">value_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">value_vec</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">value_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
      <span class="n">dh</span> <span class="o">=</span> <span class="n">d</span> <span class="o">//</span> <span class="n">h</span>
      <span class="c1"># TODO(b/119531146): Reshape is inefficient here. Use one-hot matmul</span>
      <span class="c1"># avoids the data formatting. Change this back to reshape once XLA</span>
      <span class="c1"># has optimized reshape performance.</span>
      <span class="n">rhs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="o">//</span> <span class="n">dh</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">value_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
          <span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="o">%</span> <span class="n">dh</span><span class="p">,</span> <span class="n">dh</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">value_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
              <span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dh</span><span class="p">])</span>
      <span class="n">value_proj</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BTD,DNH-&gt;BTNH&#39;</span><span class="p">,</span> <span class="n">value_vec</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">encoded</span><span class="p">,</span> <span class="n">atten_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DotAtten</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_proj</span><span class="p">,</span> <span class="n">key_proj</span><span class="p">,</span>
                                          <span class="n">value_proj</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">segment_mask</span><span class="p">,</span>
                                          <span class="n">per_step_padding</span><span class="p">)</span>
    <span class="c1"># Post projection</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">post</span><span class="p">,</span> <span class="n">encoded</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">atten_probs</span></div>

<div class="viewcode-block" id="MultiHeadedAttention.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_key_vec</span><span class="p">,</span>
                 <span class="n">cached_value_vec</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value vector given the query of the current step.</span>

<span class="sd">    This function is used by autoregressive decoding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:        [B, 1, D].</span>
<span class="sd">      cached_key_vec:   [T, B, N, H].</span>
<span class="sd">      cached_value_vec: [T, B, N, H].</span>
<span class="sd">      paddings:         [B, T], or None if there is no padding.</span>
<span class="sd">      segment_mask:     [B, 1, T, S] or None.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, 1, T] if</span>
<span class="sd">        not None.</span>
<span class="sd">      time_step: A scalar, the current decode step, 0-based.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded:           [B, 1, D].</span>
<span class="sd">      updated_key_vec:   [T, B, N, H].</span>
<span class="sd">      updated_value_vec: [T, B, N, H].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If value projection is disabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_value_proj</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Value projection must be enabled for Transformer &#39;</span>
                       <span class="s1">&#39;machine translation.&#39;</span><span class="p">)</span>

    <span class="n">new_key_vec</span> <span class="o">=</span> <span class="n">query_vec</span>
    <span class="n">new_value_vec</span> <span class="o">=</span> <span class="n">query_vec</span>
    <span class="n">t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">cached_key_vec</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="c1"># Project inputs to key, value and query. Each has shape [B, 1, N, H].</span>
    <span class="n">new_key_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">new_key_vec</span><span class="p">)</span>
    <span class="n">new_value_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">new_value_vec</span><span class="p">)</span>
    <span class="n">query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

    <span class="c1"># The extended_key and extended_value have shape [T, B, N, H].</span>
    <span class="n">cached_key_vec</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span>
        <span class="n">cached_key_vec</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_key_proj</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
    <span class="n">cached_value_vec</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span>
        <span class="n">cached_value_vec</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_value_proj</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
    <span class="n">extended_key</span> <span class="o">=</span> <span class="n">cached_key_vec</span>
    <span class="n">extended_value</span> <span class="o">=</span> <span class="n">cached_value_vec</span>

    <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">new_key_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DotAttenOneStep</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span>
        <span class="n">query_proj</span><span class="p">,</span>
        <span class="n">extended_key</span><span class="p">,</span>
        <span class="n">extended_value</span><span class="p">,</span>
        <span class="n">paddings</span><span class="p">,</span>
        <span class="n">segment_mask</span><span class="p">,</span>
        <span class="n">per_step_padding</span><span class="p">,</span>
        <span class="n">time_step</span><span class="o">=</span><span class="n">time_step</span><span class="p">,</span>
        <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="n">use_short_seq_opt</span><span class="p">)</span>

    <span class="c1"># Post projection.</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">post</span><span class="p">,</span> <span class="n">encoded</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">cached_key_vec</span><span class="p">,</span> <span class="n">cached_value_vec</span></div>

<div class="viewcode-block" id="MultiHeadedAttention.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="c1"># args[0]: [b, t, d], args[1]: [b, s, d], args[2]: [b, s, d],</span>
    <span class="c1"># args[3]: [b, s], args[4]: [b, t, s] if not None</span>
    <span class="n">args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="c1"># O(b * t * s * d) computation for self-attention and there are four</span>
    <span class="c1"># projection layers, two of which has O(b * t * d^2), the other two has</span>
    <span class="c1"># O(b * s * d^2). Each multiple-sum took 2 flops. Approximately</span>
    <span class="c1"># self_attention took 15 flops per element since softmax is expensive.</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="mi">15</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">t</span> <span class="o">*</span> <span class="n">s</span> <span class="o">*</span> <span class="n">d</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">t</span> <span class="o">*</span> <span class="n">d</span> <span class="o">*</span> <span class="n">d</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">s</span> <span class="o">*</span> <span class="n">d</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="n">flops</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">)))</span></div></div>


<div class="viewcode-block" id="MultiHeadedAttentionXL"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadedAttentionXL</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transformer-XL multiheaded attention with relative positional embedding.</span>

<span class="sd">  https://arxiv.org/pdf/1901.02860.pdf section 3.3.</span>

<span class="sd">  Notice this is only intended for self attention.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiHeadedAttentionXL.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttentionXL</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;rel_pos_emb_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Dimension of relative positional embedding.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;skip_term_b&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, skip term_b in the paper section 3.3.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a MultiHeadedAttentionXL object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttentionXL</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">assert</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span> <span class="s1">&#39;Packed input not implemented yet.&#39;</span>

    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalide rel_pos_emb_dim: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>

      <span class="n">emb_params</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">PositionalEmbeddingLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">embedding_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;pos_emb&#39;</span><span class="p">,</span> <span class="n">emb_params</span><span class="p">)</span>

      <span class="c1"># Projection layer for relative position encoding</span>
      <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
      <span class="n">pos_proj_tpl</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">,</span>
          <span class="n">num_heads</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
          <span class="n">dim_per_head</span><span class="o">=</span><span class="n">dim_per_head</span><span class="p">,</span>
          <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;pos_proj&#39;</span><span class="p">,</span> <span class="n">pos_proj_tpl</span><span class="p">)</span>

      <span class="n">u_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_per_head</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="n">v_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_per_head</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="n">u_pc</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">v_pc</span><span class="p">)</span>

<div class="viewcode-block" id="MultiHeadedAttentionXL._AttenLogits"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL._AttenLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">per_step_padding</span><span class="p">):</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># This layer only supports self attention.</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>

    <span class="c1"># [1, 2T - 1]</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">t</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relative_pos&#39;</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">FPropWithPosition</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
    <span class="c1"># [1, 2T - 1, N, H]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_proj</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
    <span class="c1"># [2T - 1, N, H]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">sin_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">relative_atten_util</span><span class="o">.</span><span class="n">AttenLogitsTransformerXL</span><span class="p">(</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">u</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">skip_term_b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionXL._AttenLogitsOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL._AttenLogitsOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogitsOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Attention logits for one single target (query) step.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, N, H].</span>
<span class="sd">      key:      [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      time_step: Current time step.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [S, B, N]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">n</span>

    <span class="c1"># Transformer_XL relative attention.</span>
    <span class="k">if</span> <span class="n">time_step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`time_step` can not be None when using relative &#39;</span>
                       <span class="s1">&#39;position encoding in attention.&#39;</span><span class="p">)</span>
    <span class="c1"># term a and c.</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,SBNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">u</span><span class="p">,</span>
                       <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">time_step</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># [1, s, emb_dim]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">FPropWithPosition</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">,</span> <span class="n">position</span><span class="p">)</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_proj</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
    <span class="c1"># [s, n, h]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">sin_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># term b an d.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">skip_term_b</span><span class="p">:</span>
      <span class="n">logits</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,SNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">logits</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;NH,SNH-&gt;SN&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionXL.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_key_vec</span><span class="p">,</span>
                 <span class="n">cached_value_vec</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># TODO(jamesqin): support use_short_seq_opt for TransofrmerXL attention.</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">use_short_seq_opt</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttentionXL</span><span class="p">,</span>
                 <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_key_vec</span><span class="p">,</span>
                                  <span class="n">cached_value_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">segment_mask</span><span class="p">,</span>
                                  <span class="n">per_step_padding</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
                                  <span class="n">use_short_seq_opt</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="MultiHeadedAttentionRPE"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadedAttentionRPE</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiheaded attention with relative positional embedding ...</span>

<span class="sd">  See https://arxiv.org/pdf/1803.02155.pdf.</span>

<span class="sd">  Notice this is only intended for self attention.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiHeadedAttentionRPE.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttentionRPE</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;rel_pos_emb_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Dimension of relative positional embedding.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;rel_pos_radius&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Relative distance is clipped to [-radius, radius].&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;skip_value_emb&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If skipping value positional embedding.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_global_emb&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;If using global relative positional embedding. Only effective if &#39;</span>
        <span class="s1">&#39;`rel_pos_emb_tpl` is not None.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a MultiHeadedAttentionRPE object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttentionRPE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">assert</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span> <span class="s1">&#39;Packed input not implemented yet.&#39;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_radius</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid rel_pos_radius: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_radius</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">rel_pos_emb_dim</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">rel_pos_emb_dim</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span>

    <span class="n">rel_pos_emb_tpl</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">RelativePositionalEmbeddingLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">radius</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">rel_pos_radius</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">rel_pos_emb_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rel_pos_emb_dim</span> <span class="o">!=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">:</span>
      <span class="c1"># Projection layer for relative position encoding</span>
      <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
      <span class="n">pos_proj_tpl</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">rel_pos_emb_dim</span><span class="p">,</span>
          <span class="n">num_heads</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
          <span class="n">dim_per_head</span><span class="o">=</span><span class="n">dim_per_head</span><span class="p">,</span>
          <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">pos_proj_tpl</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span>
        <span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">AUTO_REUSE</span> <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">use_global_emb</span> <span class="k">else</span> <span class="kc">False</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;key_emb&#39;</span><span class="p">,</span> <span class="n">rel_pos_emb_tpl</span><span class="p">)</span>
      <span class="c1"># Add projection layer if rel_pos_emb_dim is different from hidden_dim.</span>
      <span class="k">if</span> <span class="n">pos_proj_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;key_pos_proj&#39;</span><span class="p">,</span> <span class="n">pos_proj_tpl</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">skip_value_emb</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;value_emb&#39;</span><span class="p">,</span> <span class="n">rel_pos_emb_tpl</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pos_proj_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;value_pos_proj&#39;</span><span class="p">,</span> <span class="n">pos_proj_tpl</span><span class="p">)</span>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._RelativePositionValueEmb"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._RelativePositionValueEmb">[docs]</a>  <span class="k">def</span> <span class="nf">_RelativePositionValueEmb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets relative positional value embedding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      key: The attention key, a tensor of shape [batch, seqlen, dim]</span>

<span class="sd">    Returns:</span>
<span class="sd">      Relative positional embedding, a Tensor of shape</span>
<span class="sd">      [tgt_time=seqlen, src_time=seqlen, num_heads, attenion_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">emb_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_emb</span>
    <span class="n">emb_theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">value_emb</span>

    <span class="n">seqlen</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">src_time_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">seqlen</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">seqlen</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">tgt_time_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">seqlen</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">])</span>

    <span class="c1"># [tgt_time=T, src_time=T, num_heads x hidden_dim]</span>
    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">emb_layer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">emb_theta</span><span class="p">,</span> <span class="n">src_time_indices</span> <span class="o">-</span> <span class="n">tgt_time_indices</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">tgt_time</span><span class="p">,</span> <span class="n">src_time</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">)</span>

    <span class="n">pos_proj_layer</span> <span class="o">=</span> <span class="s1">&#39;value_pos_proj&#39;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos_proj_layer</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos_proj_layer</span><span class="p">)</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="nb">getattr</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">pos_proj_layer</span><span class="p">),</span> <span class="n">pos_emb</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">pos_emb</span><span class="p">,</span>
          <span class="p">[</span><span class="n">tgt_time</span><span class="p">,</span> <span class="n">src_time</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">])</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._AttenLogits"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">per_step_padding</span><span class="p">):</span>
    <span class="c1"># TODO(jamesqin): optimize it.</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># This layer only supports self attention.</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>

    <span class="c1"># [1, 2T - 1]</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">t</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># [1, 2T - 1, rel_pos_emb_dim]</span>
    <span class="n">abs_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_emb</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key_emb</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;key_pos_proj&#39;</span><span class="p">):</span>
      <span class="c1"># [1, 2T - 1, N, H]</span>
      <span class="n">abs_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key_pos_proj</span><span class="p">,</span> <span class="n">abs_emb</span><span class="p">)</span>
      <span class="c1"># [2T - 1, N, H]</span>
      <span class="n">abs_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">abs_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">abs_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">abs_emb</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">relative_atten_util</span><span class="o">.</span><span class="n">AttenLogitsRPE</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">abs_emb</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._AttenLogitsOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenLogitsOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogitsOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Attention logits for one single target (query) step.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, N, H].</span>
<span class="sd">      key:      [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      time_step: Current time step.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [S, B, N]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">n</span>

    <span class="c1"># Transformer_XL relative attention.</span>
    <span class="k">if</span> <span class="n">time_step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`time_step` can not be None when using relative &#39;</span>
                       <span class="s1">&#39;position encoding in attention.&#39;</span><span class="p">)</span>
    <span class="c1"># Gets positional embedding.</span>
    <span class="c1"># [1, S]</span>
    <span class="n">rel_dists</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">time_step</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># [1, S, rel_pos_emb_dim]</span>
    <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_emb</span><span class="o">.</span><span class="n">FPropDefaultTheta</span><span class="p">(</span><span class="n">rel_dists</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;key_pos_proj&#39;</span><span class="p">):</span>
      <span class="c1"># [1, S, N, H]</span>
      <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key_pos_proj</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">)</span>
      <span class="c1"># [S, 1, N, H]</span>
      <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,SBNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span>
                     <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span> <span class="o">+</span> <span class="n">pos_emb</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._AttenContext"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenContext">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="c1"># TODO(jamesqin): optimize it.</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNij,BjNH-&gt;BiNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">skip_value_emb</span><span class="p">:</span>
      <span class="n">encoded</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNij,ijNH-&gt;BiNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span>
                           <span class="bp">self</span><span class="o">.</span><span class="n">_RelativePositionValueEmb</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">encoded</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._AttenContextOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenContextOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenContextOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">time_step</span><span class="p">):</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">n</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;SBN,SBNH-&gt;BNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">skip_value_emb</span><span class="p">:</span>
      <span class="c1"># [1, S]</span>
      <span class="n">rel_dists</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">time_step</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
      <span class="c1"># [1, S, rel_pos_emb_dim]</span>
      <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_emb</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value_emb</span><span class="p">,</span> <span class="n">rel_dists</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;value_pos_proj&#39;</span><span class="p">):</span>
        <span class="c1"># [1, S, N, H]</span>
        <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value_pos_proj</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">)</span>
        <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
      <span class="n">logits</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;SBN,SNH-&gt;BNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_key_vec</span><span class="p">,</span>
                 <span class="n">cached_value_vec</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># TODO(jamesqin): support use_short_seq_opt.</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">use_short_seq_opt</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttentionRPE</span><span class="p">,</span>
                 <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_key_vec</span><span class="p">,</span>
                                  <span class="n">cached_value_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">segment_mask</span><span class="p">,</span>
                                  <span class="n">per_step_padding</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
                                  <span class="n">use_short_seq_opt</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">return</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="LocalCausalSelfAttention"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalCausalSelfAttention">[docs]</a><span class="k">class</span> <span class="nc">LocalCausalSelfAttention</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Dot-product causal self attention using a sliding window.</span>

<span class="sd">  We use the following capital letters to denote certain</span>
<span class="sd">  tensor parameters.</span>

<span class="sd">    B = batch size</span>
<span class="sd">    S=T= length of the key/value (source) and query (target)</span>
<span class="sd">    D = model dimension</span>
<span class="sd">    N = number of attention heads</span>
<span class="sd">    H = dimensions of each attention head</span>
<span class="sd">    W = block size</span>
<span class="sd">    L = left context size, including left L-1 positions and self</span>
<span class="sd">    R = right context size</span>
<span class="sd">    F = L + R = context size of one position.</span>
<span class="sd">    C = L + R + W - 1 = context size of a block of W positions.</span>
<span class="sd">    U = ceiling(T/W).</span>

<span class="sd">  The key difference to base class is on calculating logits:</span>
<span class="sd">    Base class:</span>
<span class="sd">      1)  Compute the full S x T attention.</span>
<span class="sd">      2)  Apply a S x T mask to enforce local attention window.</span>
<span class="sd">    This implementation:</span>
<span class="sd">      1)  Compute a W x C attention for each of the U blocks. Where the i-th</span>
<span class="sd">      block has query[W*i:W*(i+1)] and key[W*(i-1)-L-1:W*(i+1)+R].</span>
<span class="sd">      2)  Apply a W x C mask for each block.</span>

<span class="sd">  Effectively, we reduce both time and space complexities for computing the</span>
<span class="sd">  sliding window attention from O(S * T) to O(S * C). In practice we observe</span>
<span class="sd">  reduced HBM usage on TPU but no speed gains.</span>

<span class="sd">  Note: Cross attention is not supported. As a result in speech models this</span>
<span class="sd">  class can only be used for encoder.</span>

<span class="sd">  TODO(weihan): add masking based local attention to the base class.</span>

<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="LocalCausalSelfAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalCausalSelfAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for LocalCausalSelfAttention.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">LocalCausalSelfAttention</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Size of a processing block, if unset, default to &#39;</span>
        <span class="s1">&#39;max(1, left_context-1).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;left_context&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Number of left positions to attend &#39;</span>
        <span class="s1">&#39;(including current position).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;right_context&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of right positions to attend.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a LocalCausalSelfAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">LocalCausalSelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Left context should be at least one.&#39;</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span> <span class="s1">&#39;Packed input not implemented yet.&#39;</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">block_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;block_size not set, use default value </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">))</span>

    <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span> <span class="s1">&#39;Packed input not implemented yet.&#39;</span>

<div class="viewcode-block" id="LocalCausalSelfAttention._AttenLogits"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalCausalSelfAttention._AttenLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BUTNH,BUSNH-&gt;BNUTS&#39;</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocalCausalSelfAttention.AttenProbs"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalCausalSelfAttention.AttenProbs">[docs]</a>  <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query</span><span class="p">,</span>
                 <span class="n">key</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">unused_per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute attention probability.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, T, N, H].</span>
<span class="sd">      key:      [B, S=T, N, H].</span>
<span class="sd">      paddings: [B, T].</span>
<span class="sd">      segment_mask: [B, 1, T, S] not used right now.</span>
<span class="sd">      unused_per_step_padding: Not used.</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits: [B, U, N, W, 2 * W]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasRank</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">])</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>

    <span class="c1"># -&gt; [B, U, C, N, H]</span>
    <span class="n">key_block_context</span> <span class="o">=</span> <span class="n">relative_atten_util</span><span class="o">.</span><span class="n">ExtractBlockContext</span><span class="p">(</span>
        <span class="n">key</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">left_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span>
        <span class="n">right_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key_block_context</span><span class="p">)</span>

    <span class="c1"># -&gt; [B, U, W, N, H]</span>
    <span class="n">query_blocks</span> <span class="o">=</span> <span class="n">relative_atten_util</span><span class="o">.</span><span class="n">ConvertToBlocks</span><span class="p">(</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_blocks</span><span class="p">)</span>

    <span class="c1"># -&gt; [B, U, C]</span>
    <span class="n">paddings_block_context</span> <span class="o">=</span> <span class="n">relative_atten_util</span><span class="o">.</span><span class="n">ExtractBlockContext</span><span class="p">(</span>
        <span class="n">paddings</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">left_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span>
        <span class="n">right_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">,</span>
        <span class="n">padding_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># -&gt; [B, N, U, W, C]</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">paddings_block_context</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">]),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Make local casual paddings.</span>
    <span class="c1"># -&gt; [U, W, C]</span>
    <span class="n">local_causal_padding</span> <span class="o">=</span> <span class="n">relative_atten_util</span><span class="o">.</span><span class="n">MakeCausalPadding</span><span class="p">(</span>
        <span class="n">seq_len</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">left_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span>
        <span class="n">right_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">paddings</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">paddings</span> <span class="o">+=</span> <span class="n">local_causal_padding</span>

    <span class="c1"># -&gt; [B, N, U, W, C]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttenLogits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_blocks</span><span class="p">,</span> <span class="n">key_block_context</span><span class="p">)</span>

    <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">*</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">*</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">paddings</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">padded_logits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">probs</span></div>

<div class="viewcode-block" id="LocalCausalSelfAttention._DotAtten"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalCausalSelfAttention._DotAtten">[docs]</a>  <span class="k">def</span> <span class="nf">_DotAtten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">theta</span><span class="p">,</span>
                <span class="n">query</span><span class="p">,</span>
                <span class="n">key</span><span class="p">,</span>
                <span class="n">value</span><span class="p">,</span>
                <span class="n">paddings</span><span class="p">,</span>
                <span class="n">segment_mask</span><span class="p">,</span>
                <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Main attention function.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, T, N, H].</span>
<span class="sd">      key:      [B, S=T, N, H].</span>
<span class="sd">      value:    [B, S=T, N, H].</span>
<span class="sd">      paddings: [B, S=T].</span>
<span class="sd">      segment_mask: [B, 1, S=T, S=T].</span>
<span class="sd">      per_step_padding: A mask of shape [B, T, S=T] if not None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, T, N, H].</span>
<span class="sd">      atten_probs: [B, N, T, S].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Scale the query projection.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">:</span>
      <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">query</span> <span class="o">*=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># -&gt; [B, N, U, W, C]</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">AttenProbs</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">segment_mask</span><span class="p">,</span>
                            <span class="n">per_step_padding</span><span class="p">)</span>

    <span class="c1"># Apply dropout to probs.</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">atten_dropout</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>

    <span class="c1"># -&gt; [B, U, C, N, H]</span>
    <span class="n">value_block_context</span> <span class="o">=</span> <span class="n">relative_atten_util</span><span class="o">.</span><span class="n">ExtractBlockContext</span><span class="p">(</span>
        <span class="n">value</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">left_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span>
        <span class="n">right_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">)</span>

    <span class="c1"># Compute the attention context vector.</span>
    <span class="c1"># -&gt; [B, U, W, N, H]</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNUWC,BUCNH-&gt;BUWNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value_block_context</span><span class="p">)</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">u</span> <span class="o">*</span> <span class="n">w</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
    <span class="c1"># Remove the extra time padding introduced by converting to blocks.</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">encoded</span><span class="p">[:,</span> <span class="p">:</span><span class="n">t0</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">probs</span></div>

<div class="viewcode-block" id="LocalCausalSelfAttention.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalCausalSelfAttention.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_key_vec</span><span class="p">,</span>
                 <span class="n">cached_value_vec</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>

<div class="viewcode-block" id="LocalCausalSelfAttention.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalCausalSelfAttention.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="LocalCausalSelfAttentionXL"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalCausalSelfAttentionXL">[docs]</a><span class="k">class</span> <span class="nc">LocalCausalSelfAttentionXL</span><span class="p">(</span><span class="n">LocalCausalSelfAttention</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Local causal version of transformer-xl self attention.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="LocalCausalSelfAttentionXL.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalCausalSelfAttentionXL.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">LocalCausalSelfAttentionXL</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;rel_pos_emb_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Dimension of relative positional embedding.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;skip_term_b&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, skip term_b in the paper section 3.3.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a LocalCausalSelfAttentionXL object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">LocalCausalSelfAttentionXL</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalide rel_pos_emb_dim: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>

      <span class="n">emb_params</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">PositionalEmbeddingLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">embedding_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;pos_emb&#39;</span><span class="p">,</span> <span class="n">emb_params</span><span class="p">)</span>

      <span class="c1"># Projection layer for relative position encoding</span>
      <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
      <span class="n">pos_proj_tpl</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">,</span>
          <span class="n">num_heads</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
          <span class="n">dim_per_head</span><span class="o">=</span><span class="n">dim_per_head</span><span class="p">,</span>
          <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;pos_proj&#39;</span><span class="p">,</span> <span class="n">pos_proj_tpl</span><span class="p">)</span>

      <span class="n">u_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_per_head</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="n">v_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_per_head</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="n">u_pc</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">v_pc</span><span class="p">)</span>

<div class="viewcode-block" id="LocalCausalSelfAttentionXL._AttenLogits"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalCausalSelfAttentionXL._AttenLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">left_context</span>
    <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">right_context</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">l</span> <span class="o">+</span> <span class="n">r</span>
    <span class="c1"># term a and c</span>
    <span class="n">term_ac</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BUTNH,BUSNH-&gt;BNUTS&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">u</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

    <span class="c1"># term b and d</span>
    <span class="c1"># [1, F]</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="n">r</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">FPropWithPosition</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
    <span class="c1"># [1, F, N, H]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_proj</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
    <span class="c1"># [F, N, H]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">sin_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">skip_term_b</span><span class="p">:</span>
      <span class="c1"># [B, N, U, W, F]</span>
      <span class="n">term_bd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BUWNH,FNH-&gt;BNUWF&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>

      <span class="c1"># Perform relative shift in order to get [B, N, U, W, C]</span>
      <span class="c1"># Pads the input to [B, N, U, C, C+1]</span>
      <span class="n">term_bd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">term_bd</span><span class="p">,</span>
                       <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">-</span> <span class="n">w</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">)))</span>

      <span class="c1"># Reshapes to [B, N, U, C+1, C]. Note the output last dim is 1-smaller</span>
      <span class="c1"># than the input, which &quot;pushses&quot; one element off to the next row for each</span>
      <span class="c1"># row. The accumulated effect is row_i is right-shifted i steps (i&gt;=0).</span>
      <span class="n">term_bd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">term_bd</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>

      <span class="c1"># Keeps useful slices. [B, N, U, W, C]</span>
      <span class="n">term_bd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">term_bd</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># [N, F]</span>
      <span class="n">term_d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;NH,FNH-&gt;NF&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
      <span class="c1"># [N, W, F]</span>
      <span class="n">term_d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">term_d</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="c1"># [N, C, C+1]</span>
      <span class="n">term_d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">term_d</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">-</span> <span class="n">w</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">)))</span>
      <span class="c1"># [N, C+1, C]</span>
      <span class="n">term_d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">term_d</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
      <span class="c1"># Keeps useful slices. [N, W, C]</span>
      <span class="n">term_d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">term_d</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">term_bd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">term_d</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">term_ac</span> <span class="o">+</span> <span class="n">term_bd</span></div></div>


<div class="viewcode-block" id="TransformerAttentionLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer">[docs]</a><span class="k">class</span> <span class="nc">TransformerAttentionLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiheaded attention sub-layer in Transformer layer.</span>

<span class="sd">  Input is first normalized using Layer Normalization. Output of layer</span>
<span class="sd">  normalization is processed using multi-headed attention. And finally, the</span>
<span class="sd">  output of the attention layer is combined with the residual connection.</span>

<span class="sd">  This layer will be used in the following two scenarios:</span>

<span class="sd">  1. Multi-Headed Self-Attention, where attention keys, values (source_vecs) and</span>
<span class="sd">     queries come from the same previous layer output.</span>
<span class="sd">  2. Masked Multi-Headed Self-Attention, where attention keys, values and</span>
<span class="sd">     queries all come from the same previous layer output, but rightward</span>
<span class="sd">     activations are masked to prevent information flow from future. This is the</span>
<span class="sd">     use case for Transformer decoder self-attention layers. Can be activated by</span>
<span class="sd">     setting is_masked flag of this layer.</span>
<span class="sd">  3. Multi-Headed Cross-Attention, where attention keys and values</span>
<span class="sd">     (source_vecs) are coming from a different source (output of the encoder),</span>
<span class="sd">     and queries coming from the previous layer outputs (decoder).</span>

<span class="sd">  We use the same capital letters to denote certain tensor parameters as</span>
<span class="sd">  MultiHeadedAttention class.</span>

<span class="sd">    B = batch size</span>
<span class="sd">    S = length of the key/value (source)</span>
<span class="sd">    T = length of the query (target)</span>
<span class="sd">    D = model dimension</span>
<span class="sd">    N = number of attention heads</span>
<span class="sd">    H = dimensions of each attention head.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TransformerAttentionLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">TransformerAttentionLayer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the transformer block input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the attention hidden dim.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">&#39;Number of attention heads.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;is_masked&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If set, uses masked MultiHededAttention.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ln_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Layer norm default params&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_tpl&#39;</span><span class="p">,</span>
             <span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(),</span>
             <span class="s1">&#39;Multi-Headed Dot-Product Attention default params&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;dropout_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Residual dropout params template. keep_prop will be reset to &#39;</span>
        <span class="s1">&#39;(1.0 - residual_dropout_prob).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;atten_dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">&#39;Probability at which we apply dropout to the attention probs. &#39;</span>
        <span class="s1">&#39;This practically drops memory values at random positions.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;residual_dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">&#39;Probability at which we apply dropout to the residual layers, &#39;</span>
        <span class="s1">&#39;such that, residual(x, y) = (x + dropout(y)).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;add_unnormalized_input&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;If set, uses unnormalized input in the residual add.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;add_skip_connection&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;If True, add input (or normalized input) to the output.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TransformerAttentionLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="c1"># Initialize multiheaded attention.</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;multihead_atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
      <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

      <span class="c1"># Initialize attention layer normalization.</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ln_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten_ln&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;layer_norm&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

      <span class="c1"># Initialize residual dropout.</span>
      <span class="n">dropout_tpl</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">dropout_tpl</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">residual_dropout_prob</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;residual_dropout&#39;</span><span class="p">,</span> <span class="n">dropout_tpl</span><span class="p">)</span>

<div class="viewcode-block" id="TransformerAttentionLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span>
            <span class="n">source_vecs</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">per_step_padding_override</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the result of Transformer attention layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:   [B, T, D].</span>
<span class="sd">      source_vecs: [B, S, D] (cross_attention) or None (self-attention).</span>
<span class="sd">      paddings:    [B, S].</span>
<span class="sd">      per_step_padding_override: [B, T, T] for self attention or</span>
<span class="sd">                                 [B, T, S] for cross attention.</span>
<span class="sd">      segment_mask: [B, 1, T, S].</span>

<span class="sd">    Returns:</span>
<span class="sd">      output: [B, T, D].</span>
<span class="sd">      atten_probs: [B, N, T, S].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">unnormalized_query_vec</span> <span class="o">=</span> <span class="n">query_vec</span>

    <span class="c1"># Layer normalization.</span>
    <span class="n">query_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

    <span class="c1"># For self-attention: keys = queries.</span>
    <span class="k">if</span> <span class="n">source_vecs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">query_vec</span>

    <span class="c1"># Generates mask, with shape [b, t, t].</span>
    <span class="k">if</span> <span class="n">per_step_padding_override</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_masked</span> <span class="ow">and</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># causal padding.</span>
        <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">CausalPadding</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span>
            <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">per_step_padding</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">per_step_padding_override</span>

    <span class="c1"># Multiheaded attention.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">):</span>
      <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">atten_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span>
          <span class="n">query_vec</span><span class="p">,</span>  <span class="c1"># query</span>
          <span class="n">source_vecs</span><span class="p">,</span>  <span class="c1"># key</span>
          <span class="n">source_vecs</span><span class="p">,</span>  <span class="c1"># value</span>
          <span class="n">paddings</span><span class="p">,</span>
          <span class="n">segment_mask</span><span class="o">=</span><span class="n">segment_mask</span><span class="p">,</span>
          <span class="n">per_step_padding</span><span class="o">=</span><span class="n">per_step_padding</span><span class="p">)</span>

    <span class="c1"># Residual connection.</span>
    <span class="n">ctx_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">residual_dropout</span><span class="p">,</span> <span class="n">ctx_vec</span><span class="p">)</span>
    <span class="n">input_to_add</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">unnormalized_query_vec</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_unnormalized_input</span> <span class="k">else</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_skip_connection</span><span class="p">:</span>
      <span class="n">ctx_vec</span> <span class="o">+=</span> <span class="n">input_to_add</span>
    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">atten_probs</span></div>

<div class="viewcode-block" id="TransformerAttentionLayer.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the result and update cached states for the current step.</span>

<span class="sd">    This function is used by autoregressive decoding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec: [B, 1, D]</span>
<span class="sd">      cached_states: A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding. key   - [T, B,</span>
<span class="sd">        N, H]. value - [T, B, N, H].</span>
<span class="sd">      time_step: A scalar, the current decode step, 0-based.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      cur_output: [B, 1, D]</span>
<span class="sd">      updated_states: A `.NestedMap` object containing the updated states.</span>
<span class="sd">      key   - [T, B, N, H].</span>
<span class="sd">      value - [T, B, N, H].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If not used as masked/causal self-attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_masked</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;ExtendStep should be used only by masked/causal self-attention.&#39;</span><span class="p">)</span>

    <span class="n">t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">cached_states</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">unnormalized_query_vec</span> <span class="o">=</span> <span class="n">query_vec</span>

    <span class="c1"># Generates mask, with shape [b, 1, t].</span>
    <span class="n">zero_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">t</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">t</span><span class="p">],</span> <span class="n">time_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">zero_padding</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">zero_padding</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">per_step_padding</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">per_step_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Layer normalization.</span>
    <span class="n">query_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

    <span class="c1"># Multiheaded masked/causal self-attention.</span>
    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">updated_key_vec</span><span class="p">,</span> <span class="n">updated_value_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_states</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">cached_states</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="kc">None</span><span class="p">,</span> <span class="n">per_step_padding</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">use_short_seq_opt</span><span class="p">)</span>
    <span class="n">updated_states</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">key</span><span class="o">=</span><span class="n">updated_key_vec</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">updated_value_vec</span><span class="p">)</span>

    <span class="c1"># Residual connection.</span>
    <span class="n">ctx_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">residual_dropout</span><span class="p">,</span> <span class="n">ctx_vec</span><span class="p">)</span>
    <span class="n">input_to_add</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">unnormalized_query_vec</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_unnormalized_input</span> <span class="k">else</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_skip_connection</span><span class="p">:</span>
      <span class="n">ctx_vec</span> <span class="o">+=</span> <span class="n">input_to_add</span>
    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">updated_states</span></div></div>


<div class="viewcode-block" id="TransformerLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer">[docs]</a><span class="k">class</span> <span class="nc">TransformerLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transformer layer with multiheaded attention.</span>

<span class="sd">  Applies self-attention followed by a cross-attention and feed forward layer.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TransformerLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">TransformerLayer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;has_aux_atten&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If set, introduces a second attention layer&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;mask_self_atten&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If True, use masked self-attention.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the transformer block input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;output_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the transformer block output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;tr_atten_tpl&#39;</span><span class="p">,</span>
             <span class="n">TransformerAttentionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(),</span>
             <span class="s1">&#39;Transformer Attention Layer params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;tr_self_atten_tpl&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Attention template for self attention. If unset, use tr_atten_tpl.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;tr_fflayer_tpl&#39;</span><span class="p">,</span>
        <span class="n">layers_with_attention</span><span class="o">.</span><span class="n">TransformerFeedForwardLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
            <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">2048</span><span class="p">),</span> <span class="s1">&#39;Transformer Feed-Forward Layer params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, each training example may pack multiple sequences.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TransformerLayer.SetNumInputNodes"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.SetNumInputNodes">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetNumInputNodes</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">num_input_nodes</span><span class="p">):</span>
    <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">num_input_nodes</span></div>

<div class="viewcode-block" id="TransformerLayer.NumOutputNodes"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.NumOutputNodes">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">NumOutputNodes</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TransformerLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="c1"># Initialize masked multi-headed self-attention</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">self_atten_tpl</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">self_atten_tpl</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_atten_tpl</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">self_atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;multihead_self_atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">is_masked</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">mask_self_atten</span>
      <span class="n">params</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;self_atten&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span><span class="p">:</span>
        <span class="c1"># Initialize multi-headed cross-attention</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
        <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;multihead_cross_atten&#39;</span>
        <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
        <span class="n">params</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;cross_atten&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

      <span class="c1"># Initialize feed-forward layer</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;tr_fflayer&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;fflayer&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<div class="viewcode-block" id="TransformerLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">aux_vec</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">aux_paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">per_step_padding_override</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">aux_segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer decoder layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:    [target_batch, target_time, dim].</span>
<span class="sd">      paddings:     [target_batch, target_time].</span>
<span class="sd">      aux_vec:      [source_batch, source_time, dim].</span>
<span class="sd">      aux_paddings: [source_batch, source_time].</span>
<span class="sd">      per_step_padding_override: [target_batch, target_time, target_time].</span>
<span class="sd">      segment_mask:     [target_batch, 1, target_time, target_time]</span>
<span class="sd">      aux_segment_mask: [source_batch, 1, target_time, source_time]</span>

<span class="sd">    target_batch can be a multiple of source_batch, where samples in</span>
<span class="sd">    target_batch are arranged in the order of [m, source_batch] where m =</span>
<span class="sd">    target_batch / source_batch.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The fflayer output with shape [target_batch, target_time, dim].</span>
<span class="sd">      atten_probs: [B, N, T, S].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># First the self-attention layer.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">aux_segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Need to specify aux_segment_mask &#39;</span>
                                            <span class="s1">&#39;for packed input.&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;self_atten&#39;</span><span class="p">):</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span>
          <span class="n">query_vec</span><span class="p">,</span>
          <span class="kc">None</span><span class="p">,</span>
          <span class="n">paddings</span><span class="p">,</span>
          <span class="n">segment_mask</span><span class="o">=</span><span class="n">segment_mask</span><span class="p">,</span>
          <span class="n">per_step_padding_override</span><span class="o">=</span><span class="n">per_step_padding_override</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;aux_atten&#39;</span><span class="p">):</span>
        <span class="c1"># Next the cross-attention layer.</span>
        <span class="n">target_batch</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">source_batch</span><span class="p">,</span> <span class="n">source_time</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
        <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">cross_atten</span><span class="p">,</span>
            <span class="n">atten_vec</span><span class="p">,</span>
            <span class="n">aux_vec</span><span class="p">,</span>
            <span class="n">aux_paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="n">aux_segment_mask</span><span class="p">)</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">atten_probs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">atten_probs</span><span class="p">,</span>
            <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">source_time</span><span class="p">])</span>
        <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">atten_probs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
        <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">atten_probs</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">source_time</span><span class="p">])</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>

    <span class="c1"># Finally the feed-forward layer.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;fflayer&#39;</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">),</span> <span class="n">atten_probs</span></div>

<div class="viewcode-block" id="TransformerLayer.InitStates"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.InitStates">[docs]</a>  <span class="k">def</span> <span class="nf">InitStates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">target_max_length</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">atten_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="k">else</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">atten_dim</span><span class="p">:</span>  <span class="c1"># Check for Pathways as atten_tpl.hidden_dim is not set.</span>
      <span class="n">atten_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">atten_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
    <span class="c1"># TODO(shafey): Determine if we want to make the cached shape 128 to</span>
    <span class="c1"># avoid padding and more efficient interpolation in beamsearch.</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">key</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_max_length</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span>
                   <span class="n">dim_per_head</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="n">value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_max_length</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span>
                   <span class="n">dim_per_head</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span></div>

<div class="viewcode-block" id="TransformerLayer.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">aux_vec</span><span class="p">,</span>
                 <span class="n">aux_paddings</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer decoder layer, extend one step in autoregressive decoding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:    [target_batch, 1, dim].</span>
<span class="sd">      aux_vec:      [source_batch, source_time, dim]</span>
<span class="sd">      aux_paddings: [source_batch, source_time]</span>
<span class="sd">      cached_states: A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding. key   -</span>
<span class="sd">        [target_time, target_batch, num_heads, dim_per_head]. value -</span>
<span class="sd">        [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      time_step: A scalar, the current decode step, 0-based.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      cur_output: [target_batch, 1, dim]</span>
<span class="sd">      updated_states: A `.NestedMap` object containing the updated states.</span>
<span class="sd">      key   - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      value - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">target_batch</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">source_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># First the self-attention layer.</span>
    <span class="n">atten_vec</span><span class="p">,</span> <span class="n">updated_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_states</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
        <span class="n">use_short_seq_opt</span><span class="p">)</span>

    <span class="c1"># Next the cross-attention layer.</span>
    <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
    <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">cross_atten</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">aux_vec</span><span class="p">,</span>
                                          <span class="n">aux_paddings</span><span class="p">)</span>
    <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Finally the feed-forward layer.</span>
    <span class="n">cur_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">target_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">atten_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cur_output</span><span class="p">,</span> <span class="n">updated_states</span></div></div>


<span class="c1"># mt_attention_layer.MultiHeadedAttentionXL</span>
<span class="n">ATTEN_TRANSFORMER_XL</span> <span class="o">=</span> <span class="s1">&#39;transformer_xl&#39;</span>
<span class="c1"># mt_attention_layer.MultiHeadedAttentionRPE</span>
<span class="n">ATTEN_RPE</span> <span class="o">=</span> <span class="s1">&#39;rpe&#39;</span>


<div class="viewcode-block" id="UseRelativeAttentionInTransformerLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.UseRelativeAttentionInTransformerLayer">[docs]</a><span class="k">def</span> <span class="nf">UseRelativeAttentionInTransformerLayer</span><span class="p">(</span><span class="n">transformer_params</span><span class="p">,</span>
                                           <span class="n">rel_pos_emb_dim</span><span class="p">,</span>
                                           <span class="n">atten_type</span><span class="o">=</span><span class="n">ATTEN_TRANSFORMER_XL</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Uses transformer-xl attention for self attention of a transformer layer.</span>

<span class="sd">  Args:</span>
<span class="sd">    transformer_params: A mt_attention_layer.TransformerLayer.Params() object.</span>
<span class="sd">    rel_pos_emb_dim: (int) Relative positional embedding dim to be set.</span>
<span class="sd">    atten_type: (string) Attention type. Supported:</span>
<span class="sd">      - &#39;transformer_xl&#39;: mt_attention_layer.MultiHeadedAttentionXL</span>
<span class="sd">      - &#39;rpe&#39;: mt_attention_layer.MultiHeadedAttentionRPE</span>

<span class="sd">  Returns:</span>
<span class="sd">    A mt_attention_layer.TransformerLayer.Params() object with relative pos emb.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">transformer_params</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span> <span class="n">TransformerLayer</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unsupported input transformer layer: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                     <span class="n">transformer_params</span><span class="o">.</span><span class="n">cls</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">atten_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ATTEN_TRANSFORMER_XL</span><span class="p">,</span> <span class="n">ATTEN_RPE</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Relative attention type: </span><span class="si">%s</span><span class="s1"> unsupported&#39;</span> <span class="o">%</span> <span class="n">atten_type</span><span class="p">)</span>

  <span class="c1"># Gets multiheaded attention tpl from self attention config in transformer.</span>
  <span class="n">trans_params_copy</span> <span class="o">=</span> <span class="n">transformer_params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="o">=</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span>

  <span class="c1"># If already using relative attention class.</span>
  <span class="k">if</span> <span class="n">atten_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="ow">in</span> <span class="p">(</span><span class="n">MultiHeadedAttentionRPE</span><span class="p">,</span> <span class="n">MultiHeadedAttentionXL</span><span class="p">,</span>
                       <span class="n">LocalCausalSelfAttentionXL</span><span class="p">):</span>
    <span class="n">atten_tpl</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="o">=</span> <span class="n">rel_pos_emb_dim</span>
    <span class="k">return</span> <span class="n">trans_params_copy</span>

  <span class="k">if</span> <span class="n">atten_type</span> <span class="o">==</span> <span class="n">ATTEN_TRANSFORMER_XL</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">atten_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">MultiHeadedAttention</span><span class="p">:</span>
      <span class="n">rel_atten_tpl</span> <span class="o">=</span> <span class="n">MultiHeadedAttentionXL</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">atten_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">LocalCausalSelfAttention</span><span class="p">:</span>
      <span class="n">rel_atten_tpl</span> <span class="o">=</span> <span class="p">(</span><span class="n">LocalCausalSelfAttentionXL</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unsupported attention: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">atten_tpl</span><span class="o">.</span><span class="n">cls</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">atten_type</span> <span class="o">==</span> <span class="n">ATTEN_RPE</span><span class="p">:</span>
    <span class="n">rel_atten_tpl</span> <span class="o">=</span> <span class="n">MultiHeadedAttentionRPE</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>

  <span class="n">rel_atten_tpl</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">CopyFieldsTo</span><span class="p">(</span><span class="n">atten_tpl</span><span class="p">,</span> <span class="n">rel_atten_tpl</span><span class="p">)</span>
  <span class="n">rel_atten_tpl</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="o">=</span> <span class="n">rel_pos_emb_dim</span>

  <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">rel_atten_tpl</span>
  <span class="k">return</span> <span class="n">trans_params_copy</span></div>


<div class="viewcode-block" id="ClearRelativeAttentionInTransformerLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.ClearRelativeAttentionInTransformerLayer">[docs]</a><span class="k">def</span> <span class="nf">ClearRelativeAttentionInTransformerLayer</span><span class="p">(</span><span class="n">transformer_params</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Removes relative position attention in the transformer layer.</span>

<span class="sd">  Args:</span>
<span class="sd">    transformer_params: A mt_attention_layer.TransformerLayer param.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A mt_attention_layer.TransformerLayer param without relative attention.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">transformer_params</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span> <span class="n">TransformerLayer</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unsupported input transformer layer: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                     <span class="n">transformer_params</span><span class="o">.</span><span class="n">cls</span><span class="p">)</span>
  <span class="n">trans_params_copy</span> <span class="o">=</span> <span class="n">transformer_params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="o">=</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="n">attention_tpl</span> <span class="o">=</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span>
  <span class="k">if</span> <span class="n">attention_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">MultiHeadedAttentionXL</span><span class="p">:</span>
    <span class="n">new_attention_tpl</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
  <span class="k">elif</span> <span class="n">attention_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="p">(</span><span class="n">LocalCausalSelfAttentionXL</span><span class="p">):</span>
    <span class="n">new_attention_tpl</span> <span class="o">=</span> <span class="n">LocalCausalSelfAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unsupported attention params: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">attention_tpl</span><span class="o">.</span><span class="n">cls</span><span class="p">)</span>

  <span class="n">new_attention_tpl</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">CopyFieldsTo</span><span class="p">(</span>
      <span class="n">attention_tpl</span><span class="p">,</span> <span class="n">new_attention_tpl</span><span class="p">,</span> <span class="n">skip</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;rel_pos_emb_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;skip_term_b&#39;</span><span class="p">])</span>
  <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">new_attention_tpl</span>
  <span class="k">return</span> <span class="n">trans_params_copy</span></div>


<div class="viewcode-block" id="TransformerDecoderLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerDecoderLayer">[docs]</a><span class="k">class</span> <span class="nc">TransformerDecoderLayer</span><span class="p">(</span><span class="n">TransformerLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transformer decoder layer with multiheaded attention.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TransformerDecoderLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerDecoderLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoderLayer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">p</span><span class="o">.</span><span class="n">mask_self_atten</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">p</span></div></div>


<div class="viewcode-block" id="StackedTransformerLayers"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers">[docs]</a><span class="k">class</span> <span class="nc">StackedTransformerLayers</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A stack of Batch-Major Transformer layers.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="StackedTransformerLayers.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">StackedTransformerLayers</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;has_aux_atten&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If set, introduces a second attention layer&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;mask_self_atten&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If True, use masked self-attention.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_layers&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Num of layers in this stack.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;mdl_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Model dimension in Transformer layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;The hidden layer dimension in Transformer layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_atten_heads&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Num of attention heads.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
             <span class="s1">&#39;Apply dropout at this prob at various places.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;add_unnormalized_input&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;If set, uses unnormalized input in the residual add.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;transformer_layer_params_tpl&#39;</span><span class="p">,</span> <span class="n">TransformerLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;A template of TransformerLayer.params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;final_layer_norm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If true, apply layer normalization to the final output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, each training example may pack multiple sequences.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_fused_layernorm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether to use fused layernorm.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">StackedTransformerLayers</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">mdl_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_atten_heads</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span> <span class="o">&lt;</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">_LayerParams</span><span class="p">(</span><span class="n">ii</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Construct ii-th layer params.&quot;&quot;&quot;</span>
      <span class="n">p_ii</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">transformer_layer_params_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">p</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;layer_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">ii</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">has_aux_atten</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">mask_self_atten</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">mask_self_atten</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">mdl_dim</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">mdl_dim</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_atten_heads</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">residual_dropout_prob</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">add_unnormalized_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">add_unnormalized_input</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">residual_dropout_prob</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">relu_dropout_prob</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span>
      <span class="k">return</span> <span class="n">p_ii</span>

    <span class="n">layer_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">_LayerParams</span><span class="p">(</span><span class="n">ii</span><span class="p">)</span> <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;x_layers&#39;</span><span class="p">,</span> <span class="n">layer_params</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">:</span>
      <span class="n">final_ln_p</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">mdl_dim</span><span class="p">,</span> <span class="n">use_fused_layernorm</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_fused_layernorm</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;final_ln&#39;</span><span class="p">,</span> <span class="n">final_ln_p</span><span class="p">)</span>

<div class="viewcode-block" id="StackedTransformerLayers.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">aux_vec</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">aux_paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">aux_segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Stacked Transformer layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:      [batch, target_time, dim].</span>
<span class="sd">      paddings:       [batch, target_time].</span>
<span class="sd">      aux_vec:        [batch, source_time, dim].</span>
<span class="sd">      aux_paddings:   [batch, source_time].</span>
<span class="sd">      segment_mask:     [batch, 1, target_time, target_time]</span>
<span class="sd">      aux_segment_mask: [batch, 1, target_time, source_time]</span>

<span class="sd">    Returns:</span>
<span class="sd">      The attention context vector with shape [batch, target_time, dim].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">x_out</span> <span class="o">=</span> <span class="n">query_vec</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">x_in</span> <span class="o">=</span> <span class="n">x_out</span>
        <span class="n">x_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">x_layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_in</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span>
                                          <span class="n">aux_vec</span><span class="p">,</span> <span class="n">aux_paddings</span><span class="p">,</span> <span class="n">segment_mask</span><span class="p">,</span>
                                          <span class="n">aux_segment_mask</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">:</span>
      <span class="n">x_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_ln</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">final_ln</span><span class="p">,</span> <span class="n">x_out</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_out</span><span class="p">,</span> <span class="n">paddings</span></div>

<div class="viewcode-block" id="StackedTransformerLayers.InitStates"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers.InitStates">[docs]</a>  <span class="k">def</span> <span class="nf">InitStates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">x_layers</span><span class="o">=</span><span class="p">[</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">InitStates</span><span class="p">(</span><span class="n">layer_theta</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">layer_theta</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_layers</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">x_layers</span><span class="p">)</span>
    <span class="p">])</span></div>

<div class="viewcode-block" id="StackedTransformerLayers.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">aux_vec</span><span class="p">,</span>
                 <span class="n">aux_paddings</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer decoder layer, extend one step in autoregressive decoding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:    [target_batch, 1, dim].</span>
<span class="sd">      aux_vec:      [source_batch, source_time, dim]</span>
<span class="sd">      aux_paddings: [source_batch, source_time]</span>
<span class="sd">      cached_states: A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding.</span>
<span class="sd">        cached_states.x_layers is a list corresponding to self.x_layers, where</span>
<span class="sd">        each element is a NestedMap with attention keys and values:</span>
<span class="sd">        &quot;key&quot;   - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">        &quot;value&quot; - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      time_step: A scalar, the current decode step, 0-based.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      cur_output: The last decoder layer output of shape [target_batch, 1, dim].</span>
<span class="sd">      updated_states: A `.NestedMap` object containing the updated states.</span>
<span class="sd">      updated_states.x_layers is a list corresponding to self.x_layers, where</span>
<span class="sd">      each element is a NestedMap with attention keys and values:</span>
<span class="sd">      &quot;key&quot;   - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      &quot;value&quot; - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">updated_states</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">x_layers</span><span class="o">=</span><span class="p">[])</span>
      <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">query_vec</span>
      <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">layer_theta</span><span class="p">,</span> <span class="n">layer_states</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_layers</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">x_layers</span><span class="p">,</span>
                                                  <span class="n">cached_states</span><span class="o">.</span><span class="n">x_layers</span><span class="p">):</span>
        <span class="n">decoder_output</span><span class="p">,</span> <span class="n">updated_layer_states</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span>
            <span class="n">layer_theta</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">,</span> <span class="n">aux_vec</span><span class="p">,</span> <span class="n">aux_paddings</span><span class="p">,</span> <span class="n">layer_states</span><span class="p">,</span>
            <span class="n">time_step</span><span class="p">,</span> <span class="n">use_short_seq_opt</span><span class="p">)</span>
        <span class="n">updated_states</span><span class="o">.</span><span class="n">x_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">updated_layer_states</span><span class="p">)</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">decoder_output</span>
    <span class="k">return</span> <span class="n">decoder_output</span><span class="p">,</span> <span class="n">updated_states</span></div></div>


<div class="viewcode-block" id="TransformerFeedForwardLayerWithTaskId"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId">[docs]</a><span class="k">class</span> <span class="nc">TransformerFeedForwardLayerWithTaskId</span><span class="p">(</span>
    <span class="n">layers_with_attention</span><span class="o">.</span><span class="n">TransformerFeedForwardLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;TransformerFeedForwardLayer with optional task_id input args.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TransformerFeedForwardLayerWithTaskId.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">TransformerFeedForwardLayerWithTaskId</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_task_ids&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If set, introduces a second attention layer&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TransformerFeedForwardLayerWithTaskId.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Feed-forward, residual and layer-norm.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: [batch, time, dim].</span>
<span class="sd">      paddings: [batch, time]</span>
<span class="sd">      task_id: optional task_id with shape [batch]</span>

<span class="sd">    Returns:</span>
<span class="sd">      tensor of the same shape with inputs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_task_ids</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">task_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Must pass task_id if use_task_ids.&#39;</span><span class="p">)</span>
    <span class="n">inputs_normalized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;res_proj_layer&#39;</span><span class="p">):</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">res_proj_layer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">res_proj_layer</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">expanded_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">fflayer_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs_normalized</span><span class="p">,</span> <span class="n">expanded_paddings</span><span class="p">]</span>
    <span class="n">fflayer_args</span> <span class="o">+=</span> <span class="p">[</span><span class="n">task_id</span><span class="p">]</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_task_ids</span> <span class="k">else</span> <span class="p">[]</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">residual_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span>
                                                   <span class="o">*</span><span class="n">fflayer_args</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">h</span></div></div>


<div class="viewcode-block" id="GPipeTransformerLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer">[docs]</a><span class="k">class</span> <span class="nc">GPipeTransformerLayer</span><span class="p">(</span><span class="n">TransformerLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;GPipe compatible transformer layer.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="GPipeTransformerLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">GPipeTransformerLayer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span> <span class="o">=</span> <span class="n">TransformerFeedForwardLayerWithTaskId</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="GPipeTransformerLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">source_vecs</span><span class="p">,</span>
            <span class="n">source_paddings</span><span class="p">,</span>
            <span class="n">target_vecs</span><span class="p">,</span>
            <span class="n">target_paddings</span><span class="p">,</span>
            <span class="n">source_segment_mask</span><span class="p">,</span>
            <span class="n">target_segment_mask</span><span class="p">,</span>
            <span class="n">transparent_acc</span><span class="p">,</span>
            <span class="n">transparent_acc_helper</span><span class="p">,</span>
            <span class="n">source_task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">target_task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span><span class="p">:</span>  <span class="c1"># Decoder FProp</span>
        <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span>
            <span class="n">target_vecs</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span>
            <span class="n">target_paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="n">target_segment_mask</span><span class="p">)</span>
        <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">cross_atten</span><span class="p">,</span>
            <span class="n">atten_vec</span><span class="p">,</span>
            <span class="n">source_vecs</span><span class="p">,</span>
            <span class="n">source_paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="n">source_segment_mask</span><span class="p">)</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span>
                                       <span class="n">target_paddings</span><span class="p">,</span> <span class="n">target_task_id</span><span class="p">)</span>
        <span class="n">atten_vec</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">target_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_paddings</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">target_paddings</span><span class="p">,</span>
                <span class="n">source_segment_mask</span><span class="p">,</span> <span class="n">target_segment_mask</span><span class="p">,</span> <span class="n">transparent_acc</span><span class="p">,</span>
                <span class="n">transparent_acc_helper</span><span class="p">,</span> <span class="n">source_task_id</span><span class="p">,</span> <span class="n">target_task_id</span><span class="p">)</span>
      <span class="c1"># Encoder FProp</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span>
          <span class="n">source_vecs</span><span class="p">,</span>
          <span class="kc">None</span><span class="p">,</span>
          <span class="n">source_paddings</span><span class="p">,</span>
          <span class="n">segment_mask</span><span class="o">=</span><span class="n">source_segment_mask</span><span class="p">)</span>
      <span class="n">atten_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">source_paddings</span><span class="p">,</span>
                                     <span class="n">source_task_id</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">source_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="k">return</span> <span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">source_paddings</span><span class="p">,</span> <span class="n">target_vecs</span><span class="p">,</span> <span class="n">target_paddings</span><span class="p">,</span>
              <span class="n">source_segment_mask</span><span class="p">,</span> <span class="n">target_segment_mask</span><span class="p">,</span> <span class="n">transparent_acc</span><span class="p">,</span>
              <span class="n">transparent_acc_helper</span><span class="p">,</span> <span class="n">source_task_id</span><span class="p">,</span> <span class="n">target_task_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="GPipeTransformerLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="n">flops_per_element</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">source_batch</span><span class="p">,</span> <span class="n">src_time</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="n">flops_per_element</span> <span class="o">*</span> <span class="n">src_time</span> <span class="o">*</span> <span class="n">src_time</span> <span class="o">*</span> <span class="n">source_batch</span> <span class="o">*</span> <span class="n">dim</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">args</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="n">flops</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,)</span> <span class="o">+</span> <span class="n">args</span><span class="p">)</span></div>

<div class="viewcode-block" id="GPipeTransformerLayer.SetupDeterministicDropout"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer.SetupDeterministicDropout">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetupDeterministicDropout</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Replaced dropout layers in transformer with deterministic ones.&quot;&quot;&quot;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">dropout_tpl</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">dropout_tpl</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">residual_dropout_tpl</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">fflayer_tpl</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">params</span></div>

<div class="viewcode-block" id="GPipeTransformerLayer.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">aux_vec</span><span class="p">,</span>
                 <span class="n">aux_paddings</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer decoder layer, extend one step in autoregressive decoding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:    [target_batch, 1, dim].</span>
<span class="sd">      aux_vec:      [source_batch, source_time, dim]</span>
<span class="sd">      aux_paddings: [source_batch, source_time]</span>
<span class="sd">      cached_states: A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding. key   -</span>
<span class="sd">        [target_time, target_batch, num_heads, dim_per_head]. value -</span>
<span class="sd">        [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      time_step: A scalar, the current decode step, 0-based.</span>
<span class="sd">      task_id: [batch_size]: the input task_id meta information.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      cur_output: [target_batch, 1, dim]</span>
<span class="sd">      updated_states: A `.NestedMap` object containing the updated states.</span>
<span class="sd">      key   - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      value - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">target_batch</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">source_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># First the self-attention layer.</span>
    <span class="n">atten_vec</span><span class="p">,</span> <span class="n">updated_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_states</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
        <span class="n">use_short_seq_opt</span><span class="p">)</span>

    <span class="c1"># Next the cross-attention layer.</span>
    <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
    <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">cross_atten</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">aux_vec</span><span class="p">,</span>
                                          <span class="n">aux_paddings</span><span class="p">)</span>
    <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Finally the feed-forward layer.</span>
    <span class="n">cur_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">target_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">atten_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">task_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cur_output</span><span class="p">,</span> <span class="n">updated_states</span></div></div>


<span class="c1"># pyformat: disable</span>
<div class="viewcode-block" id="Builder"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder">[docs]</a><span class="k">class</span> <span class="nc">Builder</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">Base</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Builder for self-attention layers.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="Builder.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">Builder</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;model_dim&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;Model dim of this layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Number of heads in the atten layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ff_hidden_dim&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;Hidden dim of the feedforward layer&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;residual_dropout_prob&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Dropout prob to the output of each sub-layer before it is added &#39;</span>
             <span class="s1">&#39;to the sub-layer input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ff_activation_fn&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
             <span class="s1">&#39;Activation function in Feedforward layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ff_residual_weight&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Weight given to F(x) in the residual &#39;</span>
             <span class="s1">&#39;connection: y = x + ff_residual_weight * F(x), in Feedforward &#39;</span>
             <span class="s1">&#39;layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;relu_dropout_prob&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Probability at which we apply dropout to the hidden layer of &#39;</span>
             <span class="s1">&#39;feed-forward network.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_dropout_prob&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Probability at which we apply dropout to the attention layer&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;selfatten_add_unnormalized_input&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;Whether to use unnormalized input in the residual add.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;selfatten_enable_value_proj&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;Whether value v is pre-projected before self attention or not.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;conv_activation&#39;</span><span class="p">,</span> <span class="s1">&#39;RELU&#39;</span><span class="p">,</span>
             <span class="s1">&#39;Activation function for convolution layer in Builder.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_splits&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
             <span class="s1">&#39;Number of model parallelism splits.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_micro_batches&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
             <span class="s1">&#39;Number of spatial partition along the batch dimension. &#39;</span>
             <span class="s1">&#39;When num_micro_batches &gt; 1, the effective batch size of the &#39;</span>
             <span class="s1">&#39;intermediate activation is batch_size // num_micro_batches.&#39;</span>
             <span class="s1">&#39;This allows models to try larger batch size which might improve &#39;</span>
             <span class="s1">&#39;model quality&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;glu_with_tanh&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If the Gated Linear Unit should apply tanh on the activation &#39;</span>
             <span class="s1">&#39;input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether to support packed input&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;enable_per_dim_scale&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;Whether using per_dim_scale or scaling by a constant factor.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_fused_layernorm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether to use fused layernorm.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_bias&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether to use bias for projection layer.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Builder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_splits</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">num_micro_batches</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">deterministic_dropout</span>

<div class="viewcode-block" id="Builder._Dropout"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Dropout">[docs]</a>  <span class="k">def</span> <span class="nf">_Dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a DropoutLayer Params.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Builder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">drop_prob</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._Add"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Add">[docs]</a>  <span class="k">def</span> <span class="nf">_Add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">residual_weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">residual_weight</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span>
                    <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._ExpandDims"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._ExpandDims">[docs]</a>  <span class="k">def</span> <span class="nf">_ExpandDims</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),</span>
                    <span class="n">fn_flops</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._Squeeze"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Squeeze">[docs]</a>  <span class="k">def</span> <span class="nf">_Squeeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">:]),</span>
                    <span class="n">fn_flops</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._Glu"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Glu">[docs]</a>  <span class="k">def</span> <span class="nf">_Glu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_GLUFn</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="n">gated_inputs</span><span class="p">,</span> <span class="n">act_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">act_inputs</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gated_inputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_GatedTanhFn</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="n">gated_inputs</span><span class="p">,</span> <span class="n">act_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">act_inputs</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gated_inputs</span><span class="p">)</span>

    <span class="n">fn</span> <span class="o">=</span> <span class="n">_GatedTanhFn</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">glu_with_tanh</span> <span class="k">else</span> <span class="n">_GLUFn</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">fn</span><span class="o">=</span><span class="n">fn</span><span class="p">,</span>
                    <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span><span class="p">]),</span>
                    <span class="n">fn_flops</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">15</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._Pad"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Pad">[docs]</a>  <span class="k">def</span> <span class="nf">_Pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ApplyPadding</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">x</span><span class="p">),</span>
        <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">fn_flops</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">size</span><span class="p">))</span></div>

<div class="viewcode-block" id="Builder._MultiHeadedAtten"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._MultiHeadedAtten">[docs]</a>  <span class="k">def</span> <span class="nf">_MultiHeadedAtten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a MultiHeadedAttention params.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">atten_p</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">atten_dropout_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span>
        <span class="n">enable_value_proj</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">selfatten_enable_value_proj</span><span class="p">,</span>
        <span class="n">enable_per_dim_scale</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">,</span>
        <span class="n">packed_input</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span>
        <span class="n">fprop_dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_bias</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">deterministic_dropout</span><span class="p">:</span>
      <span class="n">atten_p</span><span class="o">.</span><span class="n">dropout_tpl</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">atten_p</span></div>

<div class="viewcode-block" id="Builder.Feedforward"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.Feedforward">[docs]</a>  <span class="k">def</span> <span class="nf">Feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">is_causal</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">sub_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;i.vec-&gt;after_feedforward&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
             <span class="s1">&#39;feedforward&#39;</span><span class="p">,</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_LN</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
                      <span class="n">use_fused_layernorm</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_fused_layernorm</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Linear</span><span class="p">(</span><span class="s1">&#39;linear01&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_hidden_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Bias</span><span class="p">(</span><span class="s1">&#39;bias01&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_hidden_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Activation</span><span class="p">(</span><span class="s1">&#39;act&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_activation_fn</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;relu_dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">relu_dropout_prob</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Linear</span><span class="p">(</span><span class="s1">&#39;linear02&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Bias</span><span class="p">(</span><span class="s1">&#39;bias02&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">residual_dropout_prob</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;i.vec,after_feedforward-&gt;added&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_residual_weight</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;added,i.paddings-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Pad</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.paddings-&gt;o.paddings&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">)),</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">sub_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;i.segment_mask-&gt;o.segment_mask&#39;</span><span class="p">,</span>
                       <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="s1">&#39;segment_mask&#39;</span><span class="p">)))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>  <span class="c1"># input NestedMap with {vec, paddings, segment_mask}</span>
        <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>  <span class="c1"># output NestedMap with {vec, paddings, segment_mask}</span>
        <span class="o">*</span><span class="n">sub_list</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._MaybeSplit"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._MaybeSplit">[docs]</a>  <span class="k">def</span> <span class="nf">_MaybeSplit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_splits</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">num_micro_batches</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

    <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">blocks</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">num_layers</span> <span class="o">&gt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_splits</span>
    <span class="n">layers_per_split</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_splits</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">cells</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="n">blocks</span><span class="p">:</span>
      <span class="n">head</span><span class="p">,</span> <span class="n">blocks</span> <span class="o">=</span> <span class="n">blocks</span><span class="p">[:</span><span class="n">layers_per_split</span><span class="p">],</span> <span class="n">blocks</span><span class="p">[</span><span class="n">layers_per_split</span><span class="p">:]</span>
      <span class="n">cells</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="s1">&#39;cell_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cells</span><span class="p">)),</span> <span class="o">*</span><span class="n">head</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">cells</span><span class="p">)</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">num_splits</span>

    <span class="k">return</span> <span class="n">gpipe</span><span class="o">.</span><span class="n">PipeliningLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">cell_tpl</span><span class="o">=</span><span class="n">cells</span><span class="p">,</span>
        <span class="n">nested_map_fprop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">num_micro_batches</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_micro_batches</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._DepthwiseConv2D"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._DepthwiseConv2D">[docs]</a>  <span class="k">def</span> <span class="nf">_DepthwiseConv2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">filter_size</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A depthwise convolution block for lightweight conv.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">conv_builder_params</span> <span class="o">=</span> <span class="n">conv_layers</span><span class="o">.</span><span class="n">Builder</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">conv_builder</span> <span class="o">=</span> <span class="n">conv_builder_params</span><span class="o">.</span><span class="n">Instantiate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">conv_builder</span><span class="o">.</span><span class="n">DepthwiseConv2D</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">in_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">depth_multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">filter_shape</span><span class="o">=</span><span class="p">[</span><span class="n">filter_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">conv_activation</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._NormalizedDepthwiseConv2D"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._NormalizedDepthwiseConv2D">[docs]</a>  <span class="k">def</span> <span class="nf">_NormalizedDepthwiseConv2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A depthwise convolution block for lightweight conv.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">conv_builder_params</span> <span class="o">=</span> <span class="n">conv_layers</span><span class="o">.</span><span class="n">Builder</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">conv_builder</span> <span class="o">=</span> <span class="n">conv_builder_params</span><span class="o">.</span><span class="n">Instantiate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">conv_builder</span><span class="o">.</span><span class="n">NormalizedDepthwiseConv2D</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">in_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">dropconnect_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span>
        <span class="n">deterministic_dropout</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">deterministic_dropout</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder.LConv"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.LConv">[docs]</a>  <span class="k">def</span> <span class="nf">LConv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">name</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">convolution_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;[DEPRECATED] A lightweight convolution block as described in.</span>

<span class="sd">    Use conv_layer_builder.LConv() instead.</span>

<span class="sd">    https://arxiv.org/abs/1901.10430</span>
<span class="sd">    Corresponding PyTorch Implementation (L587):</span>
<span class="sd">    https://github.com/pytorch/fairseq/blob/v0.6.2/fairseq/models/lightconv.py</span>


<span class="sd">    This block can be used as an alternative to self-attention block.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of the params</span>
<span class="sd">      kernel_size: kernel size used in the conv layer.</span>
<span class="sd">      is_causal: is causal padding or not.</span>
<span class="sd">      convolution_fn: Convolution to apply, default _NormalizedDepthwiseConv2D.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A LightWeightConvLayerBlock layer params.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">convolution_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">convolution_fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_NormalizedDepthwiseConv2D&#39;</span><span class="p">)</span>

    <span class="n">sub_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;i.vec-&gt;pre_conv&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
             <span class="s1">&#39;pre_conv&#39;</span><span class="p">,</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_LN</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
                      <span class="n">use_fused_layernorm</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_fused_layernorm</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Linear</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Bias</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Glu</span><span class="p">(</span><span class="s1">&#39;glu&#39;</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_ExpandDims</span><span class="p">(</span><span class="s1">&#39;expand&#39;</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;pre_conv,i.paddings-&gt;post_conv,o.paddings&#39;</span><span class="p">,</span>
         <span class="n">convolution_fn</span><span class="p">(</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">is_causal</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;post_conv-&gt;after_dropout&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
             <span class="s1">&#39;post_conv&#39;</span><span class="p">,</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Squeeze</span><span class="p">(</span><span class="s1">&#39;squeeze&#39;</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Linear</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Bias</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">residual_dropout_prob</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;i.vec,after_dropout-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)),</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">sub_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;i.segment_mask-&gt;o.segment_mask&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="s1">&#39;segment_mask&#39;</span><span class="p">)))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>  <span class="c1"># input NestedMap with {vec, paddings, segment_mask}</span>
        <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>  <span class="c1"># output NestedMap with {vec, paddings, segment_mask}</span>
        <span class="o">*</span><span class="n">sub_list</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="Builder.LconvBlock"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.LconvBlock">[docs]</a>  <span class="k">def</span> <span class="nf">LconvBlock</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">is_causal</span><span class="p">,</span>
                 <span class="n">convolution_fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A lightweight conv block followed by a feedforward one.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">LConv</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lconv&#39;</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">,</span>
            <span class="n">convolution_fn</span><span class="o">=</span><span class="n">convolution_fn</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Feedforward</span><span class="p">(</span><span class="s1">&#39;ff&#39;</span><span class="p">,</span> <span class="n">is_causal</span><span class="p">))</span></div>

<div class="viewcode-block" id="Builder.Seq"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.Seq">[docs]</a>  <span class="k">def</span> <span class="nf">Seq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="n">subs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a stack of sequential layers.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="n">subs</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder.LConvStack"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.LConvStack">[docs]</a>  <span class="k">def</span> <span class="nf">LConvStack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">kernel_sizes</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a stack of LConv layers with kernel size in kernel_sizes.&quot;&quot;&quot;</span>
    <span class="n">blocks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kernel_sizes</span><span class="p">):</span>
      <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">LconvBlock</span><span class="p">(</span>
              <span class="n">name</span><span class="o">=</span><span class="s1">&#39;block_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
              <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
              <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">,</span>
              <span class="n">convolution_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MaybeSplit</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="n">blocks</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._Stride"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Stride">[docs]</a>  <span class="k">def</span> <span class="nf">_Stride</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Strides the input sequence.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of this layer.</span>
<span class="sd">      stride: To use every k-th token, set the stride to k. When stride == 0,</span>
<span class="sd">        only returns the first token of the input. When stride == 1, returns</span>
<span class="sd">        every token in the input.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A layer params that does stride.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
          <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span>
          <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),</span>
          <span class="n">fn_flops</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">::</span><span class="n">stride</span><span class="p">],</span>
        <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),</span>
        <span class="n">fn_flops</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._StridedAttention"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._StridedAttention">[docs]</a>  <span class="k">def</span> <span class="nf">_StridedAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes self attention with optional stride.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of this layer.</span>
<span class="sd">      stride: If omitted, the default is 1: use every token in the query. To use</span>
<span class="sd">        every k-th token, set the stride to k. When set to 0, only use the first</span>
<span class="sd">        token of the query.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A self attention layer params.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">input_to_add</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;i.vec&#39;</span>
                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">selfatten_add_unnormalized_input</span> <span class="k">else</span> <span class="s1">&#39;after_ln&#39;</span><span class="p">)</span>

    <span class="n">attention_inputs</span> <span class="o">=</span> <span class="s1">&#39;strided_query,after_ln,after_ln,i.paddings&#39;</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">attention_inputs</span> <span class="o">+=</span> <span class="s1">&#39;,i.segment_mask&#39;</span>

    <span class="n">sub_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;i.vec-&gt;after_ln&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_LN</span><span class="p">(</span><span class="s1">&#39;LN&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
                  <span class="n">use_fused_layernorm</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_fused_layernorm</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;after_ln-&gt;strided_query&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Stride</span><span class="p">(</span><span class="s1">&#39;query_after_stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">-&gt;after_att,prob&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attention_inputs</span><span class="p">),</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAtten</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;after_att-&gt;after_dropout&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">residual_dropout_prob</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">-&gt;strided_input&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_to_add</span><span class="p">),</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Stride</span><span class="p">(</span><span class="s1">&#39;before_add&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;strided_input,after_dropout-&gt;o.vec&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.paddings-&gt;o.paddings&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Stride</span><span class="p">(</span><span class="s1">&#39;padding_after_Stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">)),</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">sub_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;i.segment_mask-&gt;o.segment_mask&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="s1">&#39;segment_mask&#39;</span><span class="p">)))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>  <span class="c1"># input NestedMap with {vec, paddings, segment_mask}</span>
        <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>  <span class="c1"># output NestedMap with {vec, paddings, segment_mask}</span>
        <span class="o">*</span><span class="n">sub_list</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder.TransformerEncoderLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.TransformerEncoderLayer">[docs]</a>  <span class="k">def</span> <span class="nf">TransformerEncoderLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;(inputs, paddings) -&gt; (encoded, paddings).</span>

<span class="sd">    Args:</span>
<span class="sd">      name: the string name of the encoder layer params.</span>
<span class="sd">      stride: To use every k-th token, set the stride to k. When stride == 0,</span>
<span class="sd">        only returns the first token of the input. When stride == 1, returns</span>
<span class="sd">        every token in the input.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A transformer encoder layer params that supports optional stride.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Hack to be compatible with ckpt generated by self._rep</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
        <span class="s1">&#39;block&#39;</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_StridedAttention</span><span class="p">(</span><span class="s1">&#39;self_atten&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Feedforward</span><span class="p">(</span><span class="s1">&#39;ff&#39;</span><span class="p">)))</span></div>

<div class="viewcode-block" id="Builder.Stack"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.Stack">[docs]</a>  <span class="k">def</span> <span class="nf">Stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a stack of sequential layers.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MaybeSplit</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="n">blocks</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder.TransformerEncoderStack"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.TransformerEncoderStack">[docs]</a>  <span class="k">def</span> <span class="nf">TransformerEncoderStack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a stack of num_layers self-attention layers.&quot;&quot;&quot;</span>
    <span class="n">blocks</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;iter_</span><span class="si">{:0&gt;3d}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Stack</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span></div></div>
<span class="c1"># pyformat: enable</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2018

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>