

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lingvo.core.py_utils &mdash; Lingvo  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.py_utils</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.py_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python2, python3</span>
<span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Common utilities.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span> <span class="k">as</span> <span class="nn">py_collections</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">hashlib</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">zlib</span>

<span class="kn">import</span> <span class="nn">lingvo.compat</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">hyperparams</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">retry</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">symbolic</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="k">import</span> <span class="n">tshape</span>
<span class="kn">from</span> <span class="nn">lingvo.core.ops</span> <span class="k">import</span> <span class="n">py_x_ops</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="nb">zip</span>

<span class="kn">from</span> <span class="nn">tensorflow.contrib.model_pruning.python.layers</span> <span class="k">import</span> <span class="n">core_layers</span> <span class="k">as</span> <span class="n">pruning_layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">node_def_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.protobuf</span> <span class="k">import</span> <span class="n">rewriter_config_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">function</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.tpu</span> <span class="k">import</span> <span class="n">tpu_function</span>  <span class="c1"># pylint: disable=g-direct-tensorflow-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">deprecation</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="s1">&#39;If False, we disable all asserts.&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span><span class="s1">&#39;enable_check_numerics&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="s1">&#39;If False, we bypass calls to CheckNumerics.&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span><span class="s1">&#39;print_debug_tensors&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
                     <span class="s1">&#39;Whether to print debug tensors.&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span>
    <span class="s1">&#39;xla_device&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;If non-empty, can be cpu, gpu, or tpu (case sensitive)&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span><span class="s1">&#39;nas_run&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If True, this is a NAS training run.&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s1">&#39;use_resource_var&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s1">&#39;Use ResourceVariable instead of Variable; this option is &#39;</span>
    <span class="s1">&#39;ignored when xla_device=tpu, as TPU requires resource &#39;</span>
    <span class="s1">&#39;variables&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s1">&#39;tpu_compatible&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Create variables in a way compatible with TPU. &#39;</span>
    <span class="s1">&#39;This should be true for any job that will interact &#39;</span>
    <span class="s1">&#39;with variables or a checkpoint that will be produced &#39;</span>
    <span class="s1">&#39;or consumed by TPU&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s1">&#39;pin_vars_to_cpu&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s1">&#39;Pin variables to cpu:0.  This is useful for weight-sharing / multi-core &#39;</span>
    <span class="s1">&#39;inference on TPUs in which TPU core variables are managed via &#39;</span>
    <span class="s1">&#39;TPUPartitionedCallOp.&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s1">&#39;no_identity_on_vars&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s1">&#39;Do not add tf.identity() on vars. This allows TPUPartitionedCallOp to use&#39;</span>
    <span class="s1">&#39;variable handles directly for weight-sharing / multi-core &#39;</span>
    <span class="s1">&#39;inference on TPUs.&#39;</span><span class="p">)</span>

<span class="n">FLAGS</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">FLAGS</span>

<span class="n">ENQUEUE_OPS</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_enqueue_ops&#39;</span>
<span class="n">CLOSE_QUEUE_OPS</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_close_queue_ops&#39;</span>

<span class="n">TPU_EMBEDDING_LOAD_OPS</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_tpu_embedding_load_ops&#39;</span>
<span class="n">TPU_EMBEDDING_RETRIEVE_OPS</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_tpu_embedding_retrieve_ops&#39;</span>
<span class="n">TPU_EMBEDDING</span> <span class="o">=</span> <span class="s1">&#39;__tpu_embedding&#39;</span>
<span class="n">TPU_EMBEDDING_ACTIVATIONS</span> <span class="o">=</span> <span class="s1">&#39;__tpu_embedding_activations&#39;</span>

<span class="c1"># pylint: disable=protected-access</span>
<span class="n">deprecation</span><span class="o">.</span><span class="n">_PRINT_DEPRECATION_WARNINGS</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># pylint: enable=protected-access</span>


<span class="k">def</span> <span class="nf">Assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">assert_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">assert_greater_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">assert_greater</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_greater</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">assert_less_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_less_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">assert_less</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_less</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">assert_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>
      <span class="n">Assert</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">l</span><span class="p">)),</span> <span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span>
      <span class="n">Assert</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">r</span><span class="p">)),</span> <span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">assert_shape_match</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">:</span>
    <span class="n">filepath</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">extract_stack</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">3</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;msg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;LINGVO ASSERT </span><span class="si">%s</span><span class="s1">:</span><span class="si">%s</span><span class="s1">(</span><span class="si">%s</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span>
        <span class="sa">r</span><span class="s1">&#39;.*/&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">filepath</span><span class="p">),</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">py_x_ops</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">assert_same_dim0</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">py_x_ops</span><span class="o">.</span><span class="n">assert_same_dim0</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_CheckNumerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">:</span>
    <span class="k">if</span> <span class="s1">&#39;name&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;:\d+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_CheckNumerics&#39;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">check_numerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span> <span class="k">if</span> <span class="n">message</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">CheckNumerics</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Check numerics for tensors in inp.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_check_numerics</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">inp</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">_CheckNumerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inp</span><span class="p">]</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">_CheckNumerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inp</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_CheckNumerics</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">message</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">with_dependencies</span><span class="p">(</span><span class="n">dependencies</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">dependencies</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">)</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_PrintOptions</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="n">original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">get_printoptions</span><span class="p">()</span>
  <span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">yield</span>
  <span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="o">**</span><span class="n">original</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_Print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">_PrintOptions</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> = </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">Log</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Prints out values of tensors.</span>

<span class="sd">  Useful for debugging. E.g.,</span>
<span class="sd">    x = ... a tf.Tensor ...</span>
<span class="sd">    y = ... a tf.Tensor ...</span>
<span class="sd">    z = compute(x, y)</span>
<span class="sd">    z = Log(z, &#39;debug compute()&#39;, x=x, y=y)</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A Tensor. Log happens after this tensor&#39;s computed.</span>
<span class="sd">    prefix: Every tensor is logged with this prefix.</span>
<span class="sd">    **kwargs: keywords and tensors. Tensors are logged in the sort order of</span>
<span class="sd">      these keywards.</span>

<span class="sd">  Returns:</span>
<span class="sd">    value is returned.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Ensures tensors are printed in order.</span>
  <span class="n">last</span> <span class="o">=</span> <span class="n">value</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
      <span class="n">last</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_func</span><span class="p">(</span><span class="n">_Print</span><span class="p">,</span> <span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39; : &#39;</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="p">[])</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_Save</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
  <span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">.</span><span class="si">%08d</span><span class="s1">.</span><span class="si">%s</span><span class="s1">.npy&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">prefix</span><span class="o">.</span><span class="n">decode</span><span class="p">(),</span> <span class="n">steps</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">decode</span><span class="p">())</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">Open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">outfile</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">Save</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Saves values of tensors into files.</span>

<span class="sd">  Useful for debugging. E.g.,</span>
<span class="sd">    x = ... a tf.Tensor ...</span>
<span class="sd">    y = ... a tf.Tensor ...</span>
<span class="sd">    z = compute(x, y)</span>
<span class="sd">    z = Save(z, &#39;/path/tmp&#39;, x=x, y=y, z=z)</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A Tensor. Saving happens after this tensor is computed.</span>
<span class="sd">    filename_prefix: Every tensor is saved with this filename prefix.</span>
<span class="sd">    **kwargs: keywords and tensors. Tensors are logged in the sort order of</span>
<span class="sd">      these keywards.</span>

<span class="sd">  Returns:</span>
<span class="sd">    value is returned.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">last</span> <span class="o">=</span> <span class="n">value</span>
  <span class="n">steps</span> <span class="o">=</span> <span class="n">GetGlobalStep</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
      <span class="n">last</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_func</span><span class="p">(</span><span class="n">_Save</span><span class="p">,</span> <span class="p">[</span><span class="n">steps</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="p">[])</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">HasRank</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Syntactic sugar for asserting that tensor has the expected rank.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">expected_rank</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="n">expected_rank</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;Ranks did not match, got </span><span class="si">%d</span><span class="s1">, &#39;</span>
        <span class="s1">&#39;expected </span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">expected_rank</span><span class="p">)],</span>
                             <span class="n">tensor</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span>


<span class="k">def</span> <span class="nf">HasShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">expected_shape</span><span class="p">,</span> <span class="n">ndims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Syntactic sugar for asserting that tensor has the expected shape.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A Tensor.</span>
<span class="sd">    expected_shape: A Python list or a 1D tensor.</span>
<span class="sd">    ndims: If not None, check only the first `ndims` dimensions of `tensor`.</span>
<span class="sd">      Must be equal to the length of `expected_shape` if not None.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The input `tensor`</span>
<span class="sd">  Raises:</span>
<span class="sd">    A runtime error if the assertion fails.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">:</span>
    <span class="n">filepath</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">extract_stack</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">3</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;LINGVO ASSERT </span><span class="si">%s</span><span class="s1">:</span><span class="si">%s</span><span class="s1">(</span><span class="si">%s</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;.*/&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
                                                 <span class="n">filepath</span><span class="p">),</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_x_ops</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)[:</span><span class="n">ndims</span><span class="p">],</span> <span class="n">expected_shape</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">tensor</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span>


<span class="k">def</span> <span class="nf">GetShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ndims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns tensor&#39;s shape as a list which can be unpacked, unlike tf.shape.</span>

<span class="sd">  Tries to return static shape if it&#39;s available. Note that this means</span>
<span class="sd">  some of the outputs will be ints while the rest will be Tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: The input tensor.</span>
<span class="sd">    ndims: If not None, returns the shapes for the first `ndims` dimensions.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="n">dynamic_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

  <span class="c1"># Early exit for unranked tensor.</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">dynamic_shape</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">dynamic_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndims</span><span class="p">)]</span>

  <span class="c1"># Ranked tensor.</span>
  <span class="k">if</span> <span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">ndims</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ndims</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">ndims</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">)</span>

  <span class="c1"># Return mixture of static and dynamic dims.</span>
  <span class="n">static_shape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
  <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">static_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">if</span> <span class="n">static_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dynamic_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndims</span><span class="p">)</span>
  <span class="p">]</span>
  <span class="k">return</span> <span class="n">shapes</span>


<span class="k">def</span> <span class="nf">use_xla</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="n">res</span> <span class="o">=</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">xla_device</span>
  <span class="k">if</span> <span class="n">res</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">xla_device</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="s1">&#39;gpu&#39;</span><span class="p">,</span> <span class="s1">&#39;tpu&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span> <span class="nf">use_tpu</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="n">res</span> <span class="o">=</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">xla_device</span> <span class="o">==</span> <span class="s1">&#39;tpu&#39;</span>
  <span class="k">if</span> <span class="n">res</span><span class="p">:</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">enable_asserts</span>  <span class="c1"># asserts not supported on tpu</span>
  <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span> <span class="nf">nas_run</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">return</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">nas_run</span>


<span class="k">def</span> <span class="nf">tpu_compat</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">return</span> <span class="n">use_tpu</span><span class="p">()</span> <span class="ow">or</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">tpu_compatible</span>


<span class="k">def</span> <span class="nf">use_resource_variables</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">return</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">use_resource_var</span> <span class="ow">or</span> <span class="n">tpu_compat</span><span class="p">()</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">outside_all_rewrites</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">yield</span>


<span class="k">def</span> <span class="nf">RunOnTpuHost</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Runs the given function call on TPU host.</span>

<span class="sd">  Invokes func(\*args, \*\*kwargs) directly if not running on tpu.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: the function to invoke.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The function return value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">outside_compilation</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="n">_tpu_device_assignment</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">SetTpuDeviceAssignment</span><span class="p">(</span><span class="n">tpu_device_assignment</span><span class="p">):</span>
  <span class="k">global</span> <span class="n">_tpu_device_assignment</span>
  <span class="k">if</span> <span class="n">_tpu_device_assignment</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;tpu_device_assignment was already set, &#39;</span>
                       <span class="s1">&#39;overwriting with new assignment.&#39;</span><span class="p">)</span>
  <span class="n">_tpu_device_assignment</span> <span class="o">=</span> <span class="n">tpu_device_assignment</span>


<span class="c1"># This function should called in unittest only.</span>
<span class="k">def</span> <span class="nf">ClearTpuDevice</span><span class="p">():</span>
  <span class="k">global</span> <span class="n">_tpu_device_assignment</span>
  <span class="n">_tpu_device_assignment</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">GetTpuDeviceAssignment</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">_tpu_device_assignment</span>


<span class="k">def</span> <span class="nf">SessionConfig</span><span class="p">(</span><span class="n">soft_placement</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a session config proto.</span>

<span class="sd">  Args:</span>
<span class="sd">    soft_placement: Turns allow_soft_placement on iff True.</span>
<span class="sd">    inline: Turns do_function_inlining on iff True.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A TF session config proto.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">session_config</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span>
      <span class="n">allow_soft_placement</span><span class="o">=</span><span class="n">soft_placement</span><span class="p">,</span>
      <span class="n">graph_options</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphOptions</span><span class="p">(</span>
          <span class="n">optimizer_options</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">OptimizerOptions</span><span class="p">(</span>
              <span class="n">opt_level</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">OptimizerOptions</span><span class="o">.</span><span class="n">L1</span><span class="p">,</span> <span class="n">do_function_inlining</span><span class="o">=</span><span class="n">inline</span><span class="p">)))</span>
  <span class="c1"># Disable layout optimizer which increases GPU memory usage.</span>
  <span class="n">session_config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">rewrite_options</span><span class="o">.</span><span class="n">layout_optimizer</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">rewriter_config_pb2</span><span class="o">.</span><span class="n">RewriterConfig</span><span class="o">.</span><span class="n">OFF</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">session_config</span>


<span class="n">_NAME_PATTERN</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;[A-Za-z_][A-Za-z0-9_]*&#39;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NestedMap</span><span class="p">(</span><span class="nb">dict</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A simpler helper to maintain a dict.</span>

<span class="sd">  E.g.::</span>

<span class="sd">      &gt;&gt;&gt; foo = NestedMap()</span>
<span class="sd">      &gt;&gt;&gt; foo[&#39;x&#39;] = 10</span>
<span class="sd">      &gt;&gt;&gt; foo.y = 20</span>
<span class="sd">      &gt;&gt;&gt; assert foo.x * 2 == foo.y</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Disable pytype attribute checking.</span>
  <span class="n">_HAS_DYNAMIC_ATTRIBUTES</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">NestedMap</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="nf">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">NestedMap</span><span class="o">.</span><span class="n">CheckKey</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">NestedMap</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setitem__</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrapper to help user know what available attributes are.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">NestedMap</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">AttributeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">; available attributes: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                           <span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">keys</span><span class="p">())))</span>

  <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># Don&#39;t delegate w/ super: dict.copy() -&gt; dict.</span>
    <span class="k">return</span> <span class="n">NestedMap</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_memo</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">DeepCopy</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">DeepCopy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">flat_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span><span class="n">flat_v</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">CheckKey</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Asserts that key is valid NestedMap key.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span> <span class="n">_NAME_PATTERN</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="n">key</span>

  <span class="k">def</span> <span class="nf">Flatten</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Flatten the `.NestedMap` and returns values in a list.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">Expand</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="p">:</span>
          <span class="n">ret</span> <span class="o">+=</span> <span class="n">Expand</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">v</span><span class="p">]</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
      <span class="n">ret</span> <span class="o">+=</span> <span class="n">Expand</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">FlattenItems</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Flatten the `.NestedMap` and returns &lt;key, value&gt; pairs in a list.</span>

<span class="sd">    For lists, keys will be returned with `_&lt;idx&gt;` appended, e.g. `x.y_10.z`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of &lt;key, value&gt; pairs, where keys for nested entries will be</span>
<span class="sd">      represented in the form of `foo.bar`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">Expand</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
          <span class="n">global_key</span> <span class="o">=</span> <span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> <span class="n">k</span> <span class="k">if</span> <span class="n">key</span> <span class="k">else</span> <span class="n">k</span>
          <span class="n">ret</span> <span class="o">+=</span> <span class="n">Expand</span><span class="p">(</span><span class="n">global_key</span><span class="p">,</span> <span class="n">v</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">ret</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
          <span class="n">ret</span> <span class="o">+=</span> <span class="n">Expand</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[(</span><span class="n">key</span><span class="p">,</span> <span class="n">v</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">Expand</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy of this `.NestedMap` with fn applied on each value.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">DoTransform</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="p">:</span>
          <span class="n">ret</span> <span class="o">+=</span> <span class="p">[</span><span class="n">DoTransform</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">ret</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
      <span class="n">ret</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">DoTransform</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">Filter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy of this `.NestedMap` with entries that fn(entry) is True.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">FilterKeyVal</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">fn</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">FilterKeyVal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy of this `.NestedMap` with filtered by fn.</span>

<span class="sd">    If fn(key, entry) is True, the entry is copied into the returned NestedMap.</span>
<span class="sd">    Otherwise, it is not copied.</span>
<span class="sd">    For lists, keys will be processed with indices, e.g. `x.y[10].z`.</span>
<span class="sd">    This is different from FlattenItems.</span>

<span class="sd">    Args:</span>
<span class="sd">      fn: a callable of (string, entry)-&gt;boolean.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `.NestedMap` contains copied entries from this `&#39;.NestedMap`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">DoFilter</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Recursively copy value with the filter fn applied.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
          <span class="n">v</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
          <span class="k">if</span> <span class="n">prefix</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">.</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">k</span>
          <span class="n">filtered</span> <span class="o">=</span> <span class="n">DoFilter</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">filtered</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ret</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">filtered</span>
        <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
          <span class="n">filtered</span> <span class="o">=</span> <span class="n">DoFilter</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">filtered</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lst</span> <span class="o">+=</span> <span class="p">[</span><span class="n">filtered</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">lst</span> <span class="k">if</span> <span class="n">lst</span> <span class="k">else</span> <span class="kc">None</span>
      <span class="k">elif</span> <span class="n">fn</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">value</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">DoFilter</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Pack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lst</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy of this with each value replaced by a value in lst.&quot;&quot;&quot;</span>

    <span class="k">class</span> <span class="nc">DoPack</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lst</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">del</span> <span class="n">x</span>
        <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_iter</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">DoPack</span><span class="p">(</span><span class="n">lst</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">IsCompatible</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns true if self and other is compatible.</span>

<span class="sd">    Args:</span>
<span class="sd">      other: Another `.NestedMap`.  If x and y are two compatible `.NestedMap`,</span>
<span class="sd">        `x.Pack(y.Flatten())` produces y and `y.Pack(x.Flatten())` produces x.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">DoCompare</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Compares x and y.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
          <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">!=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
          <span class="k">return</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="n">DoCompare</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]):</span>
            <span class="k">return</span> <span class="kc">False</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
          <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="n">DoCompare</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
      <span class="k">return</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="n">DoCompare</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_ToStrings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns debug strings in a list for this `.NestedMap`.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">Print</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Recursively walk value.&quot;&quot;&quot;</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">prefix</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">.</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">k</span>
          <span class="n">ret</span> <span class="o">+=</span> <span class="n">Print</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
          <span class="n">ret</span> <span class="o">+=</span> <span class="n">Print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">+=</span> <span class="p">[(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">value</span><span class="p">)]</span>
      <span class="k">return</span> <span class="n">ret</span>

    <span class="n">kv</span> <span class="o">=</span> <span class="n">Print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="n">maxlen</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">kv</span><span class="p">])</span> <span class="k">if</span> <span class="n">kv</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">k</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">*</span> <span class="p">(</span><span class="mi">4</span> <span class="o">+</span> <span class="n">maxlen</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">k</span><span class="p">))</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kv</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">DebugString</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a debug string for this `.NestedMap`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ToStrings</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">VLog</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Logs the debug string at the level.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">level</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">level</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">prefix</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">prefix</span> <span class="o">=</span> <span class="s1">&#39;nmap: &#39;</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ToStrings</span><span class="p">():</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="n">level</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_Unique</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A helper to uniqify variables in a NestedMap.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_vset</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vset</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_vset</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
      <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">ToUniqueList</span><span class="p">(</span><span class="n">nmap</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the flattened `nmap` with duplicates removed.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">nmap</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">_Unique</span><span class="p">())</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">ReadOnlyAttrDictView</span><span class="p">(</span><span class="n">backing</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps a dict to provide a read-only view of its contents.</span>

<span class="sd">  Dict keys can also be accessed by attribute.</span>

<span class="sd">  Args:</span>
<span class="sd">    backing: Dict-like object to wrap.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Read-only Mapping that can be accessed by index ([&#39;foo&#39;]) or attr (d.foo).</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">class</span> <span class="nc">Wrapper</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrapper object.&quot;&quot;&quot;</span>

    <span class="c1"># Disable pytype attribute checking.</span>
    <span class="n">_HAS_DYNAMIC_ATTRIBUTES</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">backing</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">backing</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="n">backing</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">backing</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__hasattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">backing</span>

    <span class="k">def</span> <span class="nf">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;Dictionary is read-only.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;Dictionary is read-only.&#39;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">Wrapper</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">ToStaticShape</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
  <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span>
      <span class="nb">any</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">IsExpr</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">)):</span>
    <span class="k">return</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">EvalExpr</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">STATIC_VALUES</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">shape</span>


<span class="k">class</span> <span class="nc">RNNCellStateInit</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;State initialization functions for RNN cell init state.&quot;&quot;&quot;</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_Params</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;method&#39;</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span>
             <span class="s1">&#39;Initialization method. Should be one of zeros, random_normal.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;Random seed used to generate initial values.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Freeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">p</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Zeros</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;tf.zeros().&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">RNNCellStateInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">RandomNormal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;tf.random.normal().&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">RNNCellStateInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;random_normal&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">DefaultRNNCellStateInit</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">RNNCellStateInit</span><span class="o">.</span><span class="n">Zeros</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">InitRNNCellState</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_eval</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Initial state definitions for RNN cell implementations.</span>

<span class="sd">  Args:</span>
<span class="sd">    shape: A array of ints/symbols for specifying the shape of the state.</span>
<span class="sd">    init: Hyperparameters as returned by one of the static implemetaitons in</span>
<span class="sd">      RNNCellStateInit.</span>
<span class="sd">    dtype: The dype of the states. Defaults to tf.float32.</span>
<span class="sd">    name: An optional name for the operation.</span>
<span class="sd">    is_eval: Bool, set to True if we need special behavior in eval mode.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor of the specified shape, and sampled from the distribution as</span>
<span class="sd">    defined by the init parameters.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">ToStaticShape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">DefaultRNNCellStateInit</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>

  <span class="n">method</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">method</span>
  <span class="k">if</span> <span class="p">((</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;zeros&#39;</span><span class="p">])</span> <span class="ow">or</span> <span class="p">(</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;random_normal&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">is_eval</span><span class="p">)):</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;random_normal&#39;</span><span class="p">]:</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Initialization method (</span><span class="si">%s</span><span class="s1">) not supported.&#39;</span> <span class="o">%</span> <span class="n">method</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">init_state</span>


<span class="k">class</span> <span class="nc">WeightInit</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Static class providing weight initialization config params.&quot;&quot;&quot;</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_Params</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;method&#39;</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="s1">&#39;Initialization method.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="s1">&#39;Initialization scale.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;Random seed used to generate initial values.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Freeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">p</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Gaussian</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_normal(0, 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Uniform</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_uniform(-1.0, 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">UniformPositive</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_uniform(0., 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform_positive&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Xavier</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Xavier initialization (x = sqrt(6. / (in + out)); [-x, x]).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">XavierWithFixupParams</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">depth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">layers_per_residual_block</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Xavier initialization with Fixup.&quot;&quot;&quot;</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">layers_per_residual_block</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GeoMeanXavier</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A variant of Xavier (x = sqrt(3. / sqrt(in * out)); [-x, x]).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Constant</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussian</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.truncated_normal(0, 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GaussianSqrtDim</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_normal(0, 1 / sqrt(dim0)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GaussianSqrtFanIn</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_normal(0, 1 / sqrt(fan_in)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GaussianSqrtFanOut</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_normal(0, 1 / sqrt(fan_out)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">UniformSqrtDim</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.uniform(-1 / sqrt(dim0), 1 / sqrt(dim0)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">UniformUnitScaling</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * sqrt(3) / sqrt(dim0) * tf.uniform(-1, 1).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform_unit_scaling&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussianSqrtDim</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.truncated_normal(0, 1 / sqrt(dim0)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussianSqrtFanIn</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.truncated_normal(0, 1 / sqrt(fan_in)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussianSqrtFanOut</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.truncated_normal(0, 1 / sqrt(fan_out)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">KaimingUniformFanInRelu</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;kaiming_uniform_fanin_relu&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">KaimingUniformFanInLeakyRelu</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">5.</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>


<span class="n">_DEFAULT_XAVIER_INIT</span> <span class="o">=</span> <span class="mf">1.000001</span>


<span class="k">def</span> <span class="nf">DefaultParamInit</span><span class="p">():</span>
  <span class="c1"># Here we use 1.000001 as a signature for user picking up the</span>
  <span class="c1"># default param initializer.</span>
  <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">_DEFAULT_XAVIER_INIT</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">IsDefaultParamInit</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;xavier&#39;</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="n">_DEFAULT_XAVIER_INIT</span> <span class="ow">and</span>
          <span class="n">p</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">WeightParams</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a hyperparams for a weight variable given the shape/init/dtype.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">_DEFAULT_XAVIER_INIT</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
  <span class="k">if</span> <span class="n">collections</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">collections</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="s1">&#39;The weight data type.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="s1">&#39;The weight shape.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;init&#39;</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="s1">&#39;Initialization method.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span> <span class="n">collections</span><span class="p">,</span>
           <span class="s1">&#39;Variable collections this weight belongs to.&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">p</span>


<span class="k">def</span> <span class="nf">FindNeeded</span><span class="p">(</span><span class="n">endpoints</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;List names of tensors and operations required to compute endpoints.&quot;&quot;&quot;</span>
  <span class="n">names_seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="n">queue</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">endpoints</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Operation</span><span class="p">):</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
  <span class="k">while</span> <span class="n">queue</span><span class="p">:</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">name</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">names_seen</span><span class="p">:</span>
      <span class="n">names_seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="n">names_seen</span><span class="o">.</span><span class="n">update</span><span class="p">((</span><span class="n">o</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">))</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">names_seen</span>


<span class="k">def</span> <span class="nf">FindNeededInList</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">endpoints</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return tensors from tensor_list needed to compute any of endpoints.&quot;&quot;&quot;</span>
  <span class="n">all_needed</span> <span class="o">=</span> <span class="n">FindNeeded</span><span class="p">(</span><span class="n">endpoints</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor_list</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">all_needed</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">_CollectionGetter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get graph local value from a defined collection.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">default_factory</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_key</span> <span class="o">=</span> <span class="n">key</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_factory</span> <span class="o">=</span> <span class="n">default_factory</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_key</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">collection</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">collection</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">collection</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_factory</span><span class="p">()</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">value</span>


<span class="c1"># Global variable to control multitask variable reuse</span>
<span class="c1"># If False (default) the default tf.get_variable is used, that is:</span>
<span class="c1"># - Reusing scopes only allow getting existing variables</span>
<span class="c1"># - Non-reusing scopes only allow getting new variables</span>
<span class="c1"># With OPPORTUNISTIC_VARIABLE_REUSE==True:</span>
<span class="c1"># - Reusing scopes only allow getting existing variables, as usual</span>
<span class="c1"># - Non-reusing scopes reuse new variables or get new ones</span>
<span class="n">_OPPORTUNISTIC_VARIABLE_REUSE_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;__lingvo_opportunistic_variable_reuse&#39;</span><span class="p">,)</span>

<span class="n">_get_opportunistic_variable_reuse</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span>
    <span class="n">_OPPORTUNISTIC_VARIABLE_REUSE_KEY</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">[</span><span class="kc">False</span><span class="p">])</span>

<span class="n">_VARIABLE_RENAME_RULES_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;__lingvo_variable_rename_rules&#39;</span><span class="p">,)</span>

<span class="n">_get_rename_rules_stack</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span><span class="n">_VARIABLE_RENAME_RULES_KEY</span><span class="p">,</span>
                                            <span class="k">lambda</span><span class="p">:</span> <span class="p">[])</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">OpportunisticVariableReuseScope</span><span class="p">(</span><span class="n">enable_opportunistic_reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="n">opportunistic_var_reuse</span> <span class="o">=</span> <span class="n">_get_opportunistic_variable_reuse</span><span class="p">()</span>
  <span class="n">old_val</span> <span class="o">=</span> <span class="n">opportunistic_var_reuse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">opportunistic_var_reuse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">enable_opportunistic_reuse</span>
  <span class="k">yield</span>
  <span class="n">opportunistic_var_reuse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_val</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">VariableRenameScope</span><span class="p">(</span><span class="n">renames</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Append the renaming rules to the stack of renames.</span>

<span class="sd">  Args:</span>
<span class="sd">    renames: pairs of (regexp, new_name_format). If the regexp matches, the</span>
<span class="sd">      new_name_format will be interpolated using the matched groups.</span>

<span class="sd">  Yields:</span>
<span class="sd">    scope in which the renaming rules are applied</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">rename_rules_stack</span> <span class="o">=</span> <span class="n">_get_rename_rules_stack</span><span class="p">()</span>
  <span class="n">rename_rules_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">renames</span><span class="p">)</span>
  <span class="k">yield</span>
  <span class="n">rename_rules_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">GetVariableName</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get variable name after application of all renaming rules.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: untransformed variable name with scope_name prepended</span>

<span class="sd">  Returns:</span>
<span class="sd">    name possibly modified using renaming rules</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">matched</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">new_name</span> <span class="o">=</span> <span class="n">name</span>
  <span class="k">for</span> <span class="n">renames</span> <span class="ow">in</span> <span class="n">_get_rename_rules_stack</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">regexp</span><span class="p">,</span> <span class="n">name_format</span> <span class="ow">in</span> <span class="n">renames</span><span class="p">:</span>
      <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">regexp</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">matched</span><span class="p">:</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;Multiple matches for: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="n">matched</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">new_name</span> <span class="o">=</span> <span class="n">name_format</span> <span class="o">%</span> <span class="n">match</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">new_name</span> <span class="o">!=</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;WARNING!!! Renaming variable &#39;</span><span class="si">%s</span><span class="s2">&#39; to &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">new_name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">new_name</span>


<span class="k">def</span> <span class="nf">GenerateSeedFromName</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generate a random seed from a name string.&quot;&quot;&quot;</span>
  <span class="n">md5</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">()</span>
  <span class="n">md5</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span>
  <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">md5</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">(),</span> <span class="mi">16</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>


<span class="c1"># To keep track of all the variables ever gets created by the CreateVariable</span>
<span class="c1"># routine below.</span>
<span class="n">_ALL_VARS_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;__lingvo_all_vars&#39;</span><span class="p">,)</span>

<span class="n">_get_all_vars</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span><span class="n">_ALL_VARS_KEY</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">{})</span>


<span class="k">class</span> <span class="nc">_ThreadLocalStack</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_ThreadLocalStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>


<span class="n">_VARIABLE_SHAPE_PREFIXES</span> <span class="o">=</span> <span class="n">_ThreadLocalStack</span><span class="p">()</span><span class="o">.</span><span class="n">stack</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">VariableShapePrefixContext</span><span class="p">(</span><span class="n">shape_prefix</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add a shape prefix to variable created by CreateVariable().</span>

<span class="sd">  Args:</span>
<span class="sd">    shape_prefix: a positive integer of shape prefix.</span>

<span class="sd">  Yields:</span>
<span class="sd">    None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">shape_prefix</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">shape_prefix</span><span class="p">)</span>
  <span class="n">_VARIABLE_SHAPE_PREFIXES</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape_prefix</span><span class="p">)</span>
  <span class="k">yield</span>
  <span class="n">_VARIABLE_SHAPE_PREFIXES</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">GetVariableShapePrefixes</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Return the list of shape prefixes for CreateVariable().&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_VARIABLE_SHAPE_PREFIXES</span>


<span class="k">def</span> <span class="nf">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns (fan_in, fan_out) of a weight variable of the give shape.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># Following _compute_fans() from TF&#39;s init_ops.py.</span>
    <span class="k">return</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">receptive_field_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]:</span>
      <span class="n">receptive_field_size</span> <span class="o">*=</span> <span class="n">s</span>
    <span class="n">fan_in</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">receptive_field_size</span>
    <span class="n">fan_out</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">receptive_field_size</span>
    <span class="k">return</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span>


<span class="c1"># TODO(yonghui): Add support for partitioned Variables.</span>
<span class="k">def</span> <span class="nf">CreateVariable</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                   <span class="n">params</span><span class="p">,</span>
                   <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                   <span class="n">init_wrapper</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates tf.Variable according to param_config.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: A string, name of the variable.</span>
<span class="sd">    params: A WeightParams specifying the details of how this variable should be</span>
<span class="sd">      constructed and initialized.</span>
<span class="sd">    reuse: Whether or not to reuse an existing variable. It has the same</span>
<span class="sd">      semantics as the reuse arg in tf.variable_scope.</span>
<span class="sd">    trainable: Whether or not the variable is trainable.</span>
<span class="sd">    init_wrapper: a callback which takes a tf initializer callable and returns a</span>
<span class="sd">      tensor. It is used when shape of the variable isn&#39;t statically</span>
<span class="sd">      determinable.</span>
<span class="sd">    collections: Override the default variable collection (</span>
<span class="sd">      tf.GraphKeys.GLOBAL_VARIABLES).</span>

<span class="sd">  Returns:</span>
<span class="sd">    tf.identity(var), var pair. The tf.identity() node is colocated</span>
<span class="sd">    with var. In the case of FLAGS.no_identity_on_vars, simply returns</span>
<span class="sd">    a var, var pair.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">)</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">ToStaticShape</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">dim0</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">shape</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">dim_size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">]),</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">shape</span><span class="p">)</span>
    <span class="n">dim0</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span>
  <span class="n">method</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">method</span>
  <span class="n">scale</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span>
  <span class="n">seed</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span>

  <span class="k">if</span> <span class="n">IsDefaultParamInit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
        <span class="s1">&#39;WARNING!!! var </span><span class="si">%s</span><span class="s1"> is using the default xavier initializer.&#39;</span>
        <span class="s1">&#39; Make sure this is intended.&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># We are in a program/test which need determistic randomization.</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># We are not given a per-variable random seed. We use hash of</span>
      <span class="c1"># variable name as a stable random seed.</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
        <span class="n">var_name</span> <span class="o">=</span> <span class="n">GetVariableName</span><span class="p">(</span><span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">seed</span> <span class="o">=</span> <span class="n">GenerateSeedFromName</span><span class="p">(</span><span class="n">var_name</span><span class="p">)</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span>
  <span class="p">]):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
      <span class="c1"># This is probably not the right method to use when len(shape) &gt; 2,</span>
      <span class="c1"># e.g. dim0 will be 3 with a 3x3 conv2d kernel.</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
          <span class="s1">&#39;Initializing </span><span class="si">%s</span><span class="s1"> of shape </span><span class="si">%s</span><span class="s1"> with method </span><span class="si">%s</span><span class="s1">: dim0=</span><span class="si">%s</span><span class="s1">. &#39;</span>
          <span class="s1">&#39;Make sure that it is intended.&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">dim0</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dim0</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">]:</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span><span class="p">]:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_out</span><span class="p">)</span>

  <span class="n">init_dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span>
  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span>
      <span class="s1">&#39;gaussian_sqrt_fanout&#39;</span>
  <span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span>
        <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
        <span class="n">minval</span><span class="o">=-</span><span class="n">scale</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform_positive&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
        <span class="n">minval</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform_unit_scaling&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">uniform_unit_scaling_initializer</span><span class="p">(</span>
        <span class="n">factor</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;truncated_gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span><span class="p">,</span>
      <span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span>
  <span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span>
        <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;constant&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">]:</span>
    <span class="c1"># pylint: disable=unused-argument</span>
    <span class="k">def</span> <span class="nf">XavierUniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Xavier initialization (x = sqrt(6. / (in + out)); scale*[-x, x]).&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">shape</span><span class="se">\&#39;</span><span class="s1"> must not be </span><span class="se">\&#39;</span><span class="s1">None</span><span class="se">\&#39;</span><span class="s1"> or 0 for XavierUniform&#39;</span><span class="p">)</span>
      <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;xavier&#39;</span><span class="p">:</span>
        <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span> <span class="o">+</span> <span class="n">fan_out</span><span class="p">))</span>
      <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">:</span>
        <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span> <span class="o">*</span> <span class="n">fan_out</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

    <span class="c1"># pylint: enable=unused-argument</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">XavierUniform</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;kaiming_uniform_fanin_relu&#39;</span><span class="p">,</span> <span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span>
  <span class="p">]:</span>
    <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span><span class="p">:</span>
      <span class="c1"># Assume the &#39;a&#39; parameter is the &#39;scale&#39; argument.</span>
      <span class="n">gain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">gain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
    <span class="n">std_dev</span> <span class="o">=</span> <span class="n">gain</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">std_dev</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
        <span class="n">minval</span><span class="o">=-</span><span class="n">bound</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">bound</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;init_type not supported.&#39;</span>
  <span class="k">if</span> <span class="n">init_wrapper</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;Expecting </span><span class="se">\&#39;</span><span class="s1">params.shape</span><span class="se">\&#39;</span><span class="s1"> being None when &#39;</span>
        <span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">init_wrapper</span><span class="se">\&#39;</span><span class="s1"> is specified, instead getting </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># Later variable will init from Tensor value instead of intializer callable.</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_wrapper</span><span class="p">(</span><span class="n">init_dtype</span><span class="p">,</span> <span class="n">v_init</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">ComplexWrapper</span><span class="p">(</span><span class="n">init</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">_Wrapper</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
        <span class="c1"># A more complex alternative may be to use the init function for</span>
        <span class="c1"># magnitudes and uniform random for phases instead.</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">shape</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">init</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">init_dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

      <span class="k">return</span> <span class="n">_Wrapper</span>

    <span class="n">v_init</span> <span class="o">=</span> <span class="n">ComplexWrapper</span><span class="p">(</span><span class="n">v_init</span><span class="p">)</span>

  <span class="c1"># TODO(yonghui): Possibly get away from variable_scope and implement our own</span>
  <span class="c1"># variable sharing mechanism.</span>
  <span class="k">def</span> <span class="nf">GetVar</span><span class="p">(</span><span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;reuse: Whether to reuse the variables.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">var_shape</span> <span class="o">=</span> <span class="n">GetVariableShapePrefixes</span><span class="p">()</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">var_shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
      <span class="n">var_name</span> <span class="o">=</span> <span class="n">GetVariableName</span><span class="p">(</span><span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">var_scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">VariableScope</span><span class="p">(</span>
          <span class="n">scope</span><span class="o">.</span><span class="n">reuse</span><span class="p">,</span>
          <span class="n">custom_getter</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">custom_getter</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">use_resource</span> <span class="ow">or</span> <span class="n">use_resource_variables</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">var_scope</span><span class="p">),</span> \
        <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">pin_vars_to_cpu</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;/cpu:0&#39;</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
              <span class="s1">&#39;var&#39;</span><span class="p">,</span>
              <span class="n">var_shape</span><span class="p">,</span>
              <span class="n">dtype</span><span class="p">,</span>
              <span class="n">v_init</span><span class="p">,</span>
              <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
              <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
              <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">var_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
            <span class="s1">&#39;var&#39;</span><span class="p">,</span>
            <span class="n">var_shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="p">,</span>
            <span class="n">v_init</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
            <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">var_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">_get_opportunistic_variable_reuse</span><span class="p">()[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">var</span> <span class="o">=</span> <span class="n">GetVar</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>  <span class="c1"># Possibly the variable already exists</span>
      <span class="n">var</span> <span class="o">=</span> <span class="n">GetVar</span><span class="p">(</span><span class="n">reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">GetVar</span><span class="p">()</span>

  <span class="n">all_vars</span> <span class="o">=</span> <span class="n">_get_all_vars</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Reusing var </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">cached</span> <span class="o">=</span> <span class="n">all_vars</span><span class="p">[</span><span class="n">var</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">cached</span> <span class="o">==</span> <span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Cached config:</span><span class="se">\n</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> vs new config:</span><span class="se">\n</span><span class="s1"> </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="n">cached</span><span class="o">.</span><span class="n">ToText</span><span class="p">(),</span> <span class="n">p</span><span class="o">.</span><span class="n">ToText</span><span class="p">()))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Creating var </span><span class="si">%s</span><span class="s1"> shape=</span><span class="si">%s</span><span class="s1"> on device </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">var</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">all_vars</span><span class="p">[</span><span class="n">var</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">collections</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">no_identity_on_vars</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">var</span><span class="p">,</span> <span class="n">var</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># This tf.identity colocated with var.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">var</span><span class="p">),</span> <span class="n">var</span>


<span class="n">_global_variable_scope</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">GetGlobalVariableScope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Gets the global variable scope (as if no variable_scope has been set).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The VariableScope corresponding to as if no tf.variable_scope is in effect.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_global_variable_scope</span><span class="p">:</span>
    <span class="c1"># Each thread gets its own default global variable scope, and we take</span>
    <span class="c1"># advantage of that in order to get a top-level scope. This avoids the</span>
    <span class="c1"># need to call tf.get_variable_scope() at the module level, which allows</span>
    <span class="c1"># this module to be imported without modifying global state (i.e. creating</span>
    <span class="c1"># the default graph). It is important to not mutate the global state at</span>
    <span class="c1"># module load time, because it let&#39;s us flip flags after import that affect</span>
    <span class="c1"># core TensorFlow behavior.</span>
    <span class="k">def</span> <span class="nf">Initialize</span><span class="p">():</span>
      <span class="k">global</span> <span class="n">_global_variable_scope</span>
      <span class="n">_global_variable_scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>

    <span class="n">t</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">Initialize</span><span class="p">)</span>
    <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">_global_variable_scope</span>


<span class="n">_GLOBAL_STEP_STACK</span> <span class="o">=</span> <span class="p">[]</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">GlobalStepContext</span><span class="p">(</span><span class="n">global_step_tensor</span><span class="p">):</span>
  <span class="n">_GLOBAL_STEP_STACK</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">global_step_tensor</span><span class="p">)</span>
  <span class="k">yield</span>
  <span class="n">_GLOBAL_STEP_STACK</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">GetGlobalStep</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Return the global_step.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">_GLOBAL_STEP_STACK</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_GLOBAL_STEP_STACK</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_global_step</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">GetOrCreateGlobalStepVar</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Return the global_step variable, creating it if it does not exist.</span>

<span class="sd">  Prefer GetGlobalStep if a tensor rather than a tf.Variable is sufficient.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The global_step variable, or a new created one if it does not exist.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span>
      <span class="n">GetGlobalVariableScope</span><span class="p">(),</span> <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource_variables</span><span class="p">()):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_or_create_global_step</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">LogMultiLines</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">lines</span><span class="p">):</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_LogPlacement</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">copy</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Logs theta and its copy&#39;s device placement.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">GetDevices</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Flatten a `.NestedMap` m and extracts each value&#39;s device.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()]</span>

  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;=== </span><span class="si">%s</span><span class="s1"> ===&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
  <span class="n">LogMultiLines</span><span class="p">(</span>
      <span class="n">label</span><span class="p">,</span>
      <span class="n">theta</span><span class="o">.</span><span class="n">Pack</span><span class="p">([(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> -&gt; </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
                  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">GetDevices</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">GetDevices</span><span class="p">(</span><span class="n">copy</span><span class="p">))</span>
                 <span class="p">])</span><span class="o">.</span><span class="n">DebugString</span><span class="p">())</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;==========&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">CreateLocalTheta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">device_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates local copy of theta and shards across devices device list.</span>

<span class="sd">  Leaves variables intact.</span>

<span class="sd">  Args:</span>
<span class="sd">    theta: a `.NestedMap` of variables.</span>
<span class="sd">    device_list: list of devices to shard across. If None, defaults to a list</span>
<span class="sd">      [&#39;&#39;].</span>
<span class="sd">    label: Logging label.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` of identity() wrapped theta</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">class</span> <span class="nc">AddIdentity</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_list</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_list</span> <span class="o">=</span> <span class="n">device_list</span> <span class="k">if</span> <span class="n">device_list</span> <span class="k">else</span> <span class="p">[</span><span class="s1">&#39;&#39;</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_list</span><span class="p">)]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="n">copy</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">AddIdentity</span><span class="p">(</span><span class="n">device_list</span><span class="p">))</span>
  <span class="n">_LogPlacement</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">copy</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">copy</span>


<span class="k">def</span> <span class="nf">_GetVarsToLoad</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">variable_loading_rules</span><span class="p">,</span> <span class="n">var_ignore_rules</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Determines variables to load and their names in checkpoint.&quot;&quot;&quot;</span>
  <span class="c1"># This list contains mappings from var names as they appear in the checkpoint</span>
  <span class="c1"># to the vars in our model they correspond to.</span>
  <span class="n">vars_to_load</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">model_var</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
    <span class="n">already_matched</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">regexp</span><span class="p">,</span> <span class="n">name_format</span> <span class="ow">in</span> <span class="n">variable_loading_rules</span><span class="p">:</span>
      <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">regexp</span><span class="p">,</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="c1"># Skip if var doesn&#39;t match the loading rules, or if it should be ignored.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">match</span> <span class="ow">or</span> <span class="nb">any</span><span class="p">(</span>
          <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">var_ignore_rules</span><span class="p">):</span>
        <span class="k">continue</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">already_matched</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> is already matched!&#39;</span> <span class="o">%</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span>
      <span class="n">already_matched</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">checkpoint_var_name</span> <span class="o">=</span> <span class="n">name_format</span> <span class="o">%</span> <span class="n">match</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">checkpoint_var_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;:0&#39;</span><span class="p">):</span>
        <span class="n">checkpoint_var_name</span> <span class="o">=</span> <span class="n">checkpoint_var_name</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Loading </span><span class="si">%s</span><span class="s1"> from </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model_var</span><span class="p">,</span> <span class="n">checkpoint_var_name</span><span class="p">)</span>
      <span class="n">vars_to_load</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">checkpoint_var_name</span><span class="p">,</span> <span class="n">model_var</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">vars_to_load</span>


<span class="k">def</span> <span class="nf">_OverrideVarsFromCheckpoint</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span>
                                <span class="n">variable_loading_rules</span><span class="p">,</span> <span class="n">var_ignore_rules</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Overrides variables from a provided checkpoint.&quot;&quot;&quot;</span>
  <span class="n">vars_to_load</span> <span class="o">=</span> <span class="n">_GetVarsToLoad</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">variable_loading_rules</span><span class="p">,</span>
                                <span class="n">var_ignore_rules</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">vars_to_load</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span><span class="s1">&#39;Variable loading rules did not match any vars. &#39;</span>
                      <span class="s1">&#39;All known: </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">])</span>
  <span class="n">load_var_names</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">v</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vars_to_load</span><span class="p">])</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Overriding vars from checkpoint: </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">load_var_names</span><span class="p">)</span>

  <span class="k">while</span> <span class="n">vars_to_load</span><span class="p">:</span>
    <span class="c1"># When restoring, it&#39;s possible the same value in the checkpoint</span>
    <span class="c1"># can be restored to multiple variables (e.g. during</span>
    <span class="c1"># distillation).  However, tf.train.Saver, since it&#39;s used for</span>
    <span class="c1"># both saving and restoring, requires the name in the checkpoint</span>
    <span class="c1"># to be unique for each variable.  So, we call it multiple times</span>
    <span class="c1"># with a unique set of names each time.</span>
    <span class="n">unique_vars_to_load</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">remaining_vars_to_load</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vars_to_load</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">unique_vars_to_load</span><span class="p">:</span>
        <span class="n">unique_vars_to_load</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">remaining_vars_to_load</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">var_list</span><span class="o">=</span><span class="n">unique_vars_to_load</span><span class="p">)</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="n">vars_to_load</span> <span class="o">=</span> <span class="n">remaining_vars_to_load</span>


<span class="k">def</span> <span class="nf">OverrideVarsFromCheckpoints</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">ckpts_loading_rules</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Overrides model variables from checkpoints.</span>

<span class="sd">  Args:</span>
<span class="sd">    session: Tensorflow session.</span>
<span class="sd">    all_vars: List of all the parameters in the model.</span>
<span class="sd">    ckpts_loading_rules: A dictionary of checkpoint path: loading rules.</span>
<span class="sd">      Checkpoint path must be a path to a pretrained model, and loading rules is</span>
<span class="sd">      expected to be a tuple of two lists. The first consisting of tuples of</span>
<span class="sd">      strings defining (regex to match parameter names in the model to override,</span>
<span class="sd">      format string to determine the corresponding var in the checkpoint), and</span>
<span class="sd">      the second list consisting of a list of regexes to match parameter names</span>
<span class="sd">      in the model which should not be overridden, even if they match those in</span>
<span class="sd">      the loading rules.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if colliding vars exist or loading rules is not a list.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ckpts_loading_rules</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Overriding vars from multiple checkpoints.&#39;</span><span class="p">)</span>

  <span class="n">vars_overridden</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="n">loading_rules</span> <span class="ow">in</span> <span class="n">ckpts_loading_rules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Overriding vars from checkpoint: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loading_rules</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Loading rules for </span><span class="si">%s</span><span class="s1"> must be a tuple of two lists!&#39;</span> <span class="o">%</span>
                       <span class="n">ckpt_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loading_rules</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">loading_rules</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Loading rules for </span><span class="si">%s</span><span class="s1"> must be a tuple of two lists!&#39;</span> <span class="o">%</span>
                       <span class="n">ckpt_path</span><span class="p">)</span>

    <span class="c1"># Filter the model variables to be overridden.</span>
    <span class="n">vars_to_override</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">var</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">_GetVarsToLoad</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">loading_rules</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">loading_rules</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="p">]</span>

    <span class="n">overlap</span> <span class="o">=</span> <span class="nb">set</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">vars_overridden</span><span class="p">,</span> <span class="n">vars_to_override</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">overlap</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Colliding variables to override: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">overlap</span><span class="p">)</span>

    <span class="n">_OverrideVarsFromCheckpoint</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="n">loading_rules</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">loading_rules</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">vars_overridden</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">vars_to_override</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Model variables overridden: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">vars_overridden</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_ComputeGradientsSimple</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">grad_aggregation_method</span><span class="p">,</span>
                            <span class="n">colocate_gradients_with_ops</span><span class="p">,</span> <span class="n">gate_gradients</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span>
      <span class="n">loss</span><span class="p">,</span>
      <span class="n">all_vars</span><span class="p">,</span>
      <span class="n">aggregation_method</span><span class="o">=</span><span class="n">grad_aggregation_method</span><span class="p">,</span>
      <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
      <span class="n">gate_gradients</span><span class="o">=</span><span class="n">gate_gradients</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">ComputeTpuEmbeddingGradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">activation_dict</span><span class="p">,</span> <span class="n">tpu_embedding</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a TpuEmbedding SendGradient op.</span>

<span class="sd">  Args:</span>
<span class="sd">   loss: The loss to backprop from.</span>
<span class="sd">   activation_dict: String feature -&gt; embedding activations dict.</span>
<span class="sd">   tpu_embedding: TPUEmbedding instance.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Scale the loss to account for the full batch size.</span>
  <span class="n">shards</span> <span class="o">=</span> <span class="n">tpu_function</span><span class="o">.</span><span class="n">get_tpu_context</span><span class="p">()</span><span class="o">.</span><span class="n">number_of_shards</span>
  <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">shards</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

  <span class="n">grads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">activation_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
  <span class="n">feature_to_gradient_dict</span> <span class="o">=</span> <span class="n">py_collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">(</span>
      <span class="nb">zip</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">activation_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="n">grads</span><span class="p">))</span>
  <span class="n">send_gradient_op</span> <span class="o">=</span> <span class="n">tpu_embedding</span><span class="o">.</span><span class="n">generate_send_gradients_op</span><span class="p">(</span>
      <span class="n">feature_to_gradient_dict</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">send_gradient_op</span>


<span class="k">def</span> <span class="nf">_ComputeGradientsTpu</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">grad_aggregation_method</span><span class="p">,</span>
                         <span class="n">colocate_gradients_with_ops</span><span class="p">,</span> <span class="n">gate_gradients</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes gradients for local loss across whole TPU cluster.&quot;&quot;&quot;</span>
  <span class="c1"># Scale the loss to account for the full batch size.</span>
  <span class="n">shards</span> <span class="o">=</span> <span class="n">tpu_function</span><span class="o">.</span><span class="n">get_tpu_context</span><span class="p">()</span><span class="o">.</span><span class="n">number_of_shards</span>
  <span class="k">assert</span> <span class="n">shards</span>
  <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">shards</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

  <span class="c1"># Computes the gradients.</span>
  <span class="c1"># Sum the grads so that we can compute statistics across the whole batch.</span>
  <span class="n">all_grads</span> <span class="o">=</span> <span class="n">_ComputeGradientsSimple</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">grad_aggregation_method</span><span class="p">,</span>
                                      <span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
                                      <span class="n">gate_gradients</span><span class="p">)</span>

  <span class="c1"># NOTE: We can&#39;t use tpu_optimizer.CrossShardOptimizer since</span>
  <span class="c1"># we need to scale the grads *after* the cross_replica_sum to</span>
  <span class="c1"># match GPU version!</span>
  <span class="c1"># TODO(cwhipkey): should we do something different here? - we could do</span>
  <span class="c1"># some operations on the gradients before the aggregation (see comments in</span>
  <span class="c1"># tensorflow/contrib/tpu/python/tpu/tpu_optimizer.py - see compute_gradients -</span>
  <span class="c1"># for some more details).</span>
  <span class="n">aggregated_grads</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">all_grads</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">g</span><span class="p">):</span>
        <span class="n">aggregated_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">cross_replica_sum</span><span class="p">(</span><span class="n">g</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">aggregated_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">aggregated_grads</span>


<span class="k">def</span> <span class="nf">_ComputeGradientsTpuNas</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">grad_aggregation_method</span><span class="p">,</span>
                            <span class="n">colocate_gradients_with_ops</span><span class="p">,</span> <span class="n">gate_gradients</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes gradients for local loss across whole TPU cluster.</span>

<span class="sd">  This implementation specializes for the case where weight params maybe used</span>
<span class="sd">  for different number of times in the forward computation, so that gradients</span>
<span class="sd">  should be normalized by the actual number of times they are being computed.</span>

<span class="sd">  TODO(yonghui): Maybe merge this implementation with the _ComputeGradientsTpu</span>
<span class="sd">  one.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss: The loss to backprop from.</span>
<span class="sd">    all_vars: Vars with respect to which gradients are to be computed.</span>
<span class="sd">    grad_aggregation_method: aggregation method to use when calling</span>
<span class="sd">      tf.gradients.</span>
<span class="sd">    colocate_gradients_with_ops: boolean, whether or not to colocate gradient op</span>
<span class="sd">      with the original op.</span>
<span class="sd">    gate_gradients: boolean, flag to be passed to tf.gradients.</span>

<span class="sd">  Returns:</span>
<span class="sd">    gradients to be passed back.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Computes the gradients.</span>
  <span class="c1"># Sum the grads so that we can compute statistics across the whole batch.</span>
  <span class="n">all_grads</span> <span class="o">=</span> <span class="n">_ComputeGradientsSimple</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">grad_aggregation_method</span><span class="p">,</span>
                                      <span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
                                      <span class="n">gate_gradients</span><span class="p">)</span>

  <span class="c1"># NOTE: We can&#39;t use tpu_optimizer.CrossShardOptimizer since</span>
  <span class="c1"># we need to scale the grads *after* the cross_replica_sum to</span>
  <span class="c1"># match GPU version!</span>

  <span class="c1"># TODO(cwhipkey): should we do something different here? - we could do</span>
  <span class="c1"># some operations on the gradients before the aggregation (see comments in</span>
  <span class="c1"># tensorflow/contrib/tpu/python/tpu/tpu_optimizer.py - see compute_gradients -</span>
  <span class="c1"># for some more details).</span>

  <span class="n">aggregated_grads</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">all_grads</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">g</span><span class="p">):</span>
        <span class="c1"># Q(yonghui): Is there a better way to detect a non-zero gradient?</span>
        <span class="c1"># Note(yonghui): gradient of a weight param can be all zero if that</span>
        <span class="c1"># weight param is not used in the forward computation, e.g. as in</span>
        <span class="c1"># switchable layers in neural architecture search.</span>
        <span class="n">zero_threashold</span> <span class="o">=</span> <span class="mf">1e-8</span>
        <span class="n">g_is_non_zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">g</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">zero_threashold</span><span class="p">,</span> <span class="n">g</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">num_updates</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">cross_replica_sum</span><span class="p">(</span><span class="n">g_is_non_zero</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">normalized_g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">cross_replica_sum</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_updates</span>
        <span class="n">aggregated_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">normalized_g</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">aggregated_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">aggregated_grads</span>


<span class="k">def</span> <span class="nf">ComputeGradients</span><span class="p">(</span>
    <span class="n">loss</span><span class="p">,</span>
    <span class="n">vmap</span><span class="p">,</span>
    <span class="n">grad_aggregation_method</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">AggregationMethod</span><span class="o">.</span><span class="n">EXPERIMENTAL_TREE</span><span class="p">,</span>
    <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">gate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes gradients of variables in vmap w.r.t loss.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss: A scalar Tensor.</span>
<span class="sd">    vmap: A `.NestedMap` of variables.</span>
<span class="sd">    grad_aggregation_method: Specifies the method used to combine gradient</span>
<span class="sd">      terms. Accepted values are constants defined in the class</span>
<span class="sd">      AggregationMethod.</span>
<span class="sd">    colocate_gradients_with_ops: If True, try colocating gradients with the</span>
<span class="sd">      corresponding op.</span>
<span class="sd">    gate_gradients: If True, add a tuple around the gradients returned for an</span>
<span class="sd">      operations. This avoids some race conditions.</span>

<span class="sd">  Returns:</span>
<span class="sd">    var_grad - a `.NestedMap` of (variable, gradient). You can view</span>
<span class="sd">    var_grad as an ordered list of (key, (var, grad)) tuples. Every</span>
<span class="sd">    key of var_grad exists in vmap. Every variable in vmap that</span>
<span class="sd">    contributes to loss must exist in var_grad. Every var of var_grad</span>
<span class="sd">    must exist in vmap.  grad is the corresponding gradient computed</span>
<span class="sd">    for var. grad is guaranteed to be not None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vmap</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">)</span>

  <span class="c1"># Uniqify and remove None.</span>
  <span class="n">filtered_vmap</span> <span class="o">=</span> <span class="n">vmap</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">_Unique</span><span class="p">())</span>
  <span class="k">assert</span> <span class="n">filtered_vmap</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

  <span class="c1"># Filter out variables not contributing to &#39;loss&#39;.</span>
  <span class="n">trainable_variables</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">())</span>
  <span class="n">dependent_ops_and_tensors</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">FindNeeded</span><span class="p">([</span><span class="n">loss</span><span class="p">]))</span>

  <span class="k">def</span> <span class="nf">Needed</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">trainable_variables</span><span class="p">:</span>
        <span class="c1"># Skip non-trainable variables. Otherwise,</span>
        <span class="c1"># tf.Optimizer.apply_gradients throws up an exception instead</span>
        <span class="c1"># of skipping the update.</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="c1"># Not sure needed since tf.gradients will do this for us.</span>
    <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">dependent_ops_and_tensors</span>

  <span class="n">filtered_vmap</span> <span class="o">=</span> <span class="n">filtered_vmap</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">Needed</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">filtered_vmap</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="n">filtered_vlist</span> <span class="o">=</span> <span class="n">filtered_vmap</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

  <span class="c1"># tpu vs non-tpu is slightly different.</span>
  <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">nas_run</span><span class="p">():</span>
      <span class="n">take_grad</span> <span class="o">=</span> <span class="n">_ComputeGradientsTpuNas</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">take_grad</span> <span class="o">=</span> <span class="n">_ComputeGradientsTpu</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">take_grad</span> <span class="o">=</span> <span class="n">_ComputeGradientsSimple</span>

  <span class="n">grads</span> <span class="o">=</span> <span class="n">take_grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">filtered_vlist</span><span class="p">,</span> <span class="n">grad_aggregation_method</span><span class="p">,</span>
                    <span class="n">colocate_gradients_with_ops</span><span class="p">,</span> <span class="n">gate_gradients</span><span class="p">)</span>

  <span class="c1"># Formulate pairs of (var, grad) and pack them into the same</span>
  <span class="c1"># structure as filtered_vmap.</span>
  <span class="n">var_grad</span> <span class="o">=</span> <span class="n">filtered_vmap</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">filtered_vlist</span><span class="p">,</span> <span class="n">grads</span><span class="p">)))</span>

  <span class="c1"># Removes pairs whose grad is None.</span>
  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="n">var_grad</span><span class="o">.</span><span class="n">FlattenItems</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;ComputeGradients drops </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">var_grad</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v_g</span><span class="p">:</span> <span class="n">v_g</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">MaskGradients</span><span class="p">(</span><span class="n">var_grad</span><span class="p">,</span> <span class="n">grad_mask</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes gradients of non-masked variables in vmap w.r.t loss.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_grad: A `.NestedMap` of (variable, gradient)</span>
<span class="sd">    grad_mask: A dict of (variable name, mask).</span>

<span class="sd">  Returns:</span>
<span class="sd">    var_grad - a `.NestedMap` of (variable, mask * gradient).</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">ApplyMask</span><span class="p">(</span><span class="n">entry</span><span class="p">):</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">entry</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">grad_mask</span><span class="p">[</span><span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">var_grad</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">ApplyMask</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">ApplyGradMultiplier</span><span class="p">(</span><span class="n">vs_gs_scale</span><span class="p">,</span> <span class="n">grad_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Scale gradients by grad_scale on same device as corresponding variables.</span>

<span class="sd">  Args:</span>
<span class="sd">    vs_gs_scale: A `.NestedMap` of (variable, gradient, scale).</span>
<span class="sd">    grad_scale: If None, each vs_gs entry has the scale. Otherwise, grad_scale</span>
<span class="sd">      applies to every entry.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` of (variable, gradient * grad_scale). In particular, if</span>
<span class="sd">    grad_scale is 0, the result gradient is always 0, even if the input</span>
<span class="sd">    gradient is inf or nan.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">grad_scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">vs_gs_scale</span> <span class="o">=</span> <span class="n">vs_gs_scale</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">vg</span><span class="p">:</span> <span class="p">(</span><span class="n">vg</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vg</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">grad_scale</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">ScaleOrZero</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">CheckNumerics</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="s1">&#39;Gradient for </span><span class="si">%s</span><span class="s1"> is not finite.&#39;</span> <span class="o">%</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="mf">0.</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">grad</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Scale</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scales the gradient.&quot;&quot;&quot;</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">assert</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;No grad found for &#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
            <span class="n">ScaleOrZero</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">scale</span><span class="p">),</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">grad</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">ScaleOrZero</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">vs_gs_scale</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">Scale</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">HasNanOrInfGradient</span><span class="p">(</span><span class="n">var_grads</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a bool tensor to indicate if `var_grads` contains NaNs or Infs.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_grads: A `.NestedMap` with (var, grad) tuple as the map value.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A bool scalar tensor to indicate if the `var_grads` contains NaNs or Infs.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">HasNanOrInf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_any</span><span class="p">([</span><span class="n">HasNanOrInf</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">HasNanOrInf</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">x</span><span class="p">))])</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_any</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">is_nan</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">is_inf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_any</span><span class="p">([</span><span class="n">HasNanOrInf</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="n">var_grads</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()])</span>


<span class="k">def</span> <span class="nf">ApplyGradNormCliping</span><span class="p">(</span><span class="n">vs_gs</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Clip gradients to norm on same device as corresponding variables.</span>

<span class="sd">  Args:</span>
<span class="sd">    vs_gs: A `.NestedMap` of (variable, gradient).</span>
<span class="sd">    norm: Each tensor&#39;s gradient will be scaled down to have a maximum L2-norm</span>
<span class="sd">      value of `norm`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` of (variable, scaled_gradient). In particular, if</span>
<span class="sd">    grad_scale is 0, the result gradient is always 0, even if the input</span>
<span class="sd">    gradient is inf or nan.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">vs_gs_norm</span> <span class="o">=</span> <span class="n">vs_gs</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v_g</span><span class="p">:</span> <span class="p">(</span><span class="n">v_g</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v_g</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">norm</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">ClipByNorm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">norm</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">CheckNumerics</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="s1">&#39;Gradient for </span><span class="si">%s</span><span class="s1"> is not finite.&#39;</span> <span class="o">%</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Clip</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scales the gradient.&quot;&quot;&quot;</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">norm</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">assert</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;No grad found for &#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
            <span class="n">ClipByNorm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">norm</span><span class="p">),</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">ClipByNorm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">vs_gs_norm</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">Clip</span><span class="p">)</span>


<span class="n">SKIP_LP_REGULARIZATION</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_skip_lp_regularization&#39;</span>


<span class="k">def</span> <span class="nf">AdjustGradientsWithLpLoss</span><span class="p">(</span><span class="n">var_grads</span><span class="p">,</span> <span class="n">lp_regularizer_weight</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adjusts the map of (var, grad) with Lp regularization, where p=1.0 or 2.0.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_grads: a `.NestedMap` of (variable, gradient).</span>
<span class="sd">    lp_regularizer_weight: Lp regularization weight.</span>
<span class="sd">    p: For now we support 1.0 or 2.0.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple (lp_loss, var_grads).</span>

<span class="sd">    - lp_loss: A scalar. The lp loss.</span>
<span class="sd">    - var_grads: a `.NestedMap` of (variable, gradient) regulated by Lp.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(yuancao): For now we support p=1 or 2, but this can be extended to</span>
  <span class="c1"># lp-norm in general.</span>

  <span class="k">assert</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="s1">&#39;For now we only support L1/L2 regularization.&#39;</span>

  <span class="k">def</span> <span class="nf">GetVar</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">uniq_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span><span class="o">.</span><span class="n">y</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">uniq_ids</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">var</span>

  <span class="k">def</span> <span class="nf">Skip</span><span class="p">(</span><span class="n">v_g</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">v_g</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">SKIP_LP_REGULARIZATION</span><span class="p">)</span>

  <span class="n">filtered_var_grads</span> <span class="o">=</span> <span class="n">var_grads</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">Skip</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">filtered_var_grads</span><span class="o">.</span><span class="n">FlattenItems</span><span class="p">():</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;AdjustGradientsWithLpLoss: </span><span class="si">%s</span><span class="s1">: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">2.0</span><span class="p">:</span>
    <span class="n">lp_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">SumSquared</span><span class="p">(</span>
        <span class="n">filtered_var_grads</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">GetVar</span><span class="p">)</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
  <span class="k">elif</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
    <span class="n">lp_loss</span> <span class="o">=</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">SumAbs</span><span class="p">(</span>
        <span class="n">filtered_var_grads</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">GetVar</span><span class="p">)</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">LpGrad</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adjusts item&#39;s grad w/ Lp loss term.&quot;&quot;&quot;</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="c1"># Note: IndexedSlces appears for embedding lookups.</span>
      <span class="c1"># Embedding lookup ids can have duplicate. For duplicated ids, we</span>
      <span class="c1"># only want to consider once for each ids.</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">emb</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span>  <span class="c1"># [#ids, dims]</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="c1"># Counts is a vector of size vocab_size. counts[i] is i-th words</span>
        <span class="c1"># occurances in &#39;ids&#39;.</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ids</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

        <span class="c1"># Gradients for duplicated ids will be summed when they get</span>
        <span class="c1"># applied, and hence we account for that by first dividing</span>
        <span class="c1"># gradient resulting from lp loss by how many times the id is</span>
        <span class="c1"># duplicated.</span>
        <span class="c1">#</span>
        <span class="c1"># For each id in &#39;ids&#39;, we know counts[id] is non-zero,</span>
        <span class="c1"># hence, it&#39;s always safe to take reciprocal.</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">ids</span><span class="p">))</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [#ids, 1]</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">2.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">values</span>
        <span class="k">elif</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">weights</span> <span class="o">*</span> <span class="n">grad_v</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span> <span class="o">+</span> <span class="n">delta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">var</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">SKIP_LP_REGULARIZATION</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">2.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">var</span>
        <span class="k">elif</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">grad_v</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">+=</span> <span class="n">delta</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">lp_loss</span><span class="p">,</span> <span class="n">var_grads</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">LpGrad</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">SplitRecursively</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Splits Tensors in &#39;x&#39; recursively.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: a Tensor, or a list or NestMap containing Tensors to split.</span>
<span class="sd">    num_splits: number of splits per Tensor.</span>
<span class="sd">    axis: the split axis.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of split values of length &#39;num_splits&#39;.</span>

<span class="sd">    - If &#39;x&#39; is a Tensor, a list of split Tensors.</span>
<span class="sd">    - If &#39;x&#39; is a list, a list of lists, where each sublist has the same length</span>
<span class="sd">      as &#39;x&#39; and the k&#39;th element in each sublist corresponds to a split of the</span>
<span class="sd">      k&#39;th element from &#39;x&#39;.</span>
<span class="sd">    - If &#39;x&#39; is a `.NestedMap`, a list of `.NestedMap`, where each field</span>
<span class="sd">      corresponds to a split from the same field of &#39;x&#39;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">[</span><span class="n">SplitRecursively</span><span class="p">(</span><span class="n">element</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">splits</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">]</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">NestedMap</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_splits</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="n">val_splits</span> <span class="o">=</span> <span class="n">SplitRecursively</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_splits</span><span class="p">):</span>
        <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_splits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">results</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Unexpected type for SplitRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">ConcatRecursively</span><span class="p">(</span><span class="n">splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenates tensors from &#39;splits&#39;.</span>

<span class="sd">  This is the inverse function of SplitRecursively.</span>

<span class="sd">  Args:</span>
<span class="sd">    splits: a list of splits to concatenate, where elements can be Tensors,</span>
<span class="sd">      lists, or `.NestedMap`. The elements must share the same type and</span>
<span class="sd">      structure.  For example, list elements must have the same length;</span>
<span class="sd">      `.NestedMap` must have the same set of fields.</span>
<span class="sd">    axis: the concatenation axis.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Concatenated data.</span>

<span class="sd">    - If input &#39;splits&#39; are Tensors, returns a concatenated Tensor.</span>
<span class="sd">    - If input &#39;splits&#39; are lists, returns a list of the same length where the</span>
<span class="sd">      k&#39;th element represents concatenated data of the k&#39;th element from each</span>
<span class="sd">      split.</span>
<span class="sd">    - If input &#39;splits&#39; are `.NestedMap`, returns a `.NestedMap` with each field</span>
<span class="sd">      concatenated from corresponding fields of input splits.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if &#39;splits&#39; is not a list or elements of &#39;splits&#39; do not have</span>
<span class="sd">      known or matching types.</span>
<span class="sd">    ValueError: if &#39;splits&#39; is empty or elements of &#39;splits&#39; do not have</span>
<span class="sd">      matching structures.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">splits</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Non-list inputs for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">splits</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Empty inputs for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>

  <span class="n">tmpl</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Type mismatch for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmpl</span><span class="p">)</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Length mismatch for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="n">ConcatRecursively</span><span class="p">([</span><span class="n">split</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                           <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">],</span> <span class="n">axis</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tmpl</span><span class="p">))</span>
    <span class="p">]</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">)</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Type mismatch for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">tmpl</span><span class="p">:</span>
      <span class="n">results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ConcatRecursively</span><span class="p">([</span><span class="n">split</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">],</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Unexpected type for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">splits</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">AddToPruningCollections</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add mask, threshold, and weight vars to their respective collections.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">mask</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">pruning_layers</span><span class="o">.</span><span class="n">MASK_COLLECTION</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">pruning_layers</span><span class="o">.</span><span class="n">WEIGHT_COLLECTION</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">pruning_layers</span><span class="o">.</span><span class="n">MASK_COLLECTION</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">pruning_layers</span><span class="o">.</span><span class="n">THRESHOLD_COLLECTION</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">WeightedAvg</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">sum_reduction_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes weighted average of values from a tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: a tensor of values</span>
<span class="sd">    weights: a tensor of weights</span>
<span class="sd">    sum_reduction_fn: called to reduce the values and weights to single value</span>
<span class="sd">    name: name of metric.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple (avg, total_weight).</span>

<span class="sd">    - avg: weighted average value</span>
<span class="sd">    - total_weight: sum of all weights</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;shape of values and weights tensors must match for metric &#39;</span> <span class="o">+</span> <span class="n">name</span>
  <span class="n">values</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">(</span>
      <span class="p">[</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">message</span><span class="o">=</span><span class="n">msg</span><span class="p">)],</span> <span class="n">values</span><span class="p">)</span>
  <span class="n">total_weight</span> <span class="o">=</span> <span class="n">sum_reduction_fn</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
  <span class="n">avg</span> <span class="o">=</span> <span class="n">sum_reduction_fn</span><span class="p">(</span><span class="n">values</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">total_weight</span><span class="p">,</span> <span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">avg</span><span class="p">,</span> <span class="n">total_weight</span>


<span class="k">def</span> <span class="nf">WeightedAvgOfMetrics</span><span class="p">(</span><span class="n">metrics</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the weighted average of metrics in the list.</span>

<span class="sd">  Args:</span>
<span class="sd">    metrics: list of dictionaries of metrics</span>

<span class="sd">  Returns:</span>
<span class="sd">    ret_dict - dictionary of weighted averages of each metrics.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ret_dict</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">lists_of_metrics</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lists_of_metrics</span><span class="p">:</span>
        <span class="n">lists_of_metrics</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">lists_of_metrics</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="p">))</span>

  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">values_and_weights</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">lists_of_metrics</span><span class="p">)):</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values_and_weights</span><span class="p">])</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values_and_weights</span><span class="p">])</span>
    <span class="n">ret_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">WeightedAvg</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ret_dict</span>


<span class="k">def</span> <span class="nf">ConcatPerExampleTensors</span><span class="p">(</span><span class="n">per_example</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenate per-example tensors from many hosts into one large block.</span>

<span class="sd">  Args:</span>
<span class="sd">    per_example: list of dictionaries of per-example tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    ret_dict - string -&gt; concatenated tensors.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ret_dict</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">lists_of_per_example</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">per_example</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lists_of_per_example</span><span class="p">:</span>
        <span class="n">lists_of_per_example</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">lists_of_per_example</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">lists_of_per_example</span><span class="p">)):</span>
    <span class="n">ret_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ret_dict</span>


<span class="k">def</span> <span class="nf">CombineMetrics</span><span class="p">(</span><span class="n">loss_metric_weight_pairs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Combines metrics from `loss_metric_weight_pairs` according to weights.</span>

<span class="sd">  Keys must either exist in all metrics, in which it will be processed as a</span>
<span class="sd">  weighted sum, or exist in only one metrics, in which case it will be copied.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss_metric_weight_pairs: a list of (metrics, weight) pairs, where each</span>
<span class="sd">      weight is a float and each metrics is a dict with str keys and</span>
<span class="sd">      (metric_value, target_weight) values.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A dict with the same set of keys as input metrics and values of</span>
<span class="sd">    (weighted_sum(metric_value), weighted_sum(target_weight)).</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if there exists a metric that exists in more than one element</span>
<span class="sd">      of `loss_metric_weight_pairs` but not in all of them.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">all_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span>
      <span class="n">k</span> <span class="k">for</span> <span class="n">loss_metrics</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">loss_metric_weight_pairs</span>
      <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iterkeys</span><span class="p">(</span><span class="n">loss_metrics</span><span class="p">)</span>
  <span class="p">])</span>
  <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">all_keys</span><span class="p">:</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">loss_metrics</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">loss_metric_weight_pairs</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">loss_metrics</span><span class="p">:</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">count</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_metric_weight_pairs</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Found metric </span><span class="si">%s</span><span class="s1"> which exists in more than one&#39;</span>
                       <span class="s1">&#39;but not all loss metrics.&#39;</span> <span class="o">%</span> <span class="n">k</span><span class="p">)</span>

    <span class="n">total_val</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_target_weight</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">loss_metrics</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">loss_metric_weight_pairs</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">loss_metrics</span><span class="p">:</span>
        <span class="n">val</span><span class="p">,</span> <span class="n">target_weight</span> <span class="o">=</span> <span class="n">loss_metrics</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
          <span class="c1"># Single metric, don&#39;t multiply by weight.</span>
          <span class="n">total_val</span> <span class="o">=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">target_weight</span>
          <span class="n">total_target_weight</span> <span class="o">=</span> <span class="n">target_weight</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Total weighted sum of all predictions.</span>
          <span class="n">total_val</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">val</span> <span class="o">*</span> <span class="n">target_weight</span>
          <span class="n">total_target_weight</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">target_weight</span>

    <span class="n">result</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_val</span> <span class="o">/</span> <span class="n">total_target_weight</span><span class="p">,</span> <span class="n">total_target_weight</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">_AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="n">seed</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">seed</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">and</span> <span class="n">step</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">+=</span> <span class="n">step</span> <span class="o">*</span> <span class="mi">203984</span>
  <span class="n">noises</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">noises</span>


<span class="k">def</span> <span class="nf">AddGlobalVN</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds variational noise to weights if specified by params.&quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">params</span>
  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span><span class="p">:</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">_AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">weights</span>


<span class="k">def</span> <span class="nf">AddPerStepVN</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds per-setp variational noise to weights if specified by params.&quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">params</span>
  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span><span class="p">:</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">_AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">weights</span>


<span class="k">def</span> <span class="nf">VariationalNoiseParams</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">global_vn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">per_step_vn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a hyperparams for variational noise.&quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
      <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span>
      <span class="s1">&#39;Std of the variational noise to apply . This can be a scalar,&#39;</span>
      <span class="s1">&#39; or a scalar tensor.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;global_vn&#39;</span><span class="p">,</span> <span class="n">global_vn</span><span class="p">,</span>
           <span class="s1">&#39;Adds global variational noise every training setp iff True.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;per_step_vn&#39;</span><span class="p">,</span> <span class="n">per_step_vn</span><span class="p">,</span>
           <span class="s1">&#39;Adds per-timesetp variational noise iff True.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;Random seed used to generate noise.&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">p</span>


<span class="k">def</span> <span class="nf">GetStepSeed</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Gets step_seed.&quot;&quot;&quot;</span>
  <span class="n">step_seed_tensors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="s1">&#39;step_seed&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">step_seed_tensors</span><span class="p">:</span>
    <span class="n">ResetStepSeed</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">GetStepSeed</span><span class="p">()</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">step_seed_tensors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">step_seed_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Multiple tensors in step_seed collection.&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">ResetStepSeed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Resets step_seed to specified value.&quot;&quot;&quot;</span>
  <span class="n">new_step_seed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
  <span class="n">step_seed_tensors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="s1">&#39;step_seed&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">step_seed_tensors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">step_seed_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_step_seed</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">step_seed_tensors</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s1">&#39;step_seed&#39;</span><span class="p">,</span> <span class="n">new_step_seed</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Multiple tensors in step_seed collection.&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">GetIncStepSeed</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns and increments the step_seed.&quot;&quot;&quot;</span>
  <span class="n">step_seed</span> <span class="o">=</span> <span class="n">GetStepSeed</span><span class="p">()</span>
  <span class="c1"># TODO(lepikhin): introduce a routine filling a queue of uint32 random seeds</span>
  <span class="c1"># independent of underlying PRNG used by tensorflow.</span>
  <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">step_seed</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">step_seed</span>


<span class="k">def</span> <span class="nf">GenerateStepSeedPair</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span> <span class="n">op_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generates a seed pair for deterministic random operations in functional loops.</span>

<span class="sd">  This function retrieves a unique seed pair on each call, based off the current</span>
<span class="sd">  global step and step seed. The step seed ensures this function returns a</span>
<span class="sd">  unique seed pair on each call: calling this function automatically increments</span>
<span class="sd">  the step seed. The step seed is automatically reset at the beginning of each</span>
<span class="sd">  global step in the model&#39;s FProp and works transparently through recurrent.py.</span>

<span class="sd">  Args:</span>
<span class="sd">    p: A hyperparams.Params object, containing keys &#39;random_seed&#39; and</span>
<span class="sd">      &#39;is_inference&#39;.</span>
<span class="sd">    global_step: The global step.</span>
<span class="sd">    op_seed: An additional operation-level seed to apply.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A size 2 tensor of op seeds to use for stateless_random ops.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">seed_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span> <span class="k">if</span> <span class="n">use_tpu</span><span class="p">()</span> <span class="k">else</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span>
  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Ensure GetIncStepSeed is called even inside the shortcut.</span>
    <span class="c1"># This ensures if p.random_seed is set for other ops that use this function</span>
    <span class="c1"># that they will get the same seed pair whether or not p.random_seed is set</span>
    <span class="c1"># for this specific call.</span>
    <span class="n">GetIncStepSeed</span><span class="p">()</span>
    <span class="c1"># Unlike tf.random*, stateless random ops are completely determined by the</span>
    <span class="c1"># passed-in seeds. This means at inference time the same inputs will produce</span>
    <span class="c1"># the same outputs, even if the model is supposed to have randomness such as</span>
    <span class="c1"># dropout during inference. We inject additional randomness only during</span>
    <span class="c1"># inference if the graph is exported with random_seed=None as a workaround.</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="mi">2</span><span class="p">],</span> <span class="n">maxval</span><span class="o">=</span><span class="n">seed_dtype</span><span class="o">.</span><span class="n">max</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">seed_dtype</span><span class="p">)</span>

  <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">global_step</span><span class="p">,</span> <span class="n">seed_dtype</span><span class="p">)</span>
  <span class="n">step_seed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">GetIncStepSeed</span><span class="p">(),</span> <span class="n">seed_dtype</span><span class="p">)</span>
  <span class="n">seeds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">global_step</span><span class="p">,</span> <span class="n">step_seed</span><span class="p">])</span>

  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seeds</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span>
  <span class="k">if</span> <span class="n">op_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seeds</span> <span class="o">+=</span> <span class="n">op_seed</span>
  <span class="k">return</span> <span class="n">seeds</span>


<span class="k">def</span> <span class="nf">DeterministicDropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="n">seeds</span><span class="p">,</span> <span class="n">noise_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Similar to `tf.nn.dropout()`, but fully deterministic.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A float Tensor on which to apply dropout.</span>
<span class="sd">    keep_prob: A scalar `Tensor` of keep probability.</span>
<span class="sd">    seeds: A Tensor of shape [2]. 2 seeds for deterministic random number</span>
<span class="sd">      generator.</span>
<span class="sd">    noise_shape: A 1-D `Tensor` of type `int32`, representing the shape for</span>
<span class="sd">      randomly generated keep/drop flags.</span>
<span class="sd">    name: An optional name for this operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the same shape as `x`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    InvalidArgumentError: if keep_prob is invalid.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Real</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">keep_prob</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">keep_prob</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
          <span class="s1">&#39;keep_prob must be in range (0, 1]. Value: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">keep_prob</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">seeds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">seeds</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
        <span class="n">keep_prob</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">)</span>
    <span class="c1"># uniform in [keep_prob, 1.0 + keep_prob)</span>
    <span class="c1"># StatelessRandomUniform op does not support non-float (e.g. bfloat16) dtype</span>
    <span class="c1"># and non-int32 seed types.</span>
    <span class="n">noise_shape</span> <span class="o">=</span> <span class="n">noise_shape</span> <span class="ow">or</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">keep_prob</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">stateless_uniform</span><span class="p">(</span>
        <span class="n">noise_shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)</span>
    <span class="n">binary_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">random_tensor</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">binary_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">binary_tensor</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span> <span class="o">*</span> <span class="n">binary_tensor</span>
    <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="n">BATCH_NORM_UPDATES</span> <span class="o">=</span> <span class="s1">&#39;batch_norm_updates&#39;</span>

<span class="n">_BATCH_NORM_UPDATES_DICT</span> <span class="o">=</span> <span class="s1">&#39;__batch_norm_update_dict&#39;</span>
<span class="n">_get_batch_norm_updates_dict</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span><span class="n">_BATCH_NORM_UPDATES_DICT</span><span class="p">,</span>
                                                 <span class="k">lambda</span><span class="p">:</span> <span class="p">{})</span>


<span class="k">def</span> <span class="nf">UpdateBatchNormVars</span><span class="p">(</span><span class="n">batch_norm_var</span><span class="p">,</span> <span class="n">batch_norm_stats</span><span class="p">,</span> <span class="n">decay</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Update batch normalization moving averages.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span>
      <span class="s1">&#39;AssignMovingAvg&#39;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span>
          <span class="n">batch_norm_var</span><span class="p">,</span>
          <span class="n">batch_norm_stats</span><span class="p">,</span>
          <span class="n">decay</span><span class="p">,</span>
      <span class="p">])</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">batch_norm_var</span><span class="p">):</span>
      <span class="n">decay</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="mf">1.0</span> <span class="o">-</span> <span class="n">decay</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">batch_norm_var</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
      <span class="n">update_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_norm_var</span> <span class="o">-</span> <span class="n">batch_norm_stats</span><span class="p">)</span> <span class="o">*</span> <span class="n">decay</span>
      <span class="n">bn_update</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">batch_norm_var</span><span class="p">,</span> <span class="n">update_delta</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">BATCH_NORM_UPDATES</span><span class="p">,</span> <span class="n">bn_update</span><span class="p">)</span>
  <span class="n">bn_update_dict</span> <span class="o">=</span> <span class="n">_get_batch_norm_updates_dict</span><span class="p">()</span>
  <span class="k">assert</span> <span class="n">bn_update</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">bn_update_dict</span>
  <span class="n">bn_update_dict</span><span class="p">[</span><span class="n">bn_update</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_norm_var</span><span class="p">,</span> <span class="n">batch_norm_stats</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">bn_update</span>


<span class="k">def</span> <span class="nf">FindRelevantBatchNormUpdates</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">batch_norm_updates</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Finds and returns a list of relevant batch-normalization updates.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss: The loss that is being optimized for. A tensor or a list of tensors.</span>
<span class="sd">    batch_norm_updates: A list of batch normalization updates.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A pair of lists. The first list contains all the batch normalization updates</span>
<span class="sd">    that are relevant to the loss being optimized, and the second list contains</span>
<span class="sd">    all in batch_norm_updates but not in the first list.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">dependent_ops_and_tensors</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">FindNeeded</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
  <span class="n">relevant_updates</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">irrelevant_updates</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="n">bn_update_dict</span> <span class="o">=</span> <span class="n">_get_batch_norm_updates_dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">bn_update</span> <span class="ow">in</span> <span class="n">batch_norm_updates</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">bn_update</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">bn_update_dict</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> is probably not a valid batch normalization update op.&#39;</span>
        <span class="s1">&#39; Make sure batch normalization is done through calling&#39;</span>
        <span class="s1">&#39; the py_utils.UpdateBatchNormVars helper routine.&#39;</span><span class="p">)</span>
    <span class="n">bn_stat_name</span> <span class="o">=</span> <span class="n">bn_update_dict</span><span class="p">[</span><span class="n">bn_update</span><span class="o">.</span><span class="n">name</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
    <span class="k">if</span> <span class="n">bn_stat_name</span> <span class="ow">in</span> <span class="n">dependent_ops_and_tensors</span><span class="p">:</span>
      <span class="c1"># If a batch normalization stat is computed in the forward pass in</span>
      <span class="c1"># computing loss, then the corresponding batch normalization update is</span>
      <span class="c1"># relevant. Otherwise, it is not.</span>
      <span class="n">relevant_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bn_update</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">irrelevant_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bn_update</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">relevant_updates</span><span class="p">,</span> <span class="n">irrelevant_updates</span>


<span class="n">_MODEL_SPLIT_ID_STACK</span> <span class="o">=</span> <span class="s1">&#39;__model_split_id_stack&#39;</span>
<span class="n">_get_model_split_id_stack</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span><span class="n">_MODEL_SPLIT_ID_STACK</span><span class="p">,</span>
                                              <span class="k">lambda</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">GetModelSplit</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">_get_model_split_id_stack</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">ModelSplit</span><span class="p">(</span><span class="n">split_id</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">split_id</span> <span class="o">&gt;=</span> <span class="mi">0</span>
  <span class="n">_get_model_split_id_stack</span><span class="p">()</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_id</span><span class="p">)</span>
  <span class="k">yield</span>
  <span class="n">_get_model_split_id_stack</span><span class="p">()</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="n">_SAMPLE_STEP_KEY</span> <span class="o">=</span> <span class="s1">&#39;sample_step&#39;</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">SampleStep</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A context for a sample step during decoding.</span>

<span class="sd">  Example usage::</span>

<span class="sd">      with py_utils.SampleStep(step):</span>
<span class="sd">        sample = self.DecodeOneStep()</span>

<span class="sd">  Args:</span>
<span class="sd">    step: the step tensor.</span>

<span class="sd">  Yields:</span>
<span class="sd">    a context manager for the step scope.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">stack</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="n">_SAMPLE_STEP_KEY</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">step</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_GetSampleStep</span><span class="p">():</span>
  <span class="n">stack</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">_SAMPLE_STEP_KEY</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">stack</span> <span class="k">else</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">AddDebugTensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds `tensor` to the debug collection.</span>

<span class="sd">  Prints the tensor if `--print_debug_tensors` is True.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A tensor.</span>
<span class="sd">    summarize: Only print this many entries of each tensor. If None, then a</span>
<span class="sd">      maximum of 3 elements are printed per input tensor.</span>
<span class="sd">    name: An optional name for the tensor.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor that evaluates to the same value as the input tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">print_debug_tensors</span><span class="p">:</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">_GetSampleStep</span><span class="p">()</span>
    <span class="n">tensors_to_print</span> <span class="o">=</span> <span class="p">([]</span> <span class="k">if</span> <span class="n">step</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[</span><span class="n">step</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">s</span><span class="p">:</span>
      <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Print</span><span class="p">(</span>
          <span class="n">tensor</span><span class="p">,</span>
          <span class="n">tensors_to_print</span><span class="p">,</span>
          <span class="n">message</span><span class="o">=</span><span class="s1">&#39;DEBUG tensor </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">s</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
          <span class="n">summarize</span><span class="o">=</span><span class="n">summarize</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tensor</span>


<span class="k">def</span> <span class="nf">ArgMax</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;tf.argmax wrapper.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: A tensor, whose last dimension is being reduced on.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor of rank tf.rank(logits)-1. If i == ret[indices],</span>
<span class="sd">    logits[indices, i] is the maximum among logits[indices, :].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_EnsureMatrixShape</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">x</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">2</span>
  <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">Matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;tf.matmul wrapper expecting x and y are actually matrices.&quot;&quot;&quot;</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">_EnsureMatrixShape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">_EnsureMatrixShape</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_real&#39;</span> <span class="o">%</span> <span class="n">name</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_imag&#39;</span> <span class="o">%</span> <span class="n">name</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_TransformAndSum</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">transform</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;TransformAndSum&#39;</span><span class="p">):</span>
    <span class="n">sum_transform</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor_list</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
          <span class="n">sum_transform</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">transform</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">values</span><span class="p">))]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">sum_transform</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">transform</span><span class="p">(</span><span class="n">t</span><span class="p">))]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">sum_transform</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">SumSquared</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_TransformAndSum</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">SumAbs</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_TransformAndSum</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">PiecewiseConstant</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">boundaries</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">vdtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the piecewise value of x_in.&quot;&quot;&quot;</span>
  <span class="n">x_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x_in</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">assert</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span>
  <span class="n">bs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">boundaries</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">vs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">vdtype</span><span class="p">)</span>
  <span class="c1"># The following is equivalent to &#39;return vs[index]&#39;.</span>
  <span class="n">index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">bs</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
  <span class="n">one_hot_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">depth</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">vdtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">Matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vs</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">one_hot_vec</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">PadSequenceDimension</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">pad_val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pads x to `length` using `pad_val` along the second dim.</span>

<span class="sd">  Assumes `x` is a tensor with rank &gt;= 2, and it only pads `x` to `length`</span>
<span class="sd">  along the second dim. Explicitly sets the returned tensor shape to `shape` if</span>
<span class="sd">  given. Raises runtime errors if x.shape[1] &gt; length or x.shape[i] != shape[i]</span>
<span class="sd">  where i != 1.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: the tensor to be padded with shape [batch, seq_len, ...].</span>
<span class="sd">    length: an int to specify the length to pad x to.</span>
<span class="sd">    pad_val: an int or float used to pad x.</span>
<span class="sd">    shape: an int array specifying the shape of the padded tensor if specified.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The padded tensor with shape [batch, seq_len, ...], where</span>
<span class="sd">    ret[:, :seq_len, ...] == x.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
    <span class="k">assert</span> <span class="n">rank</span> <span class="o">&gt;=</span> <span class="mi">2</span>
    <span class="n">slen</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rank</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">pad_len</span> <span class="o">=</span> <span class="n">length</span> <span class="o">-</span> <span class="n">slen</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rank</span><span class="p">)]</span>
    <span class="n">pad</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">pad_len</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]):</span>
      <span class="n">slen</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">pad_len</span> <span class="o">=</span> <span class="n">length</span> <span class="o">-</span> <span class="n">slen</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scatter_nd</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">pad_len</span><span class="p">],</span> <span class="p">[</span><span class="n">rank</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="n">pad_val</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">static_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">static_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">length</span>
    <span class="n">x</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">static_shape</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">shape</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Shape must be a list or tuple.&#39;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ensure_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">ApplyPadding</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">padded</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">broadcast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_select</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Applies padding to a tensor.</span>

<span class="sd">  This is preferable to using arithmetic means for masking out padded values</span>
<span class="sd">  such as::</span>

<span class="sd">      # Equiv to ApplyPadding(padding, x))</span>
<span class="sd">      x *= 1.0 - padding</span>
<span class="sd">      # Equiv to ApplyPadding(padding, new, old)</span>
<span class="sd">      new = old * padding + new * (1 - padding)</span>

<span class="sd">  Aside from just being easier to read and reason about, using this function</span>
<span class="sd">  is friendly to quantized representations because it does not mix arithmetic</span>
<span class="sd">  on the padding values with the values in the tensor being padded (which can</span>
<span class="sd">  have a very different range than the 0..1 padding tensor).</span>

<span class="sd">  In addition, this works around issues in quantized schemes where we are</span>
<span class="sd">  guaranteed to have an exact 0 but not necessarily any other number (i.e. 1).</span>

<span class="sd">  Args:</span>
<span class="sd">    padding: Tensor of padding values where 0 == keep and 1 == pad.</span>
<span class="sd">    x: Tensor to apply padding to.</span>
<span class="sd">    padded: Optional. Values to include for padded elements. Defaults to zeros.</span>
<span class="sd">      Must be the same shape as &#39;x&#39; if specified.</span>
<span class="sd">    broadcast: Whether to broadcast the padding shape to the shape of &#39;x&#39;. You</span>
<span class="sd">      almost certainly want this to be true as it matches how padding would be</span>
<span class="sd">      expanded if applied arithmetically.</span>
<span class="sd">    use_select: Controls whether padding is applied with a select-mask</span>
<span class="sd">      (True/default) or arithmetically (False). Some platforms have a</span>
<span class="sd">      sensitivity to one or the other and this is used to work around such</span>
<span class="sd">      issues.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor with the same shape as x with padded values masked.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">padding</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">Assert</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))),</span>
          <span class="p">[</span><span class="n">padding</span><span class="p">])</span>
  <span class="p">],</span> <span class="n">padding</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">use_select</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">padded</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">padded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">broadcast</span><span class="p">:</span>
      <span class="n">padding</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Broadcast padding to the full shape.</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">padding</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">padded</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">padded</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">padding</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">padding</span><span class="p">)</span> <span class="o">+</span> <span class="n">padded</span> <span class="o">*</span> <span class="n">padding</span>


<span class="k">def</span> <span class="nf">LengthsFromPaddings</span><span class="p">(</span><span class="n">paddings</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes lengths of each sequence in a batch, ignoring trailing padding.</span>

<span class="sd">  Args:</span>
<span class="sd">    paddings: a tensor with shape [batch, length].</span>

<span class="sd">  Returns:</span>
<span class="sd">    lengths tensor shaped [batch] containing the unpadded length of each</span>
<span class="sd">    sequence in the batch.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">paddings</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="c1"># Find the last unpadded value. Argmax returns the first index when tied.</span>
  <span class="c1"># Cannot just use tf.reduce_sum because there might be leading paddings.</span>
  <span class="n">cumsum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cumsum</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">length</span>


<span class="k">def</span> <span class="nf">TrimTrailingPaddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Trims trailing paddings from inputs.</span>

<span class="sd">  Since the number of dimensions is not fixed, this will not work on TPU.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: a tensor with shape [batch, length, ...].</span>
<span class="sd">    paddings: a tensor with shape [batch, length].</span>

<span class="sd">  Returns:</span>
<span class="sd">    Trimmed inputs and paddings. For compatibility reasons, the trimmed tensors</span>
<span class="sd">    will always have length at least 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">paddings</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">length</span> <span class="o">=</span> <span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span>
  <span class="n">max_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
  <span class="n">output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">max_length</span><span class="p">],</span> <span class="n">output_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]],</span>
                           <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">output_shape</span><span class="p">),</span> <span class="n">output_shape</span><span class="p">)</span>
  <span class="n">out_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                          <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">max_length</span><span class="p">]))</span>
  <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">out_paddings</span>


<span class="k">def</span> <span class="nf">ReversePaddedSequence</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Reverse inputs based on paddings.</span>

<span class="sd">  Only reverse the unpadded portion of `inputs`. It assumes inputs are only</span>
<span class="sd">  padded in the end.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: a tensor of [seq_length, batch_size, num_input_nodes].</span>
<span class="sd">    paddings: a tensor of float32/float64 zero or one of shape [seq_length,</span>
<span class="sd">      batch_size, 1].</span>

<span class="sd">  Returns:</span>
<span class="sd">    A reversed tensor of the same shape as `inputs`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">inversed_paddings</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">inputs_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">rint</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inversed_paddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs_length</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">Retry</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">retry</span><span class="o">.</span><span class="n">Retry</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="c1"># FailedPreconditionError: variables are not initialized.</span>
<span class="c1"># AbortedError: processes restarts.</span>
<span class="c1"># UnavailableError: Bad hardware status: 0x1</span>
<span class="n">transient_tf_errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">FailedPreconditionError</span><span class="p">,</span>
                       <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">AbortedError</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">UnavailableError</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">RetryOnTransientTfError</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">Retry</span><span class="p">(</span><span class="n">transient_tf_errors</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">PadOrTrimTo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">pad_val</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pad and slice x to the given shape.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A tensor.</span>
<span class="sd">    shape: The shape of the returned tensor.</span>
<span class="sd">    pad_val: An int or float used to pad x.</span>

<span class="sd">  Returns:</span>
<span class="sd">    &#39;x&#39; is padded with pad_val and sliced so that the result has the given</span>
<span class="sd">    shape.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">expected_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">):</span>
    <span class="n">expected_rank</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">rank</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">expected_rank</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">)</span>
  <span class="c1"># If dim-i is less than shape[i], pads on the right shape[i] -</span>
  <span class="c1"># dim-i.  Otherwise, pads [0, 0] for dim-i.</span>
  <span class="n">pad</span> <span class="o">=</span> <span class="n">shape</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">shape</span><span class="p">)</span>
  <span class="n">zeros</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">zeros</span><span class="p">,</span> <span class="n">pad</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">constant_values</span><span class="o">=</span><span class="n">pad_val</span><span class="p">)</span>
  <span class="c1"># If dim-i is larger than shape[i], we slice [0:shape[i]] for dim-i.</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">zeros</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">RepeatDim</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">multiple</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Copies elements in tensor&#39;s axis &quot;multiple&quot; times, like np.repeat.&quot;&quot;&quot;</span>
  <span class="c1"># x = [[1, 2, 3], [4, 5, 6]]</span>
  <span class="c1"># RepeatDim(x, multiple=2, axis=1) gives:</span>
  <span class="c1"># [[1, 1, 2, 2, 3, 3]. [4, 4, 5, 5, 6, 6]]</span>
  <span class="c1"># As a comparison tf.tile(x, multiples=[1, 2]) gives:\</span>
  <span class="c1"># [[1, 2, 3, 1, 2, 3], [4, 5, 6, 4, 5, 6]]</span>

  <span class="k">if</span> <span class="n">multiple</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span>
  <span class="n">t_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="n">tensor_dims</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
      <span class="p">[</span><span class="n">t_shape</span><span class="p">[:</span><span class="n">axis</span><span class="p">],</span> <span class="p">[</span><span class="n">t_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">multiple</span><span class="p">],</span> <span class="n">t_shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]],</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">multiple_dims</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="n">multiple</span><span class="p">],</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-</span> <span class="n">axis</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
  <span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">multiple_dims</span><span class="p">),</span> <span class="n">tensor_dims</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">StackTensorsRecursively</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Recursively stacks Tensors in a list of `.NestedMap`.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: a list of `.NestedMap` or Tensors to stacks.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` with stacked values or a stacked Tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">flatten</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">values</span><span class="p">]</span>
  <span class="n">stacked</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">flatten</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
    <span class="n">stacked</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">flatten</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">flatten</span><span class="p">))])]</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span><span class="n">stacked</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span> <span class="nf">MixByWeight</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a weighted random choice and bprop type from the give inputs.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: a list of callables, where each callable returns a tf.Tensor or a</span>
<span class="sd">      nested structure containing tf.Tensor. Function return types must be</span>
<span class="sd">      consistent across elements. The tf.Operation to compute the result tensor</span>
<span class="sd">      will only be invoked for one input at a time. For example, if each fn</span>
<span class="sd">      represents an input record stream, a record will be drawn only from a</span>
<span class="sd">      selected stream while the other streams will remain unchanged.</span>
<span class="sd">    weights: a 1D tensor of float &gt; 0 of the same length as inputs.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A probablistic sample from the inputs proportional to the weights. The</span>
<span class="sd">    return type will be the same as return type of individual &#39;fn&#39; from the</span>
<span class="sd">    inputs.</span>
<span class="sd">    A one-hot vector of the source selected.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)]),</span>
      <span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
  <span class="p">],</span> <span class="n">weights</span><span class="p">)</span>

  <span class="n">lower</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">upper</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">maxval</span><span class="o">=</span><span class="n">upper</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">return_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">case</span><span class="p">(</span>
      <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">lower</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">r</span><span class="p">,</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">upper</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
       <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))],</span>
      <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">selected_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">case</span><span class="p">(</span>
      <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">lower</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">r</span><span class="p">,</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">upper</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="k">lambda</span> <span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="p">)</span>
       <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))],</span>
      <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">bprop_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">selected_index</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">return_input</span><span class="p">,</span> <span class="n">bprop_index</span>


<span class="k">def</span> <span class="nf">CheckShapes</span><span class="p">(</span><span class="n">shapes</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Asserts that shapes is a tuple of tshape.Shape.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shapes</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">),</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">s</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">FPropDtype</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">params</span><span class="o">.</span><span class="n">dtype</span>


<span class="k">def</span> <span class="nf">UpdateFpropDtype</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Recursively update the fprop_dtype of the Params.&quot;&quot;&quot;</span>
  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">IterParams</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
      <span class="n">UpdateFpropDtype</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;fprop_dtype&#39;</span><span class="p">:</span>
      <span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">fprop_dtype</span>


<span class="k">def</span> <span class="nf">UpdateDtype</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Recursively update the dtype of the Params.&quot;&quot;&quot;</span>
  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">IterParams</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
      <span class="n">UpdateDtype</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span>
      <span class="n">params</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>


<span class="k">def</span> <span class="nf">NameScopeDecorator</span><span class="p">(</span><span class="n">name_scope</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Decorates a python function to introduce a tf.name_scope.</span>

<span class="sd">  Example::</span>

<span class="sd">      @py_utils.NameScopeDecorator(&#39;foobar&#39;)</span>
<span class="sd">      def MyFoobarMethod(self):</span>
<span class="sd">        # ... Do TF things</span>

<span class="sd">  Args:</span>
<span class="sd">    name_scope: The name scope to introduce.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function decorator.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">Decorator</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">Wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name_scope</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Wrapped</span>

  <span class="k">return</span> <span class="n">Decorator</span>


<span class="k">def</span> <span class="nf">SequencesToDebugStrings</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">lens</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns debug strings for the given sequences.</span>

<span class="sd">  Args:</span>
<span class="sd">    ids: int32 of [batch, len].</span>
<span class="sd">    lens: int32 of [batch].</span>
<span class="sd">    summarize: number of ids to summarize per sequence.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A string tensor of [batch].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">num_seqs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">lens</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_Body</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">lens</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">summarize</span><span class="o">=</span><span class="n">summarize</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">result</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="n">i0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">result0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">strs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_seqs</span><span class="p">,</span>
      <span class="n">_Body</span><span class="p">,</span> <span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="n">result0</span><span class="p">),</span>
      <span class="n">shape_invariants</span><span class="o">=</span><span class="p">(</span><span class="n">i0</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="kc">None</span><span class="p">])))</span>
  <span class="k">return</span> <span class="n">strs</span>


<span class="k">def</span> <span class="nf">RematerializeFn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">xs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calls fn and rematerializes fn in the backward pass.</span>

<span class="sd">  `fn(*xs) -&gt; ys`, where xs and ys can be a single tensor or a tuple of tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    fn: A python function to be rematerialized in the backprop pass.</span>
<span class="sd">    *xs: A single tensor or a list/tuple of tensors. `xs` are input args to the</span>
<span class="sd">      fn function.</span>

<span class="sd">  Returns:</span>
<span class="sd">    `fn(*xs)`</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">initial_step_seed</span> <span class="o">=</span> <span class="n">GetStepSeed</span><span class="p">()</span>
  <span class="n">final_step_seed</span> <span class="o">=</span> <span class="n">zlib</span><span class="o">.</span><span class="n">adler32</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;new_step_seed&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">encode</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">Backward</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">*</span><span class="n">dy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The backward function that rematerializes forward outputs.&quot;&quot;&quot;</span>
    <span class="n">always_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([])</span> <span class="o">&lt;</span> <span class="mf">2.0</span>
    <span class="c1"># Alternatively, can do this:</span>
    <span class="c1"># tf.where(tf.is_nan(x),</span>
    <span class="c1">#          tf.constant(float(&#39;nan&#39;), dtype=x.dtype) * tf.ones_like(x),</span>
    <span class="c1">#          x)</span>
    <span class="c1"># Skip op.inputs[0] which is initial_step_seed.</span>
    <span class="n">bak_xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">always_true</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
    <span class="k">for</span> <span class="n">dst</span><span class="p">,</span> <span class="n">src</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bak_xs</span><span class="p">,</span> <span class="n">xs</span><span class="p">):</span>
      <span class="n">dst</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">bak_xs</span><span class="p">)</span>
    <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">final_step_seed</span><span class="p">)</span>
    <span class="n">dxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">bak_xs</span><span class="p">,</span> <span class="n">grad_ys</span><span class="o">=</span><span class="n">dy</span><span class="p">)</span>
    <span class="n">dxs_final</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">dx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dxs</span><span class="p">,</span> <span class="n">bak_xs</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">dx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dxs_final</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">dxs_final</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">dxs_final</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">bak_xs</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">),)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">dxs_final</span><span class="p">)</span>

  <span class="n">xs_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">]</span>
  <span class="n">ys_shapes</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># TODO(huangyp, yonghui): Check Forward doesn&#39;t use any stateful random ops.</span>
  <span class="nd">@function</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span>
      <span class="n">initial_step_seed</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="o">*</span><span class="n">xs_dtypes</span><span class="p">,</span> <span class="n">python_grad_func</span><span class="o">=</span><span class="n">Backward</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">Forward</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">,</span> <span class="o">*</span><span class="n">fwd_xs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Forward function plus sanity checks.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">dst</span><span class="p">,</span> <span class="n">src</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">fwd_xs</span><span class="p">,</span> <span class="n">xs</span><span class="p">):</span>
      <span class="n">dst</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">fwd_xs</span><span class="p">)</span>
    <span class="c1"># Some sanity check.</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">function</span><span class="o">.</span><span class="n">get_extra_inputs</span><span class="p">()</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">function</span><span class="o">.</span><span class="n">get_extra_args</span><span class="p">()</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">function</span><span class="o">.</span><span class="n">get_extra_vars</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="n">ys_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
      <span class="n">ys_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ys</span>

  <span class="n">ys</span> <span class="o">=</span> <span class="n">Forward</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">,</span> <span class="o">*</span><span class="n">xs</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">ys_shapes</span><span class="p">):</span>
      <span class="n">y</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">ys_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="c1"># TODO(b/129159299): The ResetStepSeed below is needed to work around this</span>
  <span class="c1"># bug, which is a problem with global tensors being shared by different</span>
  <span class="c1"># inference graphs. It should be replaced with the new step seed value</span>
  <span class="c1"># returned from the Forward function when the bug is fixed.</span>
  <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">final_step_seed</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ys</span>


<span class="c1"># A set of names of stateful random number generator ops.</span>
<span class="c1"># See tensorflow/core/ops/random_ops.cc</span>
<span class="n">_STATEFUL_RANDOM_OPS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># pyformat: disable</span>
    <span class="s1">&#39;RandomUniform&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomUniformInt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomStandardNormal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ParameterizedTruncatedNormal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;TruncatedNormal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomShuffle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Multinomial&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomGamma&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomPoisson&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomPoissonV2&#39;</span><span class="p">,</span>
    <span class="c1"># pyformat: enable</span>
<span class="p">}</span>


<span class="k">def</span> <span class="nf">StatefulRandomOpsInDefun</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Checks whether the Defun depends on stateful random number ops.</span>

<span class="sd">  Stateful random number generator ops should be avoid in Recurrent() call.</span>
<span class="sd">  Otherwise, these ops produce inconsistent values between FProp and BProp.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: a _DefinedFunction to check.</span>
<span class="sd">    graph: a Graph. Set None to use the default graph.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of names of the stateful random ops.</span>

<span class="sd">  Raises:</span>
<span class="sd">    InvalidArgumentError: if the input func/graph is invalid.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">function</span><span class="o">.</span><span class="n">_DefinedFunction</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">raise</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                         <span class="s1">&#39;func is not a _DefinedFunction.&#39;</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">graph</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
  <span class="n">func</span><span class="o">.</span><span class="n">add_to_graph</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
  <span class="n">graph_def</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_graph_def</span><span class="p">()</span>

  <span class="c1"># A dict from function name to FunctionDef.</span>
  <span class="n">func_defs</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">graph_def</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">function</span><span class="p">}</span>

  <span class="k">if</span> <span class="n">func</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">func_defs</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
        <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Defun </span><span class="si">{}</span><span class="s1"> is not in the graph .&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

  <span class="n">stateful_ops</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Recursively search for stateful random op.</span>
  <span class="n">nodes</span> <span class="o">=</span> <span class="n">py_collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>
  <span class="k">while</span> <span class="n">nodes</span><span class="p">:</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">),</span> <span class="n">node</span>

    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="n">_STATEFUL_RANDOM_OPS</span><span class="p">:</span>
      <span class="n">stateful_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
      <span class="k">continue</span>

    <span class="k">def</span> <span class="nf">_AddDefunNodes</span><span class="p">(</span><span class="n">func_name</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;If the given func_name is a Defun, add its sub-nodes into nodes.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">func_defs</span><span class="p">:</span>
        <span class="n">nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">func_defs</span><span class="p">[</span><span class="n">func_name</span><span class="p">]</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>

    <span class="c1"># For functional.{While|For|If} ops, add their Defun attr into search.</span>
    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;While&#39;</span><span class="p">:</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;body&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;cond&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;For&#39;</span><span class="p">:</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;body&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;If&#39;</span><span class="p">:</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;then_branch&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;else_branch&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># For other op, check whether itself is a Defun op.</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">stateful_ops</span>


<span class="k">def</span> <span class="nf">ToPlacerholders</span><span class="p">(</span><span class="n">nmap</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts every Tensor in nmap to a placeholder.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_ToPlacerholder</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">nmap</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">_ToPlacerholder</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>